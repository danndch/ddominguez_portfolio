[
  {
    "objectID": "story_telling.html",
    "href": "story_telling.html",
    "title": "about me",
    "section": "",
    "text": "MarkDown Basics"
  },
  {
    "objectID": "story_telling.html#title-2-header",
    "href": "story_telling.html#title-2-header",
    "title": "about me",
    "section": "",
    "text": "MarkDown Basics"
  },
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "Senior Project",
    "section": "",
    "text": "The objective of this project is to design and construct an educational framework that helps Data Science students develop skills typically used by Data Engineers. This project aims to introduce students to scenarios and a pace of research and learning that keeps a scholarly mindset while incorporating aspects of a professional environment.\n\n\n\nData Engineering for Data Science\n\nThis provides a background on different topics that I researched in job boards for the most common Data Engineers Skills that jobs are asking and students can learn to fit into this description after studying all the core courses of the Data Science program at BYU-Idaho. The content of exposed in the page is:\n\n\nData Science delveig into Data Engineering\nWhy SQL?\nThe Value of Data Engineering Skills\nYour Path to Becoming a Data Science Hybrid with Data Engineering\nNow where is this being used?\nMaterial\nETL VS ELT\nELT (Extract, Load, Transform)\nSQL’s Role in ETL and ELT\nData Products\n\nOne of my major takeouts from college along my job, internship, projects on the side, and projects at society, was to discover the whole process of completing a Data Product and I believe you don’t get to do something like that and understand the parts that will make a project better until I learned the majority of the following points.\n\n\nData Products and its’ parts\nThe key characteristics of a data product include:\nSQL Elements\nParts of the query\nSubqueries\nNested Queries\nAPIs\nData Pipelines\nData Enrichment\nFeature Engineering\nData Enrichment vs Feature Engineering\nData Maturity After my research and experience I understood that the field of Data tends to confuse some of the roles involved in the industry, which sometimes might be confusing. This took me to write a presentation for starters in the Data-verse to help them use Data Science and many of the technologies related can bring as assets to their projects, industry, and business.\nProject from the USU Hackathon with SQL and Snowflake This project includes the set-up and connection to snowflakes. The goal of this project is to predict when would a package will be deliverd by an international company shipping materials to different parts of the world. This project also explores the use of a new tool name Pycaret.",
    "crumbs": [
      "Senior Project"
    ]
  },
  {
    "objectID": "projects.html#topics",
    "href": "projects.html#topics",
    "title": "Senior Project",
    "section": "",
    "text": "Data Engineering for Data Science\n\nThis provides a background on different topics that I researched in job boards for the most common Data Engineers Skills that jobs are asking and students can learn to fit into this description after studying all the core courses of the Data Science program at BYU-Idaho. The content of exposed in the page is:\n\n\nData Science delveig into Data Engineering\nWhy SQL?\nThe Value of Data Engineering Skills\nYour Path to Becoming a Data Science Hybrid with Data Engineering\nNow where is this being used?\nMaterial\nETL VS ELT\nELT (Extract, Load, Transform)\nSQL’s Role in ETL and ELT\nData Products\n\nOne of my major takeouts from college along my job, internship, projects on the side, and projects at society, was to discover the whole process of completing a Data Product and I believe you don’t get to do something like that and understand the parts that will make a project better until I learned the majority of the following points.\n\n\nData Products and its’ parts\nThe key characteristics of a data product include:\nSQL Elements\nParts of the query\nSubqueries\nNested Queries\nAPIs\nData Pipelines\nData Enrichment\nFeature Engineering\nData Enrichment vs Feature Engineering\nData Maturity After my research and experience I understood that the field of Data tends to confuse some of the roles involved in the industry, which sometimes might be confusing. This took me to write a presentation for starters in the Data-verse to help them use Data Science and many of the technologies related can bring as assets to their projects, industry, and business.\nProject from the USU Hackathon with SQL and Snowflake This project includes the set-up and connection to snowflakes. The goal of this project is to predict when would a package will be deliverd by an international company shipping materials to different parts of the world. This project also explores the use of a new tool name Pycaret.",
    "crumbs": [
      "Senior Project"
    ]
  },
  {
    "objectID": "Projects/project5.html",
    "href": "Projects/project5.html",
    "title": "Client Report - [Insert Project Title]",
    "section": "",
    "text": "paste your elevator pitch here A SHORT (4-5 SENTENCES) PARAGRAPH THAT DESCRIBES KEY INSIGHTS TAKEN FROM METRICS IN THE PROJECT RESULTS THINK TOP OR MOST IMPORTANT RESULTS.\n\n\nRead and format project data\n# Include and execute your code here\ndf = pd.read_csv(\"https://github.com/byuidatascience/data4names/raw/master/data-raw/names_year/names_year.csv\")\n\n\nHighlight the Questions and Tasks"
  },
  {
    "objectID": "Projects/project5.html#elevator-pitch",
    "href": "Projects/project5.html#elevator-pitch",
    "title": "Client Report - [Insert Project Title]",
    "section": "",
    "text": "paste your elevator pitch here A SHORT (4-5 SENTENCES) PARAGRAPH THAT DESCRIBES KEY INSIGHTS TAKEN FROM METRICS IN THE PROJECT RESULTS THINK TOP OR MOST IMPORTANT RESULTS.\n\n\nRead and format project data\n# Include and execute your code here\ndf = pd.read_csv(\"https://github.com/byuidatascience/data4names/raw/master/data-raw/names_year/names_year.csv\")\n\n\nHighlight the Questions and Tasks"
  },
  {
    "objectID": "Projects/project5.html#questiontask-1",
    "href": "Projects/project5.html#questiontask-1",
    "title": "Client Report - [Insert Project Title]",
    "section": "QUESTION|TASK 1",
    "text": "QUESTION|TASK 1\nCOPY PASTE QUESTION|TASK 1 FROM THE PROJECT HERE\ntype your results and analysis here\n\n\nRead and format data\n# Include and execute your code here\n\n\ninclude figures in chunks and discuss your findings in the figure.\n::: {#cell-Q1 chart .cell execution_count=4}\n\nplot example\n# Include and execute your code here\nchart = px.bar(df.head(200),\n    x=\"name\", \n    y=\"AK\"\n)\nchart.show()\n\n\n                                                \nMy useless chart\n\n:::\n::: {#cell-Q1 table .cell .tbl-cap-location-top tbl-cap=‘Not much of a table’ execution_count=5}\n\ntable example\n# Include and execute your code here\nmydat = df.head(1000)\\\n    .groupby('year')\\\n    .sum()\\\n    .reset_index()\\\n    .tail(10)\\\n    .filter([\"year\", \"AK\",\"AR\"])\n\ndisplay(mydat)\n\n\n\n\n\n\n\n\n\nyear\nAK\nAR\n\n\n\n\n96\n2006\n21.0\n183.0\n\n\n97\n2007\n28.0\n153.0\n\n\n98\n2008\n36.0\n212.0\n\n\n99\n2009\n34.0\n179.0\n\n\n100\n2010\n22.0\n196.0\n\n\n101\n2011\n41.0\n148.0\n\n\n102\n2012\n28.0\n140.0\n\n\n103\n2013\n26.0\n134.0\n\n\n104\n2014\n20.0\n114.0\n\n\n105\n2015\n28.0\n121.0\n\n\n\n\n\n\n:::"
  },
  {
    "objectID": "Projects/project5.html#questiontask-2",
    "href": "Projects/project5.html#questiontask-2",
    "title": "Client Report - [Insert Project Title]",
    "section": "QUESTION|TASK 2",
    "text": "QUESTION|TASK 2\nCOPY PASTE QUESTION|TASK 2 FROM THE PROJECT HERE\ntype your results and analysis here\n\n\nRead and format data\n# Include and execute your code here\n\n\ninclude figures in chunks and discuss your findings in the figure.\n::: {#cell-Q2 chart .cell execution_count=7}\n\nplot example\n# Include and execute your code here\nchart = px.bar(df.head(200),\n    x=\"name\", \n    y=\"AK\"\n)\nchart.show()\n\n\n                                                \nMy useless chart\n\n:::\n::: {#cell-Q2 table .cell .tbl-cap-location-top tbl-cap=‘Not much of a table’ execution_count=8}\n\ntable example\n# Include and execute your code here\nmydat = df.head(1000)\\\n    .groupby('year')\\\n    .sum()\\\n    .reset_index()\\\n    .tail(10)\\\n    .filter([\"year\", \"AK\",\"AR\"])\n\ndisplay(mydat)\n\n\n\n\n\n\n\n\n\nyear\nAK\nAR\n\n\n\n\n96\n2006\n21.0\n183.0\n\n\n97\n2007\n28.0\n153.0\n\n\n98\n2008\n36.0\n212.0\n\n\n99\n2009\n34.0\n179.0\n\n\n100\n2010\n22.0\n196.0\n\n\n101\n2011\n41.0\n148.0\n\n\n102\n2012\n28.0\n140.0\n\n\n103\n2013\n26.0\n134.0\n\n\n104\n2014\n20.0\n114.0\n\n\n105\n2015\n28.0\n121.0\n\n\n\n\n\n\n:::"
  },
  {
    "objectID": "Projects/project5.html#questiontask-3",
    "href": "Projects/project5.html#questiontask-3",
    "title": "Client Report - [Insert Project Title]",
    "section": "QUESTION|TASK 3",
    "text": "QUESTION|TASK 3\nCOPY PASTE QUESTION|TASK 3 FROM THE PROJECT HERE\ntype your results and analysis here\n\n\nRead and format data\n# Include and execute your code here\n\n\ninclude figures in chunks and discuss your findings in the figure.\n::: {#cell-Q3 chart .cell execution_count=10}\n\nplot example\n# Include and execute your code here\nchart = px.bar(df.head(200),\n    x=\"name\", \n    y=\"AK\"\n)\nchart.show()\n\n\n                                                \nMy useless chart\n\n:::\n::: {#cell-Q3 table .cell .tbl-cap-location-top tbl-cap=‘Not much of a table’ execution_count=11}\n\ntable example\n# Include and execute your code here\nmydat = df.head(1000)\\\n    .groupby('year')\\\n    .sum()\\\n    .reset_index()\\\n    .tail(10)\\\n    .filter([\"year\", \"AK\",\"AR\"])\n\ndisplay(mydat)\n\n\n\n\n\n\n\n\n\nyear\nAK\nAR\n\n\n\n\n96\n2006\n21.0\n183.0\n\n\n97\n2007\n28.0\n153.0\n\n\n98\n2008\n36.0\n212.0\n\n\n99\n2009\n34.0\n179.0\n\n\n100\n2010\n22.0\n196.0\n\n\n101\n2011\n41.0\n148.0\n\n\n102\n2012\n28.0\n140.0\n\n\n103\n2013\n26.0\n134.0\n\n\n104\n2014\n20.0\n114.0\n\n\n105\n2015\n28.0\n121.0\n\n\n\n\n\n\n:::"
  },
  {
    "objectID": "Projects/project3.html",
    "href": "Projects/project3.html",
    "title": "Final Post",
    "section": "",
    "text": "The rapid growth of Artificial Intelligence (AI) and Machine Learning (ML) in this digital era is pushing many industries to start using these technologies to stay competitive and leverage the significant benefits they offer for making informed decisions. But how do we keep up when there’s confusion about the roles and responsibilities within data-related jobs? Through my experience of applying to over 100 jobs, including positions at Fortune 200 companies, and interacting with employers and recruiters at career fairs, I’ve observed widespread misconceptions about the data industry and its roles, and what data can actually achieve for businesses. This situation reminds me of learning the differences between “Their,” “There,” and “They’re.” A common issue is the interchangeability of titles such as Data Analyst, Data Scientist, and Data Engineer, along with the misconception that Data Scientists work only in IT. These misunderstandings made me wonder: if even mid-to-large-sized companies confuse these terms, how can smaller companies and startups take advantage and structure their data processes for success?\nOne notable conversation with an employer at a small company highlighted a common challenge: they had an Analyst position that, in practice, was not focused on analyzing data in the way one might expect. When asked if they had a dedicated full-time data analyst to create reports, reveal trends, and provide insights for decision-makers, the answer was no. Yet, the idea of having someone to perform in-depth analysis and help understand trends was seen as highly beneficial. This interaction underscores the widespread need for clearer roles and a more structured approach to data analysis within organizations, regardless of size.\nThe concept of Data Maturity is designed to address this gap by providing a comprehensive understanding of the necessary actions and tools organizations need to effectively employ data and generate insights. It guides companies in identifying their specific needs, where they need to invest more effort, and how to establish a reliable process for data analysis. The aim is to lay a solid foundation in the data processes of individuals and organizations, ensuring they have the right information when they need it. By structuring new information with a clear end goal—answering questions and making informed decisions—Data Maturity empowers businesses to leverage their data more effectively and achieve greater success.\nThis approach is particularly crucial in an era dominated by AI and ML, where the ability to navigate the complex data landscape can set companies apart. Data Maturity acts as a map, offering key landmarks and guidelines rather than a rigid, step-by-step recipe. This framework aims to transform the data process, making it more efficient and aligned with organizational goals, thereby facilitating a deeper understanding and better use of data for strategic decision-making.\n\n\n\n\n\n\n\nThe intention of the following text is to help understand different data roles, but as well to help individuals and companies to start applying the data process and make use of tools like Data Science, Data Analysis, Data Engineering, Machine Learning, AI.\nOur purpose as being involved in the field is to facilitate Data Products from the beginning till the end.\n\n\n\nThinking about making well-informed decisions for your business using reliable and meaningful information? Data Maturity is here to guide you through the process, from starting out to achieving your goals. If you’re considering diving into the world of AI and Machine Learning because you’ve heard about the incredible advantages they can bring, you’re on the right track. While we can’t promise every decision will be perfect, AI and ML can significantly improve your chances of success.\nIt all starts with a question – the “why” or “how” behind a problem you’re looking to solve. In the world of data, every question is an opportunity to embark on a new journey of discovery. This is particularly exciting for those who are keen to uncover insights from data.\n\n\nImagine we’re stepping into a forest, looking to handpick the perfect trees for crafting furniture. This is similar to the exploratory phase, where we scope out the landscape to find the data we need. Whether we’re generating new data or sourcing it from elsewhere, this phase is about gathering the raw materials for our project.\n\n\n\nThink of extracting data like selecting and cutting down the right trees from our forest. We’re identifying and collecting the data we believe will be most useful for our end goal. Extracting involves pulling together all this data, whether it’s from documents, files, databases, or other sources, to make it accessible and ready for use.\nTransforming data is akin to milling those trees into usable lumber, shaping it to fit our project’s needs. This crucial step involves cleaning and organizing the data, using tools and programming languages like R, Python, or SQL, to ensure it’s in a usable form.\nLoading the data then involves putting it to use, much like utilizing our prepared lumber to start building. This can lead to two paths: one where data is immediately used to inform decisions through dashboards or reports, and another where it’s stored in a Data Warehouse for future use. Data stored in a warehouse can be organized in a Data Mart, making it easily accessible for specific needs.\n\n\n\nNow that our data (or wood, in our analogy) is prepared and ready, we need to decide what to build with it. In data terms, this means analyzing the information to uncover trends, insights, and potential risks. This stage can already inform some decisions, but there’s more depth to explore for greater confidence.\n\n\n\nWith our data cleaned and relevant, it’s time to dig deeper and find out what’s truly significant. This is where Machine Learning comes in, helping to assess the importance of our findings, much like making sure we’re studying the right material for an exam.\n\n\n\nWouldn’t it be great to know your decisions will turn out well? While we can’t guarantee 100% success, Machine Learning can get us close by predicting outcomes. This allows us to prepare for various scenarios, increasing the likelihood of success.\n\n\n\nThe real test is in the significance of our predictions. This might involve some trial and error, but it’s all about understanding if our predictions hold up in the real world. If they do, our confidence in making decisions grows.\n\n\n\nOnce we have significant results, we can start training AI. AI is all about teaching computers to perform tasks like analyzing data or making recommendations. It’s a big step but can bring incredible benefits to your business.\nIn the end, Data Maturity is about equipping you with the knowledge and tools to make impactful decisions. Whether it’s figuring out the best step forward, where to invest, or how to optimize your processes, AI and Machine Learning are here to help. As a Data Scientist, my goal is to make these powerful tools accessible to you and your business.\n\n\n\n\nI presented this iniciative and teaching material at the Research and Creative Works where I was able to present it to judges and people that were interested.\nThe most commmon question I got from people is What is Data Science and what do you do? It was very nice to have the opportunity to tell some of the people what do I and what solutions can Data Science bring to their business and how can it be implemented in different industries and fields.\nFrom the judges I got 17 points out of 18 with the following feedback:\nThe scoring is broken down as the following: 3 (Outstanding) 2 (Meets Expectations) 1 (Sufficient, needs some improvement)\nMastery of concepts, skills, field: 3,3\nIndependence of thought and work: 3,3\nProfessional presence of presenter: 3,3\nEffectiveness of poster/visuals: 2,3\nUnderstood questions and responded effectively: 3,3\nProgress toward objective: 3,3\nOverall impression/COMMENTS:\n\nGreat job! Graphic was very informative. Would be nice to organize/simplify all the text and maybe add pictures to make it easier to read. Very well presented by Daniel!\nI love your poster. It will be very helpful for students trying to understand their options. The handout was a great idea; I’m going to recommend that everyone do a handout! Well done!",
    "crumbs": [
      "Senior Project",
      "Data Maturity"
    ]
  },
  {
    "objectID": "Projects/project3.html#the-abstract",
    "href": "Projects/project3.html#the-abstract",
    "title": "Final Post",
    "section": "",
    "text": "The rapid growth of Artificial Intelligence (AI) and Machine Learning (ML) in this digital era is pushing many industries to start using these technologies to stay competitive and leverage the significant benefits they offer for making informed decisions. But how do we keep up when there’s confusion about the roles and responsibilities within data-related jobs? Through my experience of applying to over 100 jobs, including positions at Fortune 200 companies, and interacting with employers and recruiters at career fairs, I’ve observed widespread misconceptions about the data industry and its roles, and what data can actually achieve for businesses. This situation reminds me of learning the differences between “Their,” “There,” and “They’re.” A common issue is the interchangeability of titles such as Data Analyst, Data Scientist, and Data Engineer, along with the misconception that Data Scientists work only in IT. These misunderstandings made me wonder: if even mid-to-large-sized companies confuse these terms, how can smaller companies and startups take advantage and structure their data processes for success?\nOne notable conversation with an employer at a small company highlighted a common challenge: they had an Analyst position that, in practice, was not focused on analyzing data in the way one might expect. When asked if they had a dedicated full-time data analyst to create reports, reveal trends, and provide insights for decision-makers, the answer was no. Yet, the idea of having someone to perform in-depth analysis and help understand trends was seen as highly beneficial. This interaction underscores the widespread need for clearer roles and a more structured approach to data analysis within organizations, regardless of size.\nThe concept of Data Maturity is designed to address this gap by providing a comprehensive understanding of the necessary actions and tools organizations need to effectively employ data and generate insights. It guides companies in identifying their specific needs, where they need to invest more effort, and how to establish a reliable process for data analysis. The aim is to lay a solid foundation in the data processes of individuals and organizations, ensuring they have the right information when they need it. By structuring new information with a clear end goal—answering questions and making informed decisions—Data Maturity empowers businesses to leverage their data more effectively and achieve greater success.\nThis approach is particularly crucial in an era dominated by AI and ML, where the ability to navigate the complex data landscape can set companies apart. Data Maturity acts as a map, offering key landmarks and guidelines rather than a rigid, step-by-step recipe. This framework aims to transform the data process, making it more efficient and aligned with organizational goals, thereby facilitating a deeper understanding and better use of data for strategic decision-making.",
    "crumbs": [
      "Senior Project",
      "Data Maturity"
    ]
  },
  {
    "objectID": "Projects/project3.html#an-initiative-for-any-business-and-individuals.",
    "href": "Projects/project3.html#an-initiative-for-any-business-and-individuals.",
    "title": "Final Post",
    "section": "",
    "text": "The intention of the following text is to help understand different data roles, but as well to help individuals and companies to start applying the data process and make use of tools like Data Science, Data Analysis, Data Engineering, Machine Learning, AI.\nOur purpose as being involved in the field is to facilitate Data Products from the beginning till the end.",
    "crumbs": [
      "Senior Project",
      "Data Maturity"
    ]
  },
  {
    "objectID": "Projects/project3.html#the-pitch",
    "href": "Projects/project3.html#the-pitch",
    "title": "Final Post",
    "section": "",
    "text": "Thinking about making well-informed decisions for your business using reliable and meaningful information? Data Maturity is here to guide you through the process, from starting out to achieving your goals. If you’re considering diving into the world of AI and Machine Learning because you’ve heard about the incredible advantages they can bring, you’re on the right track. While we can’t promise every decision will be perfect, AI and ML can significantly improve your chances of success.\nIt all starts with a question – the “why” or “how” behind a problem you’re looking to solve. In the world of data, every question is an opportunity to embark on a new journey of discovery. This is particularly exciting for those who are keen to uncover insights from data.\n\n\nImagine we’re stepping into a forest, looking to handpick the perfect trees for crafting furniture. This is similar to the exploratory phase, where we scope out the landscape to find the data we need. Whether we’re generating new data or sourcing it from elsewhere, this phase is about gathering the raw materials for our project.\n\n\n\nThink of extracting data like selecting and cutting down the right trees from our forest. We’re identifying and collecting the data we believe will be most useful for our end goal. Extracting involves pulling together all this data, whether it’s from documents, files, databases, or other sources, to make it accessible and ready for use.\nTransforming data is akin to milling those trees into usable lumber, shaping it to fit our project’s needs. This crucial step involves cleaning and organizing the data, using tools and programming languages like R, Python, or SQL, to ensure it’s in a usable form.\nLoading the data then involves putting it to use, much like utilizing our prepared lumber to start building. This can lead to two paths: one where data is immediately used to inform decisions through dashboards or reports, and another where it’s stored in a Data Warehouse for future use. Data stored in a warehouse can be organized in a Data Mart, making it easily accessible for specific needs.\n\n\n\nNow that our data (or wood, in our analogy) is prepared and ready, we need to decide what to build with it. In data terms, this means analyzing the information to uncover trends, insights, and potential risks. This stage can already inform some decisions, but there’s more depth to explore for greater confidence.\n\n\n\nWith our data cleaned and relevant, it’s time to dig deeper and find out what’s truly significant. This is where Machine Learning comes in, helping to assess the importance of our findings, much like making sure we’re studying the right material for an exam.\n\n\n\nWouldn’t it be great to know your decisions will turn out well? While we can’t guarantee 100% success, Machine Learning can get us close by predicting outcomes. This allows us to prepare for various scenarios, increasing the likelihood of success.\n\n\n\nThe real test is in the significance of our predictions. This might involve some trial and error, but it’s all about understanding if our predictions hold up in the real world. If they do, our confidence in making decisions grows.\n\n\n\nOnce we have significant results, we can start training AI. AI is all about teaching computers to perform tasks like analyzing data or making recommendations. It’s a big step but can bring incredible benefits to your business.\nIn the end, Data Maturity is about equipping you with the knowledge and tools to make impactful decisions. Whether it’s figuring out the best step forward, where to invest, or how to optimize your processes, AI and Machine Learning are here to help. As a Data Scientist, my goal is to make these powerful tools accessible to you and your business.",
    "crumbs": [
      "Senior Project",
      "Data Maturity"
    ]
  },
  {
    "objectID": "Projects/project3.html#presentation-and-creative-works-conference",
    "href": "Projects/project3.html#presentation-and-creative-works-conference",
    "title": "Final Post",
    "section": "",
    "text": "I presented this iniciative and teaching material at the Research and Creative Works where I was able to present it to judges and people that were interested.\nThe most commmon question I got from people is What is Data Science and what do you do? It was very nice to have the opportunity to tell some of the people what do I and what solutions can Data Science bring to their business and how can it be implemented in different industries and fields.\nFrom the judges I got 17 points out of 18 with the following feedback:\nThe scoring is broken down as the following: 3 (Outstanding) 2 (Meets Expectations) 1 (Sufficient, needs some improvement)\nMastery of concepts, skills, field: 3,3\nIndependence of thought and work: 3,3\nProfessional presence of presenter: 3,3\nEffectiveness of poster/visuals: 2,3\nUnderstood questions and responded effectively: 3,3\nProgress toward objective: 3,3\nOverall impression/COMMENTS:\n\nGreat job! Graphic was very informative. Would be nice to organize/simplify all the text and maybe add pictures to make it easier to read. Very well presented by Daniel!\nI love your poster. It will be very helpful for students trying to understand their options. The handout was a great idea; I’m going to recommend that everyone do a handout! Well done!",
    "crumbs": [
      "Senior Project",
      "Data Maturity"
    ]
  },
  {
    "objectID": "Projects/project1.html",
    "href": "Projects/project1.html",
    "title": "Senior Project Post 1",
    "section": "",
    "text": "In the rapidly evolving world of Data, where the ability to turn vast amounts of raw data into actionable insights is highly prized. You may know Python, you may know R, but mastering the language of data and understanding the architecture that supports it are not just an advantage but it is essential. SQL and data engineering, is a realm that it is close to Data Science and there are some skills that will set you apart in your future career.\n\n\nSQL, or Structured Query Language, is the cornerstone of data manipulation and retrieval. It’s the bridge between the questions you seek to answer and the data that contains those answers. Whether it’s understanding customer behavior, predicting market trends, or improving operational efficiency, SQL empowers you to:\n\nAccess Critical Data Quickly: Navigate through complex databases with ease, ensuring that no source of insight is beyond your reach.\nCommunicate Effectively with Data: Speak the universal language understood by databases worldwide, making your analyses more versatile and impactful.\nRefine Your Analytical Precision: Hone your ability to ask nuanced questions of your data, enabling more precise, targeted analyses.\n\n\n\n\nAs Data Scientists, our vision is only as clear as the data at our disposal. Data engineering skills could ensure that the data we rely on is accurate, accessible, and ready for analysis. By understanding data pipelines, storage solutions, and data processing, we will:\n\nEnhance Data Quality and Accessibility: Ensure that the data you use for your analyses is clean, well-structured, and readily available, reducing the time from question to insight.\nScale Your Data Efforts: Tackle larger datasets and more complex problems with confidence, knowing that your data infrastructure is robust and scalable.\nBridge the Gap Between Data and Decision-Making: Play that role in translating data insights into strategic actions, making your contributions invaluable to any team.\nYou don’t want to sit and wait: If you provide to your employer a solution more than waiting for the Data Engineer to give you the data you need to work, what if you could provide that solution as well, not having to depend a 100% on someone else gives you freedome and a higher value in the market.\n\n\n\n\nEmbracing SQL and and some other data engineering concepts is not just about enlarging your technical toolbox; it’s about expanding your problem-solving skillset. In a data-driven world, these skills enable you to navigate and make sense of the multiple situations, turning data into information which will become decisions that drive progress.",
    "crumbs": [
      "Senior Project",
      "Data Engineering for DS"
    ]
  },
  {
    "objectID": "Projects/project1.html#why-sql",
    "href": "Projects/project1.html#why-sql",
    "title": "Senior Project Post 1",
    "section": "",
    "text": "SQL, or Structured Query Language, is the cornerstone of data manipulation and retrieval. It’s the bridge between the questions you seek to answer and the data that contains those answers. Whether it’s understanding customer behavior, predicting market trends, or improving operational efficiency, SQL empowers you to:\n\nAccess Critical Data Quickly: Navigate through complex databases with ease, ensuring that no source of insight is beyond your reach.\nCommunicate Effectively with Data: Speak the universal language understood by databases worldwide, making your analyses more versatile and impactful.\nRefine Your Analytical Precision: Hone your ability to ask nuanced questions of your data, enabling more precise, targeted analyses.",
    "crumbs": [
      "Senior Project",
      "Data Engineering for DS"
    ]
  },
  {
    "objectID": "Projects/project1.html#the-value-of-data-engineering-skills",
    "href": "Projects/project1.html#the-value-of-data-engineering-skills",
    "title": "Senior Project Post 1",
    "section": "",
    "text": "As Data Scientists, our vision is only as clear as the data at our disposal. Data engineering skills could ensure that the data we rely on is accurate, accessible, and ready for analysis. By understanding data pipelines, storage solutions, and data processing, we will:\n\nEnhance Data Quality and Accessibility: Ensure that the data you use for your analyses is clean, well-structured, and readily available, reducing the time from question to insight.\nScale Your Data Efforts: Tackle larger datasets and more complex problems with confidence, knowing that your data infrastructure is robust and scalable.\nBridge the Gap Between Data and Decision-Making: Play that role in translating data insights into strategic actions, making your contributions invaluable to any team.\nYou don’t want to sit and wait: If you provide to your employer a solution more than waiting for the Data Engineer to give you the data you need to work, what if you could provide that solution as well, not having to depend a 100% on someone else gives you freedome and a higher value in the market.",
    "crumbs": [
      "Senior Project",
      "Data Engineering for DS"
    ]
  },
  {
    "objectID": "Projects/project1.html#your-path-to-becoming-a-data-science-hybrid-with-data-engineering",
    "href": "Projects/project1.html#your-path-to-becoming-a-data-science-hybrid-with-data-engineering",
    "title": "Senior Project Post 1",
    "section": "",
    "text": "Embracing SQL and and some other data engineering concepts is not just about enlarging your technical toolbox; it’s about expanding your problem-solving skillset. In a data-driven world, these skills enable you to navigate and make sense of the multiple situations, turning data into information which will become decisions that drive progress.",
    "crumbs": [
      "Senior Project",
      "Data Engineering for DS"
    ]
  },
  {
    "objectID": "Projects/project1.html#top-industry",
    "href": "Projects/project1.html#top-industry",
    "title": "Senior Project Post 1",
    "section": "Top industry",
    "text": "Top industry\nLets take a look at two positions open at TESLA for an internship, they are in the same job post but they ask for two different positions.\n \nIf we were to look into a Ven Diagram we could picture Data Science on one of the sides and Data Engineering in the other, this is it could be taken from out the descriptions and put it into the Data Science or Data Engineering sides.\n\nSkills and Tasks Specific to a Data Scientist:\n\nStatistical Learning and Bayesian Models: The mention of statistical learning and Bayesian models is more aligned with the data scientist role, where a deep understanding of statistical methods and the ability to apply these methods to extract insights from data is crucial.\nAnswering Complex Questions on Fleet Usage and Behavior: This involves not just accessing and processing data but also interpreting it in a way that provides actionable insights, which is a core part of a data scientist’s role.\nMLOps Skills: While MLOps can be relevant to both roles, in the context of deploying and managing machine learning models, it leans more towards the data scientist side, especially when it involves model development, validation, and iteration based on statistical principles.\n\n\n\nSkills and Tasks Specific to a Data Engineer:\n\nBuilding Scalable Data Pipelines: The emphasis on building scalable data pipelines for deploying fleet health monitoring models is a essential data engineering task, focusing on the infrastructure that allows for efficient data flow and processing.\nData ELT Pipelines Using Airflow: Creating and maintaining data Extract, Load, Transform (ELT) pipelines, especially with tools like Airflow, is a key data engineering responsibility, ensuring that data is regularly and reliably prepared for analysis.\nQuery Optimization and Advanced SQL: While SQL is used by both data scientists and data engineers, the focus on optimizing and undertaking advanced SQL queries, especially on massive datasets, is more characteristic of data engineering, which often deals with the optimization of data access and processing.\n\n\n\nSkills and Tasks Shared Between Both Roles:\n\nData Analytics and Infrastructure: Both roles involve a level of responsibility for employing data to drive insights and actions, which includes the infrastructure that supports data analytics.\nUse of Tableau and Similar Tools for Dashboards: The creation and maintenance of analytics dashboards are common to both roles, although the focus might differ; data scientists would be more involved in the interpretation of the data, while data engineers would ensure the data is accurately and efficiently presented.\nWorking with Cross-Functional Teams: Collaboration with cross-functional teams to support design cycles, provide insights, and support decision-making is essential for both roles, though the nature of the support and insights provided may vary.\n\n\n\nThe difference\nThe job description establishes a range for the payment, it goes from $20-60 an hour. Just imagine that for being able to understand skills like ETL, stablishing a pipeline (Which is ETL), and sharp your SQL could give you 3 times more money, and you could fix your problems in less time than waiting for someone to complement this tasks, if you are hired of course.",
    "crumbs": [
      "Senior Project",
      "Data Engineering for DS"
    ]
  },
  {
    "objectID": "Projects/project1.html#difference-on-the-data-structures",
    "href": "Projects/project1.html#difference-on-the-data-structures",
    "title": "Senior Project Post 1",
    "section": "Difference on the Data Structures",
    "text": "Difference on the Data Structures\n\nData Lake\n\nAttributes:\nA Data Lake is a vast pool of raw, unstructured, and structured data stored in its native format. It is designed to store a large amount of data without a predefined schema, allowing you to keep all your data in one place without having to structure it first.\n\n\nBenefits:\n\nFlexibility: You can store data of all types and structures, making it highly adaptable to changes.\nScalability: Capable of handling massive volumes of data, from gigabytes to petabytes.\nCost-Effectiveness: Often built on inexpensive storage solutions, making it economical for storing vast amounts of data.\n\n\n\nUses:\n\nIdeal for big data and real-time analytics projects.\nUseful for organizations that want to store all their data without initially knowing how they will use it.\n\n\n\n\nData Warehouse\n\nAttributes:\nA Data Warehouse is a centralized repository for structured, filtered data that has already been processed for a specific purpose. It is highly structured and uses a schema-on-write approach, meaning the data is organized and formatted at the time of entry.\n\n\nBenefits:\n\nImproved Data Quality and Consistency: Due to its structured nature, data is cleaned and transformed, ensuring reliability.\nPerformance: Optimized for complex queries and data analysis, providing fast access to insights.\nHistorical Intelligence: Ideal for storing historical data, enabling trend analysis over time.\n\n\n\nUses:\n\nSuitable for business intelligence, reporting, and data analysis purposes.\nUsed by organizations that need to analyze their data comprehensively to make informed decisions.\n\n\n\n\nData Mart\n\nAttributes:\nA Data Mart is a subset of a data warehouse designed to focus on a specific line of business, department, or subject area. It is more tailored and subject-oriented, containing only the data relevant to a particular group or purpose.\n\n\nBenefits:\n\nIncreased Accessibility: More accessible to users due to its focus on a specific domain, making it easier to retrieve relevant information.\nFaster Query Performance: Smaller size leads to quicker data retrieval, improving efficiency.\nUser-Friendly: Easier for non-technical users to interact with and understand since it’s focused on a specific area.\n\n\n\nUses:\n\nIdeal for department-specific reports and analyses, such as sales, finance, or marketing data.\nUsed by departments within organizations that need quick, easy access to specific, relevant data.\n\n\n\n\nData Fabric\n\nAttributes:\n\nData Fabric provides a unified layer of data and connectivity across different platforms and environments, using various technologies and approaches (like metadata).\nIt is designed to provide consistent capabilities across endpoints in a hybrid and multi-cloud environment.\n\n\n\nBenefits:\n\nAgility: Enables quick access to data across the organization, regardless of its location.\nInteroperability: Facilitates seamless data sharing and integration across diverse systems and platforms.\n\n\n\nUses:\n\nIdeal for organizations with complex data ecosystems looking to streamline data access, integration, and management across multiple environments.",
    "crumbs": [
      "Senior Project",
      "Data Engineering for DS"
    ]
  },
  {
    "objectID": "Projects/project1.html#etl-extract-transform-load",
    "href": "Projects/project1.html#etl-extract-transform-load",
    "title": "Senior Project Post 1",
    "section": "ETL (Extract, Transform, Load)",
    "text": "ETL (Extract, Transform, Load)\n\nExtract: Data is sourced from various origins, such as databases, CRM systems, flat files, or APIs. This stage involves querying data sources, often using SQL for structured databases, and aggregating the data for further processing.\nTransform: This critical phase involves a series of operations such as cleansing (removing or correcting data errors), deduplication (eliminating duplicate records), normalization (structuring data to reduce redundancy), and aggregation (summarizing detailed data). Complex transformations might include pivoting data formats, merging datasets, or applying business logic to derive new calculated fields. This step is usually performed in a dedicated staging area or a transformation engine, using tools and languages suitable for data manipulation, including SQL, Python, or specialized ETL tools.\nLoad: The final step involves writing the transformed data into a target data warehouse or database. This phase is carefully managed to maintain data integrity and optimize for query performance, often involving SQL operations to insert data into structured schemas designed for analysis.",
    "crumbs": [
      "Senior Project",
      "Data Engineering for DS"
    ]
  },
  {
    "objectID": "Projects/project1.html#advantages-and-challenges",
    "href": "Projects/project1.html#advantages-and-challenges",
    "title": "Senior Project Post 1",
    "section": "Advantages and Challenges:",
    "text": "Advantages and Challenges:\n\nControl and Quality: The ETL process allows for extensive control over the transformation logic, ensuring data quality and consistency. This is particularly important in regulated industries or where data accuracy is critical.\nPerformance Consideration: ETL can relieve the target data warehouse from the processing load of transformations. However, the transformation phase can become a bottleneck if not well-optimized, especially with large volumes of data.\nTooling and Complexity: While ETL offers robustness and control, it can also introduce complexity, requiring specialized tools and expertise to manage the transformation logic and workflows.",
    "crumbs": [
      "Senior Project",
      "Data Engineering for DS"
    ]
  },
  {
    "objectID": "Projects/project1.html#when-to-use-etl",
    "href": "Projects/project1.html#when-to-use-etl",
    "title": "Senior Project Post 1",
    "section": "When to Use ETL:",
    "text": "When to Use ETL:\n\nWhen data quality and transformation logic are complex and need to be managed meticulously.\nIn environments where the computational capabilities of the target data warehouse are limited or costly.\nWhen data processing needs to be done before loading to ensure compliance, security, or data privacy.",
    "crumbs": [
      "Senior Project",
      "Data Engineering for DS"
    ]
  },
  {
    "objectID": "Projects/project1.html#advantages-and-challenges-1",
    "href": "Projects/project1.html#advantages-and-challenges-1",
    "title": "Senior Project Post 1",
    "section": "Advantages and Challenges:",
    "text": "Advantages and Challenges:\n\nScalability and Flexibility: ELT leverages the scalable compute resources of modern cloud-based data warehouses, handling vast amounts of data efficiently. The flexibility to transform data on-the-fly supports agile analytics and data exploration.\nSpeed: By loading data directly into the warehouse, ELT can make data available for analysis more quickly. This is advantageous in fast-paced environments where timely insights are critical.\nSimplification and Cost: ELT can simplify the data pipeline by reducing the need for a separate transformation layer, potentially lowering infrastructure and maintenance costs. However, this approach requires a robust data warehouse capable of handling intensive compute tasks.",
    "crumbs": [
      "Senior Project",
      "Data Engineering for DS"
    ]
  },
  {
    "objectID": "Projects/project1.html#when-to-use-elt",
    "href": "Projects/project1.html#when-to-use-elt",
    "title": "Senior Project Post 1",
    "section": "When to Use ELT:",
    "text": "When to Use ELT:\n\nIn big data scenarios where the volume, velocity, and variety of data exceed traditional database capabilities.\nWhen using cloud-based data warehouses with high processing power, where the cost of compute resources is outweighed by the need for flexibility and speed.\nIn agile environments where the requirements for data models and analyses change frequently, necessitating a flexible approach to data transformation.",
    "crumbs": [
      "Senior Project",
      "Data Engineering for DS"
    ]
  },
  {
    "objectID": "Projects/project1.html#the-ongoing-debate",
    "href": "Projects/project1.html#the-ongoing-debate",
    "title": "Senior Project Post 1",
    "section": "The Ongoing Debate",
    "text": "The Ongoing Debate\nThe debate between ETL and ELT often centers on trade-offs between control and flexibility, scalability and complexity, and the upfront costs versus long-term benefits. The right choice depends on specific project requirements, data characteristics, available infrastructure, and strategic priorities.\nETL is traditionally preferred for scenarios where data integrity, privacy, and compliance are paramount, and where transformations need to be tightly controlled and monitored. It’s ideal for environments where the cleanliness and structure of the data are critical before it can be stored or analyzed, such as in financial services or healthcare sectors where data accuracy and privacy are non-negotiable.\nELT, on the other hand, is favored in contexts where the agility and flexibility of data analysis are prioritized. The ability to store raw data and transform it as needed allows for a more explorative and iterative approach to data analytics. This is particularly beneficial in dynamic industries like e-commerce or social media, where business needs can change rapidly, and the ability to pivot and explore new data models quickly is a competitive advantage.",
    "crumbs": [
      "Senior Project",
      "Data Engineering for DS"
    ]
  },
  {
    "objectID": "Projects/project1.html#practical-examples",
    "href": "Projects/project1.html#practical-examples",
    "title": "Senior Project Post 1",
    "section": "Practical Examples",
    "text": "Practical Examples\nETL Example: A financial institution might use ETL to integrate customer transaction data from various sources. The transformation stage would include validating transaction codes, converting currencies, and anonymizing personal information to comply with data protection regulations before loading the clean, transformed data into a data warehouse for analysis and reporting.\nELT Example: A digital marketing firm might use ELT to analyze web traffic data. Raw clickstream data is ingested directly into a cloud-based data warehouse, and SQL queries are used to transform this data on-demand, creating different views for analyzing user behavior, campaign performance, and website engagement.",
    "crumbs": [
      "Senior Project",
      "Data Engineering for DS"
    ]
  },
  {
    "objectID": "ml.html",
    "href": "ml.html",
    "title": "about me",
    "section": "",
    "text": "MarkDown Basics"
  },
  {
    "objectID": "ml.html#title-2-header",
    "href": "ml.html#title-2-header",
    "title": "about me",
    "section": "",
    "text": "MarkDown Basics"
  },
  {
    "objectID": "exploration.html",
    "href": "exploration.html",
    "title": "about me",
    "section": "",
    "text": "MarkDown Basics"
  },
  {
    "objectID": "exploration.html#title-2-header",
    "href": "exploration.html#title-2-header",
    "title": "about me",
    "section": "",
    "text": "MarkDown Basics"
  },
  {
    "objectID": "cleansing.html",
    "href": "cleansing.html",
    "title": "about me",
    "section": "",
    "text": "MarkDown Basics"
  },
  {
    "objectID": "cleansing.html#title-2-header",
    "href": "cleansing.html#title-2-header",
    "title": "about me",
    "section": "",
    "text": "MarkDown Basics"
  },
  {
    "objectID": "bi_projects/twilio.html",
    "href": "bi_projects/twilio.html",
    "title": "Twilio",
    "section": "",
    "text": "This project aimed to provide my supervisor with a comprehensive understanding of the costs associated with messaging generated by each department at BYU-Idaho. The primary goal was to visualize these costs in a user friendly dashboard, breaking down the number of messages sent, the associated fees, and the types and frequency of errors returned by the serer. Another key objective was to analyze opt-out patterns, particularly in relation to specific departments’ communications, helping identify areas where student opting out was happening."
  },
  {
    "objectID": "bi_projects/twilio.html#project-overview",
    "href": "bi_projects/twilio.html#project-overview",
    "title": "Twilio",
    "section": "",
    "text": "This project aimed to provide my supervisor with a comprehensive understanding of the costs associated with messaging generated by each department at BYU-Idaho. The primary goal was to visualize these costs in a user friendly dashboard, breaking down the number of messages sent, the associated fees, and the types and frequency of errors returned by the serer. Another key objective was to analyze opt-out patterns, particularly in relation to specific departments’ communications, helping identify areas where student opting out was happening."
  },
  {
    "objectID": "bi_projects/twilio.html#challenges-and-constraints",
    "href": "bi_projects/twilio.html#challenges-and-constraints",
    "title": "Twilio",
    "section": "Challenges and Constraints",
    "text": "Challenges and Constraints\nThe dashboard required integrating data from three separate datasets:\n\nMessage Log Dataset: This dataset captured all sent and received text messages. Initially, this was a manual process, but we later automated it by establishing a connection to the Twilio API and storing the data in a database that could be periodically updated.\nInvoice Dataset: The dataset included all the charges incurred by the university. Unfortunately, there was no API available, so the data had to be manually extracted as a CSV file each month. This dataset was particularly challenging to work with due to duplicated data, mismatched formatting between the CSV and PDF versions of the invoice, and inconsistencies in the fee breakdowns, such as taxes and processing fees not being included in the message log.\nPhone Number and Department Dataset: This dataset was maintained in an Excel file and contained the frequently used phone numbers across departments. The file allowed updates through Microsoft Teams, ensuring accuracy in linking phone numbers to their respective departments.\n\n\nKey Issues\nIntegrating the datasets presented several challenges:\n\nThe price data from the message log did not account for additional fees and taxes applied in the invoice.\n\nMessage counts from the log did not match invoice totals, requiring extensive data wrangling to clean duplicates and standardize entries.\n\nThere was no shared key between the log and invoice datasets, making it difficult to reconcile the data across systems."
  },
  {
    "objectID": "bi_projects/twilio.html#data-wrangling-and-integration-process",
    "href": "bi_projects/twilio.html#data-wrangling-and-integration-process",
    "title": "Twilio",
    "section": "Data Wrangling and Integration Process",
    "text": "Data Wrangling and Integration Process\nTo tackle these issues, I implemented the following strategies:\n\nPrice Adjustment Column: I calculated the true message costs by creating a column in the log data that incorporated additional fees and taxes from the invoice. For this, I created a new measure using DAX to represent the messages as a percentage of the total costs, allowing me to allocate the invoice costs accurately to each department.\n\nMessage Count Alignment: Cleaning the duplicate entries and standardizing message counts helped match numbers between the datasets, providing a more accurate reflection of costs.\n\nEstablishing a Surrogate Key: I created a surrogate key using the year and month from the invoice, linking it to the message data. This allowed us to synchronize the messages with the correct month of the invoice and establish communication between the datasets.\n\nError Tracking: The errors encountered during message delivery were tracked using Twilio error codes. I created a lookup table to decode these error codes, giving departments insights into why students might be opting out and which errors were contributing to unnecessary costs.\n\n\nHere is how the connection look between all the tables after I finished doing the wrangling\n\n\n\nimage"
  },
  {
    "objectID": "bi_projects/twilio.html#the-dashboard",
    "href": "bi_projects/twilio.html#the-dashboard",
    "title": "Twilio",
    "section": "The Dashboard",
    "text": "The Dashboard\nThe dashboard consists of four pages, one of the most important features are the slicers that provide dig to the most specific details of the searches that my supervisor wantanded. Each of the pages provices an specific view of the messaging data and the quesitons we had to answer:\nPage 1: Cost Summary by Account\nThis page presents a high-level overview of the total costs for each of BYU-Idaho’s Twilio accounts. It includes:\n\nTotal cost per account, broken down by month and year.\n\nTaxes and fees applied to each account, offering clarity on how charges accumulate over time.\n\n\nTo create this view, I linked the message log with the invoice data by the year and month, allowing us to track how much each department was spending monthly, inclusive of tax and processing fees.\n\n\nPage 2: Departmental Messaging and Cost Breakdown\nThis page focuses on departmental performance, detailing:\n\nThe number of messages sent and received by each department.\n\nA breakdown of the cost after fees per department.\n\nThe percentage of total messaging costs that each department accounts for.\n\n\nI calculated the percentage of the total cost per department by creating a measure to multiply the total invoice by the department’s share of messages, helping us accurately allocate costs. The DAX measure I created was crucial in making this percentage calculation, ensuring an even distribution of fees.\n\n\nPage 3: Error Analysis and Opt-Out Monitoring\nErrors generate unnecessary costs, and this slide helps:\n\nIdentify which departments are responsible for messaging errors and their associated costs.\n\nDisplay a list of phone numbers that have opted out, allowing us to avoid future errors by excluding these numbers from messaging campaigns.\n\n\nThe error data was pulled in through the Twilio API, and I matched error codes with the message logs to pinpoint which departments were causing issues. I also generated a list of phone numbers that had opted out, which helped departments better understand engagement challenges.\n\n\nPage 4: Detailed Error Breakdown by Message\nThis page provides deeper insights into the errors, showing:\n\nThe type of error (via Twilio’s error codes), along with the specific phone number affected.\n\nWhen the error occurred and the content of the message sent.\n\n\nThis page is particularly useful for tracking opt-out reasons, as it allowed us to investigate if the content of certain messages was driving users away. I built search functionality within this slide to trace specific phone numbers and understand their message history."
  },
  {
    "objectID": "bi_projects/iWork.html",
    "href": "bi_projects/iWork.html",
    "title": "iWorq Solutions",
    "section": "",
    "text": "iWorQ, a leading software provider for local government agencies, tasked my team with enhancing an existing dashboard that tracks usage and account activity for their various applications. The goal was to build on the prior work and introduce new functionality to help iWorQ make data-driven decisions about user engagement, cancellations, and active usage trends."
  },
  {
    "objectID": "bi_projects/iWork.html#executive-summary",
    "href": "bi_projects/iWork.html#executive-summary",
    "title": "iWorq Solutions",
    "section": "",
    "text": "iWorQ, a leading software provider for local government agencies, tasked my team with enhancing an existing dashboard that tracks usage and account activity for their various applications. The goal was to build on the prior work and introduce new functionality to help iWorQ make data-driven decisions about user engagement, cancellations, and active usage trends."
  },
  {
    "objectID": "bi_projects/iWork.html#project-scope",
    "href": "bi_projects/iWork.html#project-scope",
    "title": "iWorq Solutions",
    "section": "Project Scope",
    "text": "Project Scope\nThe key objectives of the project were:\n\nClickable application types: Provide the ability to view usage data by application (e.g., Permit, Code Enforcement), with details on accounts using each application frequently or infrequently.\n\nSpikes in usage: Identify changes in usage behavior, such as sudden spikes or drop-offs.\n\nMost active accounts: Highlight accounts that are highly engaged across multiple applications.\n\nUser Information: Enable users to access contact details and usage information for each account and application.\n\nAutomated Updates: Implement real-time or periodic (weekly/monthly) updates to ensure the data reflects the latest user activity and cancellations.\n\nCancellation and inactivity tracking: Accurately account for cancellations, ensuring inactive applications are not mistaken for under-utilization."
  },
  {
    "objectID": "bi_projects/iWork.html#technical-process",
    "href": "bi_projects/iWork.html#technical-process",
    "title": "iWorq Solutions",
    "section": "Technical Process",
    "text": "Technical Process\nWe used Python, specifically the awswrangler and boto3 libraries, to pull data from an AWS S3 bucket. The S3 bucket contained partitioned Parquet files with historical usage data, spanning over two years. The code I developed allowed us to extract this data efficiently by filtering the relevant data using year and week partitions, ensuring only necessary data was retrieved and processed.\nThe filtering logic was crucial, as it ensured that data from the appropriate time frame (two years back) was pulled without retrieving unnecessary records. The following snippet illustrates the filtering process for pulling data from the S3 bucket:"
  },
  {
    "objectID": "bi_projects/iWork.html#dashboard-features",
    "href": "bi_projects/iWork.html#dashboard-features",
    "title": "iWorq Solutions",
    "section": "Dashboard Features",
    "text": "Dashboard Features\nThe final dashboard provides:\n\nOverview of Usage Data: For each application, users can view which accounts are using it the most or the least.\n\nActive Accounts and Users: A list of the most active accounts, including detailed information about which applications are being used and the key contacts associated with each.\n\nTrend Identification: The dashboard automatically flags usage trends, showing increases or decreases in account activity.\n\nAutomated Data Refresh: The dashboard is updated on a scheduled basis (weekly or monthly), pulling the latest data from the S3 bucket to reflect real-time usage."
  },
  {
    "objectID": "BI.html",
    "href": "BI.html",
    "title": "BI Tools",
    "section": "",
    "text": "Throughout my experience as a Data Scientist, I’ve participated in a variety of projects that required the development of dashboards and interactive reports using Business Intelligence (BI) tools. These tools, such as Tableau, Power BI, and Looker, have become an indispensable part of my data products to deliver.\nOne of the primary strengths of BI tools is their ability to create the entire data pipeline, from data extraction, transformation, and loading (ETL), to the final stage of visualization and reporting. This integration allows having a great data handling, enabling to pull data from multiple sources, clean and manipulate it, and then present it in an interactive, visually engaging manner. The ability to manage both the back-end ETL process and the front-end reporting in one platform helps a lot with the process.\nYou will find some of the tools I have worked with and links to some narrative of the project with some of the process and some snippets to showcase the dashboard."
  },
  {
    "objectID": "BI.html#why-i-use-bi-tools",
    "href": "BI.html#why-i-use-bi-tools",
    "title": "BI Tools",
    "section": "",
    "text": "Throughout my experience as a Data Scientist, I’ve participated in a variety of projects that required the development of dashboards and interactive reports using Business Intelligence (BI) tools. These tools, such as Tableau, Power BI, and Looker, have become an indispensable part of my data products to deliver.\nOne of the primary strengths of BI tools is their ability to create the entire data pipeline, from data extraction, transformation, and loading (ETL), to the final stage of visualization and reporting. This integration allows having a great data handling, enabling to pull data from multiple sources, clean and manipulate it, and then present it in an interactive, visually engaging manner. The ability to manage both the back-end ETL process and the front-end reporting in one platform helps a lot with the process.\nYou will find some of the tools I have worked with and links to some narrative of the project with some of the process and some snippets to showcase the dashboard."
  },
  {
    "objectID": "BI.html#power-bi",
    "href": "BI.html#power-bi",
    "title": "BI Tools",
    "section": "Power BI",
    "text": "Power BI\n\nTwilio\nCentenario Rent a Car\niWork Systems"
  },
  {
    "objectID": "bi_projects/Centenario.html",
    "href": "bi_projects/Centenario.html",
    "title": "Centenario Rent a Car",
    "section": "",
    "text": "Centenario Rent a Car, is a company offering both car rentals and deluxe taxi services across the U.S.-Mexico border, they needed an automated dashboard to track multiple business metrics, such as income sources, car usage, and driver activity. They are located in Mexicali, Mexico, and they travel to key U.S. airports in San Diego, Los Angeles, Phoenix, and Yuma. The goal of the project was to wrangle five years of non-tidy Excel data and create a Power BI dashboard that could provide actionable insights to improve their business operations."
  },
  {
    "objectID": "bi_projects/Centenario.html#centenario-rental-car",
    "href": "bi_projects/Centenario.html#centenario-rental-car",
    "title": "Centenario Rent a Car",
    "section": "",
    "text": "Centenario Rent a Car, is a company offering both car rentals and deluxe taxi services across the U.S.-Mexico border, they needed an automated dashboard to track multiple business metrics, such as income sources, car usage, and driver activity. They are located in Mexicali, Mexico, and they travel to key U.S. airports in San Diego, Los Angeles, Phoenix, and Yuma. The goal of the project was to wrangle five years of non-tidy Excel data and create a Power BI dashboard that could provide actionable insights to improve their business operations."
  },
  {
    "objectID": "bi_projects/Centenario.html#key-contributions",
    "href": "bi_projects/Centenario.html#key-contributions",
    "title": "Centenario Rent a Car",
    "section": "Key Contributions",
    "text": "Key Contributions\n\nData Wrangling: The provided data was stored in various Excel sheets that were untidy and disorganized. Using Python, I cleaned and consolidated the data into a tidy format, appending tables that detailed trips, cars, drivers, and destinations.\n\nPrice Calculation Function: The car pricing depended on several factors—year, car type, and destination. I created a Python function that dynamically matched these variables to calculate the final price of each trip, ensuring accurate data integration in the final dashboard."
  },
  {
    "objectID": "bi_projects/Centenario.html#the-dashboard",
    "href": "bi_projects/Centenario.html#the-dashboard",
    "title": "Centenario Rent a Car",
    "section": "The Dashboard",
    "text": "The Dashboard\nAfter cleaning and organizing the data, I used Power BI to automate the dashboard. This provided insights such as:\n* Total trips made\n* Total revenue generated\n* Breakdown of trips by destination, car type, and company served\n* Visuals showing trends over time for car usage and income sources"
  },
  {
    "objectID": "bi_projects/Centenario.html#tools-used",
    "href": "bi_projects/Centenario.html#tools-used",
    "title": "Centenario Rent a Car",
    "section": "Tools Used:",
    "text": "Tools Used:\n\nPython: Data cleaning and transformation\nPower BI: Dashboard creation and automation\nExcel: Initial data source"
  },
  {
    "objectID": "bi_projects/Centenario.html#the-dashboard-1",
    "href": "bi_projects/Centenario.html#the-dashboard-1",
    "title": "Centenario Rent a Car",
    "section": "The Dashboard",
    "text": "The Dashboard\nThe final dashboard provided Centenario Rental Car with a user friendly, real time tool for tracking key business metrics, leading to better decision-making and operational efficiency."
  },
  {
    "objectID": "bi_projects/project5.html",
    "href": "bi_projects/project5.html",
    "title": "Client Report - [Insert Project Title]",
    "section": "",
    "text": "paste your elevator pitch here A SHORT (4-5 SENTENCES) PARAGRAPH THAT DESCRIBES KEY INSIGHTS TAKEN FROM METRICS IN THE PROJECT RESULTS THINK TOP OR MOST IMPORTANT RESULTS.\n\n\nRead and format project data\n# Include and execute your code here\ndf = pd.read_csv(\"https://github.com/byuidatascience/data4names/raw/master/data-raw/names_year/names_year.csv\")\n\n\nHighlight the Questions and Tasks"
  },
  {
    "objectID": "bi_projects/project5.html#elevator-pitch",
    "href": "bi_projects/project5.html#elevator-pitch",
    "title": "Client Report - [Insert Project Title]",
    "section": "",
    "text": "paste your elevator pitch here A SHORT (4-5 SENTENCES) PARAGRAPH THAT DESCRIBES KEY INSIGHTS TAKEN FROM METRICS IN THE PROJECT RESULTS THINK TOP OR MOST IMPORTANT RESULTS.\n\n\nRead and format project data\n# Include and execute your code here\ndf = pd.read_csv(\"https://github.com/byuidatascience/data4names/raw/master/data-raw/names_year/names_year.csv\")\n\n\nHighlight the Questions and Tasks"
  },
  {
    "objectID": "bi_projects/project5.html#questiontask-1",
    "href": "bi_projects/project5.html#questiontask-1",
    "title": "Client Report - [Insert Project Title]",
    "section": "QUESTION|TASK 1",
    "text": "QUESTION|TASK 1\nCOPY PASTE QUESTION|TASK 1 FROM THE PROJECT HERE\ntype your results and analysis here\n\n\nRead and format data\n# Include and execute your code here\n\n\ninclude figures in chunks and discuss your findings in the figure.\n::: {#cell-Q1 chart .cell execution_count=4}\n\nplot example\n# Include and execute your code here\nchart = px.bar(df.head(200),\n    x=\"name\", \n    y=\"AK\"\n)\nchart.show()\n\n\n                                                \nMy useless chart\n\n:::\n::: {#cell-Q1 table .cell .tbl-cap-location-top tbl-cap=‘Not much of a table’ execution_count=5}\n\ntable example\n# Include and execute your code here\nmydat = df.head(1000)\\\n    .groupby('year')\\\n    .sum()\\\n    .reset_index()\\\n    .tail(10)\\\n    .filter([\"year\", \"AK\",\"AR\"])\n\ndisplay(mydat)\n\n\n\n\n\n\n\n\n\nyear\nAK\nAR\n\n\n\n\n96\n2006\n21.0\n183.0\n\n\n97\n2007\n28.0\n153.0\n\n\n98\n2008\n36.0\n212.0\n\n\n99\n2009\n34.0\n179.0\n\n\n100\n2010\n22.0\n196.0\n\n\n101\n2011\n41.0\n148.0\n\n\n102\n2012\n28.0\n140.0\n\n\n103\n2013\n26.0\n134.0\n\n\n104\n2014\n20.0\n114.0\n\n\n105\n2015\n28.0\n121.0\n\n\n\n\n\n\n:::"
  },
  {
    "objectID": "bi_projects/project5.html#questiontask-2",
    "href": "bi_projects/project5.html#questiontask-2",
    "title": "Client Report - [Insert Project Title]",
    "section": "QUESTION|TASK 2",
    "text": "QUESTION|TASK 2\nCOPY PASTE QUESTION|TASK 2 FROM THE PROJECT HERE\ntype your results and analysis here\n\n\nRead and format data\n# Include and execute your code here\n\n\ninclude figures in chunks and discuss your findings in the figure.\n::: {#cell-Q2 chart .cell execution_count=7}\n\nplot example\n# Include and execute your code here\nchart = px.bar(df.head(200),\n    x=\"name\", \n    y=\"AK\"\n)\nchart.show()\n\n\n                                                \nMy useless chart\n\n:::\n::: {#cell-Q2 table .cell .tbl-cap-location-top tbl-cap=‘Not much of a table’ execution_count=8}\n\ntable example\n# Include and execute your code here\nmydat = df.head(1000)\\\n    .groupby('year')\\\n    .sum()\\\n    .reset_index()\\\n    .tail(10)\\\n    .filter([\"year\", \"AK\",\"AR\"])\n\ndisplay(mydat)\n\n\n\n\n\n\n\n\n\nyear\nAK\nAR\n\n\n\n\n96\n2006\n21.0\n183.0\n\n\n97\n2007\n28.0\n153.0\n\n\n98\n2008\n36.0\n212.0\n\n\n99\n2009\n34.0\n179.0\n\n\n100\n2010\n22.0\n196.0\n\n\n101\n2011\n41.0\n148.0\n\n\n102\n2012\n28.0\n140.0\n\n\n103\n2013\n26.0\n134.0\n\n\n104\n2014\n20.0\n114.0\n\n\n105\n2015\n28.0\n121.0\n\n\n\n\n\n\n:::"
  },
  {
    "objectID": "bi_projects/project5.html#questiontask-3",
    "href": "bi_projects/project5.html#questiontask-3",
    "title": "Client Report - [Insert Project Title]",
    "section": "QUESTION|TASK 3",
    "text": "QUESTION|TASK 3\nCOPY PASTE QUESTION|TASK 3 FROM THE PROJECT HERE\ntype your results and analysis here\n\n\nRead and format data\n# Include and execute your code here\n\n\ninclude figures in chunks and discuss your findings in the figure.\n::: {#cell-Q3 chart .cell execution_count=10}\n\nplot example\n# Include and execute your code here\nchart = px.bar(df.head(200),\n    x=\"name\", \n    y=\"AK\"\n)\nchart.show()\n\n\n                                                \nMy useless chart\n\n:::\n::: {#cell-Q3 table .cell .tbl-cap-location-top tbl-cap=‘Not much of a table’ execution_count=11}\n\ntable example\n# Include and execute your code here\nmydat = df.head(1000)\\\n    .groupby('year')\\\n    .sum()\\\n    .reset_index()\\\n    .tail(10)\\\n    .filter([\"year\", \"AK\",\"AR\"])\n\ndisplay(mydat)\n\n\n\n\n\n\n\n\n\nyear\nAK\nAR\n\n\n\n\n96\n2006\n21.0\n183.0\n\n\n97\n2007\n28.0\n153.0\n\n\n98\n2008\n36.0\n212.0\n\n\n99\n2009\n34.0\n179.0\n\n\n100\n2010\n22.0\n196.0\n\n\n101\n2011\n41.0\n148.0\n\n\n102\n2012\n28.0\n140.0\n\n\n103\n2013\n26.0\n134.0\n\n\n104\n2014\n20.0\n114.0\n\n\n105\n2015\n28.0\n121.0\n\n\n\n\n\n\n:::"
  },
  {
    "objectID": "competition.html",
    "href": "competition.html",
    "title": "about me",
    "section": "",
    "text": "MarkDown Basics"
  },
  {
    "objectID": "competition.html#title-2-header",
    "href": "competition.html#title-2-header",
    "title": "about me",
    "section": "",
    "text": "MarkDown Basics"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Hi I’m Daniel, and I am a Data Scientist",
    "section": "",
    "text": "Github\n  \n  \n    \n     LinkedIn\n  \n  \n    \n     Email\n  \n  \n    \n     (385) 328-1955\n  \n\n\n\n\nHey there, I’m Daniel Dominguez. As a kid growing up in Mexico City, I was always a keen observer of the world around me. I found that I was able to glean insights and patterns from the things that most people didn’t notice. This natural curiosity and analytical mind led me to the fascinating field of data science.\nTo me, data is like a story waiting to be told. Every set of data has its own narrative, and it’s my job to uncover the hidden stories and connections within it. As someone who is passionate about storytelling, I believe that data analysis is a powerful tool for understanding our world and making better decisions.\nI’m passionate about all things data-related, but what really drives me is the ability to predict scenarios based on past and present trends. It’s like being able to read the next chapter of a gripping story before anyone else! I’m constantly amazed by how the dots connect when we analyze data, and how insights gained from one area can inform and improve other areas.\nAs a problem-solver at heart, I relish the challenge of identifying issues and devising ways to tackle them. I’m always eager to put my skills to the test and find the best possible outcomes.\nSo, if you’re interested in exploring the world of data or just want to chat, feel free to reach out to me. Let’s see what we can create together!\n\nTHIS PORTFOLIO STILL UNDER CONSTRUCTION BUT YOU WILL BE ABLE TO FIND DIFFERENT PROJECTS I HAVE WORKED ON THAT I UPLOADED\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "Projects/1_snowpark_ml_data_ingest.html",
    "href": "Projects/1_snowpark_ml_data_ingest.html",
    "title": "1. Data Ingestion",
    "section": "",
    "text": "The diamonds dataset has been widely used in data science and machine learning. We will use it to demonstrate Snowflake’s native data science transformers in terms of database functionality and Spark & Pandas comportablity, using non-synthetic and statistically appropriate data that is well known to the ML community.\n\n\nOther connection options include Username/Password, MFA, OAuth, Okta, SSO. For more information, refer to the Python Connector documentation.\n\n\n\n\n# Snowpark for Python\nfrom snowflake.snowpark import Session\nfrom snowflake.snowpark.version import VERSION\nfrom snowflake.snowpark.types import StructType, StructField, DoubleType, StringType\nimport snowflake.snowpark.functions as F\n\n# data science libs\nimport numpy as np\n\n# misc\nimport json\n\n\nimport pandas as pd\n\n\n# Make a Snowpark Connection\n\n################################################################################################################\n#  You can also use the SnowSQL Client to configure your connection params:\n#  https://docs.snowflake.com/en/user-guide/snowsql-install-config.html\n#\n#  &gt;&gt;&gt; from snowflake.ml.utils import connection_params\n#  &gt;&gt;&gt; session = Session.builder.configs(connection_params.SnowflakeLoginOptions()\n#  &gt;&gt;&gt; ).create()   \n#\n#  NOTE: If you have named connection params then specify the connection name\n#  Example:\n#  \n#  &gt;&gt;&gt; session = Session.builder.configs(\n#  &gt;&gt;&gt; connection_params.SnowflakeLoginOptions(connection_name='connections.snowml')\n#  &gt;&gt;&gt; ).create()\n#\n#################################################################################################################\n\n# Edit the connection.json before creating the session object below\n# Create Snowflake Session object\nconnection_parameters = json.load(open('connection.json'))\nsession = Session.builder.configs(connection_parameters).create()\nsession.sql_simplifier_enabled = True\n\nsnowflake_environment = session.sql('SELECT current_user(), current_version()').collect()\nsnowpark_version = VERSION\n\n# Current Environment Details\nprint('\\nConnection Established with the following parameters:')\nprint('User                        : {}'.format(snowflake_environment[0][0]))\nprint('Role                        : {}'.format(session.get_current_role()))\nprint('Database                    : {}'.format(session.get_current_database()))\nprint('Schema                      : {}'.format(session.get_current_schema()))\nprint('Warehouse                   : {}'.format(session.get_current_warehouse()))\nprint('Snowflake version           : {}'.format(snowflake_environment[0][1]))\nprint('Snowpark for Python version : {}.{}.{}'.format(snowpark_version[0],snowpark_version[1],snowpark_version[2]))\n\n\nConnection Established with the following parameters:\nUser                        : LION\nRole                        : \"TRAINING_ROLE\"\nDatabase                    : \"HACKATHON_ALGORITHM_AVENGERS\"\nSchema                      : \"PROCUREMENT_ON_TIME_DELIVERY\"\nWarehouse                   : \"U\"\nSnowflake version           : 8.9.1\nSnowpark for Python version : 1.13.0\n\n\n\n# Show the file before loading\nsession.sql(\"DESCRIBE TABLE PROCUREMENT_ON_TIME_DELIVERY.PURCHASE_ORDER_HISTORY;\").show()\n\ndf = session.read.options({\"field_delimiter\": \",\",\n                                    \"field_optionally_enclosed_by\": '\"',\n                                    \"infer_schema\": True,\n                                    \"parse_header\": True}).table(\"PROCUREMENT_ON_TIME_DELIVERY.PURCHASE_ORDER_HISTORY\")\n\n# Select only three columns from the DataFrame\n#selected_df = df.select(\"PURCHASE_DOCUMENT_ID\", \"CREATE_DATE\", \"COMPANY_CODE_ID\")\n\n# Show the selected columns\n#selected_df.show()\n\nkoch = df.to_pandas()\n\n---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n|\"name\"                     |\"type\"             |\"kind\"  |\"null?\"  |\"default\"  |\"primary key\"  |\"unique key\"  |\"check\"  |\"expression\"  |\"comment\"  |\"policy name\"  |\"privacy domain\"  |\n---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n|PURCHASE_DOCUMENT_ID       |NUMBER(38,0)       |COLUMN  |Y        |NULL       |N              |N             |NULL     |NULL          |NULL       |NULL           |NULL              |\n|CREATE_DATE                |NUMBER(38,0)       |COLUMN  |Y        |NULL       |N              |N             |NULL     |NULL          |NULL       |NULL           |NULL              |\n|COMPANY_CODE_ID            |VARCHAR(16777216)  |COLUMN  |Y        |NULL       |N              |N             |NULL     |NULL          |NULL       |NULL           |NULL              |\n|VENDOR_ID                  |VARCHAR(16777216)  |COLUMN  |Y        |NULL       |N              |N             |NULL     |NULL          |NULL       |NULL           |NULL              |\n|POSTAL_CD                  |VARCHAR(16777216)  |COLUMN  |Y        |NULL       |N              |N             |NULL     |NULL          |NULL       |NULL           |NULL              |\n|RELEASE_DATE               |NUMBER(38,0)       |COLUMN  |Y        |NULL       |N              |N             |NULL     |NULL          |NULL       |NULL           |NULL              |\n|PURCHASE_DOCUMENT_ITEM_ID  |NUMBER(38,0)       |COLUMN  |Y        |NULL       |N              |N             |NULL     |NULL          |NULL       |NULL           |NULL              |\n|MATERIAL_ID                |NUMBER(38,0)       |COLUMN  |Y        |NULL       |N              |N             |NULL     |NULL          |NULL       |NULL           |NULL              |\n|SUB_COMMODITY_DESC         |VARCHAR(16777216)  |COLUMN  |Y        |NULL       |N              |N             |NULL     |NULL          |NULL       |NULL           |NULL              |\n|MRP_TYPE_ID                |NUMBER(38,0)       |COLUMN  |Y        |NULL       |N              |N             |NULL     |NULL          |NULL       |NULL           |NULL              |\n---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n\n\n\n\nkoch.to_csv(\"koch.csv\")\n\n\nimport pandas as pd\nimport numpy as np\n\ndate_columns = ['POR_DELIVERY_DATE', 'DELIVERY_DATE', 'REQUESTED_DELIVERY_DATE', 'FIRST_GR_POSTING_DATE','CREATE_DATE']\n\nfor column in date_columns:\n    temp_series = koch[column].fillna(0).astype(int).astype(str)\n    \n    # Replace '0' string back to NaN to avoid incorrect date conversion\n    temp_series = temp_series.replace('0', np.nan)\n    \n    # Convert to datetime\n    koch[column] = pd.to_datetime(temp_series, format='%Y%m%d', errors='coerce')\n\n#Creating target column\nkoch['Time_Difference'] = koch['FIRST_GR_POSTING_DATE'] - koch['DELIVERY_DATE']\n\nkoch['Time_Difference'] = koch['Time_Difference'].dt.days\n\nfallback_diff = (koch['REQUESTED_DELIVERY_DATE'] - koch['DELIVERY_DATE']).dt.days\n\n# Fill in Time_Difference where it's NaN with the fallback difference\nkoch['Time_Difference'] = np.where(koch['Time_Difference'].isna(), fallback_diff, koch['Time_Difference'])\n\ndef determine_status(days):\n    if days &lt; 0:\n        return 'Early'\n    elif days == 0:\n        return 'On Time'\n    else:\n        return 'Late'\n\n# Assuming 'Time_Difference' is your column with the date difference in days\nkoch['Arrival_Status'] = koch['Time_Difference'].apply(determine_status)\n\n\nrows = [22641, 115701, 358807, 484118, 489462, 524500, 524553, 524570,\n        524667, 524670, 524698, 592204, 614500, 745948, 947446]\n\n# Drop rows based on indices\nkoch = koch.drop(rows)\n\n\nkoch['country'] = koch['COMPANY_CODE_ID'].str.slice(0, 2)\nkoch['region'] = koch['COMPANY_CODE_ID'].str.slice(2, 4)\n\nkoch = koch[koch['DELIVERY_DATE'] &gt;= koch['CREATE_DATE']]\n\nfrom datetime import datetime, timedelta\n\n# Get tomorrow's date for comparison\ntomorrow = datetime.today().date() + timedelta(days=1)\n\n# Filter rows where 'DELIVERY_DATE' is tomorrow or in the future\nkoch = koch[koch['DELIVERY_DATE'].dt.date &lt;= tomorrow]\n\n\n\n\n\ndef determine_status(days):\n    if days &lt; 0:\n        return 'Early'\n    elif days == 0:\n        return 'On Time'\n    else:\n        return 'Late'\n\n# Assuming 'Time_Difference' is your column with the date difference in days\nkoch['Arrival_Status'] = koch['Time_Difference'].apply(determine_status)\n\n\nrows = [22641, 115701, 358807, 484118, 489462, 524500, 524553, 524570,\n        524667, 524670, 524698, 592204, 614500, 745948, 947446]\n\n# Drop rows based on indices\nkoch = koch.drop(rows)\n\n\nkoch['country'] = koch['COMPANY_CODE_ID'].str.slice(0, 2)\nkoch['region'] = koch['COMPANY_CODE_ID'].str.slice(2, 4)\n\n\nkoch.shape\n\n(1139392, 25)\n\n\n\nkoch = koch[koch['DELIVERY_DATE'] &gt;= koch['CREATE_DATE']]\n\nfrom datetime import datetime, timedelta\n\n# Get tomorrow's date for comparison\ntomorrow = datetime.today().date() + timedelta(days=1)\n\n# Filter rows where 'DELIVERY_DATE' is tomorrow or in the future\nkoch = koch[koch['DELIVERY_DATE'].dt.date &lt;= tomorrow]\n\n\nweiurdkoch[koch['DELIVERY_DATE'] &gt;= koch['CREATE_DATE']]\n\n\ngraph1 = koch.groupby(['DELIVERY_DATE', 'Arrival_Status']).size().reset_index(name='Count')\n\n# Sort the resulting DataFrame by 'DELIVERY_DATE'\ngraph1 = graph1.sort_values(by='DELIVERY_DATE', ascending=False)\n\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Assuming 'graph1' is your DataFrame\n\n# Create a FacetGrid with 'ARRIVAL_STATUS' for creating separate subplots for each unique value in 'ARRIVAL_STATUS'\ngraph_1 = sns.FacetGrid(graph1, col=\"Arrival_Status\", col_wrap=4, sharex=False, sharey=False)  # Adjust 'col_wrap' as needed\n\n# Map a lineplot onto each subplot by specifying the x and y axis and the 'hue' for coloring different 'Arrival_Status' within each subplot\ngraph_1.map(sns.lineplot, \"DELIVERY_DATE\", \"Count\", \"Arrival_Status\", marker=\"o\")\n\n# Adding a legend and adjusting subplot titles\ngraph_1.add_legend()\ngraph_1.set_titles(\"{col_name}\")\n\n# Optionally, adjust the layout so plots don't overlap\nplt.tight_layout()\n\n# Show the plot\nplt.show()\n\n\ngraph1 = koch.groupby(['DELIVERY_DATE', 'Arrival_Status']).size().reset_index(name='Count')\n\n# Sort the resulting DataFrame by 'DELIVERY_DATE'\ngraph1 = graph1.sort_values(by='DELIVERY_DATE', ascending=False)\n\ngraph1\n\n\n\n\n\n\n\n\nDELIVERY_DATE\nArrival_Status\nCount\n\n\n\n\n5821\n2024-03-03\nOn Time\n1\n\n\n5820\n2024-03-02\nEarly\n1\n\n\n5819\n2024-03-01\nOn Time\n1\n\n\n5818\n2024-02-29\nOn Time\n24\n\n\n5817\n2024-02-29\nEarly\n1\n\n\n...\n...\n...\n...\n\n\n5\n2018-09-09\nOn Time\n1\n\n\n2\n2018-09-08\nLate\n120\n\n\n3\n2018-09-08\nOn Time\n2\n\n\n1\n2018-09-07\nOn Time\n34\n\n\n0\n2018-09-07\nLate\n8\n\n\n\n\n5822 rows × 3 columns\n\n\n\n\n\noriginal_count = len(koch)\n\nweird_dates_df = koch[koch['DELIVERY_DATE'] &gt; koch['CREATE_DATE']]\nfiltered_count = len(weird_dates_df)\n\n# Calculate the number of dropped rows\ndropped_count = original_count - filtered_count\n\nprint(f\"Number of rows dropped: {dropped_count}\")\n\n\nNumber of rows dropped: 90962\n\n\n\n\noriginal_count = len(koch)\n# Get tomorrow's date for comparison\ntomorrow = datetime.today().date() + timedelta(days=1)\n\n# Filter rows where 'DELIVERY_DATE' is tomorrow or in the future\nmore_weird = koch[koch['DELIVERY_DATE'].dt.date &lt;= tomorrow]\n\n# Get the count of the filtered rows\nmore_weird_count = len(more_weird)\n\n# Calculate the number of rows dropped\ndropped_count_2 = original_count - more_weird_count\n\nprint(f\"Number of rows dropped: {dropped_count_2}\")\n\nNumber of rows dropped: 0\n\n\n\n# Group by 'DELIVERY_DATE' and 'Arrival_Status', and count the occurrences\ngraph_data = koch.groupby(['DELIVERY_DATE', 'Arrival_Status']).size().reset_index(name='Count')\n\n# Sort the resulting DataFrame by 'DELIVERY_DATE'\ngraph_data = graph_data.sort_values(by='DELIVERY_DATE')\n\ngraph_data\n\n\n\n\n\n\n\n\nDELIVERY_DATE\nArrival_Status\nCount\n\n\n\n\n0\n2018-09-07\nLate\n8\n\n\n1\n2018-09-07\nOn Time\n34\n\n\n2\n2018-09-08\nLate\n120\n\n\n3\n2018-09-08\nOn Time\n2\n\n\n4\n2018-09-09\nLate\n6\n\n\n...\n...\n...\n...\n\n\n5818\n2024-02-29\nOn Time\n24\n\n\n5817\n2024-02-29\nEarly\n1\n\n\n5819\n2024-03-01\nOn Time\n1\n\n\n5820\n2024-03-02\nEarly\n1\n\n\n5821\n2024-03-03\nOn Time\n1\n\n\n\n\n5822 rows × 3 columns\n\n\n\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Assuming 'graph1' is your DataFrame\n\n# Create a FacetGrid with 'ARRIVAL_STATUS' for creating separate subplots for each unique value in 'ARRIVAL_STATUS'\ngraph_1 = sns.FacetGrid(graph1, col=\"Arrival_Status\", col_wrap=4, sharex=False, sharey=False)  # Adjust 'col_wrap' as needed\n\n# Map a lineplot onto each subplot by specifying the x and y axis and the 'hue' for coloring different 'Arrival_Status' within each subplot\ngraph_1.map(sns.lineplot, \"DELIVERY_DATE\", \"Count\", \"Arrival_Status\", marker=\"o\")\n\n# Adding a legend and adjusting subplot titles\ngraph_1.add_legend()\ngraph_1.set_titles(\"{col_name}\")\n\n# Optionally, adjust the layout so plots don't overlap\nplt.tight_layout()\n\n# Show the plot\nplt.show()\n\n\n\n\n\n\n\n\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Define your custom color palette for each 'Arrival_Status'\ncolor_palette = {\n    \"On Time\": \"green\",  # Ensure this matches exactly with your DataFrame's 'Arrival_Status' values\n    \"Late\": \"red\",\n    \"Early\": \"blue\"  # Corrected 'golden' to 'gold', which is a valid color name\n}\n\n# Create a FacetGrid\ng = sns.FacetGrid(graph1, col=\"Arrival_Status\", col_wrap=4, sharex=False, sharey=False)\n\n# Use map_dataframe to apply sns.lineplot\ng.map_dataframe(sns.lineplot, x=\"DELIVERY_DATE\", y=\"Count\", hue=\"Arrival_Status\", palette=color_palette, alpha =.7)\n\n# Add a legend and adjust subplot titles\ng.add_legend(title=\"Arrival Status\")\ng.set_titles(\"{col_name}\")\n\n# Set custom axis labels\ng.set_axis_labels(\"Delivery Date\", \"Count of packages received\")\n\n# Set the overall title for the figure\ng.fig.suptitle('Distribution of number of packages delivered by the status', fontsize=16)\n\n# Optionally, adjust the layout\nplt.tight_layout()\n\n# Show the plot\nplt.show()\n\n\n\n\n\n\n\n\n\nimport pandas as pd\n\n# Ensure that 'CREATE_DATE' is a datetime column\nkoch['CREATE_DATE'] = pd.to_datetime(koch['CREATE_DATE'])\n# Extract the year from 'CREATE_DATE'\nkoch['Year'] = koch['CREATE_DATE'].dt.year\n\n# Calculate the average 'Time_Difference' per 'MATERIAL_ID' and 'Year'\nmaterial_delays = koch.groupby(['MATERIAL_ID', 'Year'])['Time_Difference'].mean().reset_index()\n\n# Get top 5 materials with the highest average delay per year\ntop_materials = material_delays.sort_values('Time_Difference', ascending=False).groupby('Year').head(5)\n\n# Get bottom 5 materials with the lowest average delay per year\nbottom_materials = material_delays.sort_values('Time_Difference', ascending=True).groupby('Year').head(5)\n\n\ntop_materials.head(5)\n\n\n\n\n\n\n\n\nMATERIAL_ID\nYear\nTime_Difference\n\n\n\n\n61623\n2.100009e+09\n2018\n1627.0\n\n\n63904\n2.100031e+09\n2019\n1489.0\n\n\n60666\n2.100008e+09\n2019\n1321.0\n\n\n60088\n2.100008e+09\n2020\n1175.5\n\n\n61849\n2.100009e+09\n2020\n1066.0\n\n\n\n\n\n\n\n\nbottom_materials.head(5)\n\n\n\n\n\n\n\n\nMATERIAL_ID\nYear\nTime_Difference\n\n\n\n\n44804\n1.100261e+09\n2021\n-585.0\n\n\n54072\n1.100275e+09\n2022\n-516.0\n\n\n54045\n1.100275e+09\n2022\n-516.0\n\n\n41686\n1.100255e+09\n2022\n-488.0\n\n\n41357\n1.100254e+09\n2018\n-480.5\n\n\n\n\n\n\n\n\n# Convert 'Delivery_Date' to datetime if it's not already\ngraph2 = graph1\n\n\n\n# Extract month name and year from 'Delivery_Date'\ngraph2['Month'] = graph2['DELIVERY_DATE2'].dt.month_name()\ngraph2['Year'] = graph2['DELIVERY_DATE2'].dt.year.astype(str)  # Convert year to string for categorical plotting\n\n\ngraph2\n\n\n\n\n\n\n\n\nDELIVERY_DATE\nArrival_Status\nCount\nDELIVERY_DATE2\nMonth\nYear\nMonth_Name\n\n\n\n\n5821\n2024-03-03\nOn Time\n1\n2024-03-03\nMarch\n2024\nMarch\n\n\n5820\n2024-03-02\nEarly\n1\n2024-03-02\nMarch\n2024\nMarch\n\n\n5819\n2024-03-01\nOn Time\n1\n2024-03-01\nMarch\n2024\nMarch\n\n\n5818\n2024-02-29\nOn Time\n24\n2024-02-29\nFebruary\n2024\nFebruary\n\n\n5817\n2024-02-29\nEarly\n1\n2024-02-29\nFebruary\n2024\nFebruary\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n5\n2018-09-09\nOn Time\n1\n2018-09-09\nSeptember\n2018\nSeptember\n\n\n2\n2018-09-08\nLate\n120\n2018-09-08\nSeptember\n2018\nSeptember\n\n\n3\n2018-09-08\nOn Time\n2\n2018-09-08\nSeptember\n2018\nSeptember\n\n\n1\n2018-09-07\nOn Time\n34\n2018-09-07\nSeptember\n2018\nSeptember\n\n\n0\n2018-09-07\nLate\n8\n2018-09-07\nSeptember\n2018\nSeptember\n\n\n\n\n5822 rows × 7 columns\n\n\n\n\n\n# Assuming 'df' is your DataFrame\n\n# Create a FacetGrid, with rows for each 'Arrival_Status'\ng = sns.FacetGrid(graph2, row='Arrival_Status', height=5, aspect=3, margin_titles=True)\n\n# Map the data to lineplots\ng.map(sns.lineplot, 'Month_Name', 'Count', 'Year', marker=\"o\", palette='viridis', ci=None)\n\ng.set_axis_labels('Months', 'Number of orders Delivered')\ng.set_titles('{row_name}')\ng.add_legend(title='Years')\ng.set_xticklabels(rotation=45)\nfor ax, status in zip(g.axes.flatten(), graph2['Arrival_Status'].unique()):\n    ax.set_title(f'Your Custom Subtitle Here for {status}', y=1.02) # You can adjust the y value as needed\n\n# Show plot\nplt.show()\n\n\n# Show plot\nplt.show()\n\n\n\nC:\\Users\\Dann_\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\seaborn\\axisgrid.py:848: FutureWarning: \n\nThe `ci` parameter is deprecated. Use `errorbar=None` for the same effect.\n\n  func(*plot_args, **plot_kwargs)\nC:\\Users\\Dann_\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\seaborn\\axisgrid.py:848: FutureWarning: \n\nThe `ci` parameter is deprecated. Use `errorbar=None` for the same effect.\n\n  func(*plot_args, **plot_kwargs)\nC:\\Users\\Dann_\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\seaborn\\axisgrid.py:848: FutureWarning: \n\nThe `ci` parameter is deprecated. Use `errorbar=None` for the same effect.\n\n  func(*plot_args, **plot_kwargs)\n\n\n\n\n\n\n\n\n\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Create the main graph\nplt.figure(figsize=(12, 6))\ng = sns.catplot(x='Month', y='Count', hue='Year', data=graph1, kind='bar', height=6, aspect=2, palette='muted', order=['January', 'February', 'March', 'April', 'May', 'June', 'July', 'August', 'September', 'October', 'November', 'December'])\ng.set_axis_labels(\"Month\", \"Count\")\ng.fig.suptitle('Monthly Shipments by Year')\n\n# Rotate x-axis labels for readability if needed\nplt.xticks(rotation=45)\n\n# Show plot\nplt.show()\n\n&lt;Figure size 1200x600 with 0 Axes&gt;\n\n\n\n\n\n\n\n\n\n\nimport pandas as pd\n\n# Assuming 'Delivery_Date' is in datetime format. If not, convert it first:\n# graph1['Delivery_Date'] = pd.to_datetime(graph1['Delivery_Date'])\n\n# Extract month and year from 'Delivery_Date'\ngraph2['Month'] = graph2['DELIVERY_DATE'].dt.month\ngraph1['Year'] = graph1['DELIVERY_DATE'].dt.year\n\n# Optionally, convert 'Month' from numbers to names for better readability in the plots\ngraph1['Month_Name'] = graph1['DELIVERY_DATE'].dt.strftime('%B')\n\n\n# Create mini multiple plots\ng = sns.FacetGrid(graph2, col=\"Arrival_Status\", col_wrap=1, height=5, aspect=3, margin_titles=True)\n\ng.map_dataframe(sns.lineplot, x='Month_Name', y='Count', hue='Year', palette='muted', marker='o', ci=None)\n\n# Adjust the mini plots\ng.set_xticklabels(rotation=45)  # Rotate x-axis labels for readability\ng.add_legend()  # Add legend\ng.set_titles(\"{col_name}\")  # Set title for each subplot\ng.fig.subplots_adjust(top=0.9)\ng.fig.suptitle('Monthly Count by Year for Each Arrival Status')\n\norder = ['January', 'February', 'March', 'April', 'May', 'June', 'July', 'August', 'September', 'October', 'November', 'December']\nplt.xticks(range(len(order)), order, rotation=45)\n\n# Show the plot\nplt.show()\n\nC:\\Users\\Dann_\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\seaborn\\axisgrid.py:848: FutureWarning: \n\nThe `ci` parameter is deprecated. Use `errorbar=None` for the same effect.\n\n  func(*plot_args, **plot_kwargs)\nC:\\Users\\Dann_\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\seaborn\\axisgrid.py:848: FutureWarning: \n\nThe `ci` parameter is deprecated. Use `errorbar=None` for the same effect.\n\n  func(*plot_args, **plot_kwargs)\nC:\\Users\\Dann_\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\seaborn\\axisgrid.py:848: FutureWarning: \n\nThe `ci` parameter is deprecated. Use `errorbar=None` for the same effect.\n\n  func(*plot_args, **plot_kwargs)\n\n\n\n\n\n\n\n\n\n\nkoch_1.shape\n\n(180306, 23)\n\n\n\nkoch_2 = koch[koch['DELIVERY_DATE'].isna()]\n\n\nkoch_2.shape\n\n(45636, 23)\n\n\n\n# Filter rows where both 'FIRST_GR_POSTING_DATE' and 'SECOND_COLUMN' are null\nkoch_3 = koch[koch['FIRST_GR_POSTING_DATE'].isna() & koch['DELIVERY_DATE'].isna()]\n\n\n### THIS WE DON'T KNOW\n# Attempt to convert 'PLANNED_DELIVERY_DAYS' to numeric, forcing non-convertible values to NaN\nkoch_3['PLANNED_DELIVERY_DAYS_numeric'] = pd.to_numeric(koch_3['PLANNED_DELIVERY_DAYS'], errors='coerce')\n\nrows = [ 22641, 115701, 358807, 484118, 489462, 524500, 524553, 524570,\n            524667, 524670, 524698, 592204, 614500, 745948, 947446]\n\nfiltered_rows = koch_3.query(\"index in @rows\")\n\n\nC:\\Users\\Dann_\\AppData\\Local\\Temp\\ipykernel_31276\\2660678657.py:4: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  koch_3['PLANNED_DELIVERY_DAYS_numeric'] = pd.to_numeric(koch_3['PLANNED_DELIVERY_DAYS'], errors='coerce')\n\n\n\nrows = [22641, 115701, 358807, 484118, 489462, 524500, 524553, 524570,\n        524667, 524670, 524698, 592204, 614500, 745948, 947446]\n\n# Drop rows based on indices\nkoch = koch.drop(rows)\n\n\n\nsns.countplot(koch, x=\"country\", hue=\"country\")\n\n\n\n\n\n\n\n\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nkoch = koch.dropna(subset=['Time_Difference'])\n\nsns.countplot(koch, x=\"region\", hue=\"Arrival_Status\")\n\n\n\n\n\n\n\n\n\nkoch.shape\n\n(959086, 25)\n\n\n\n# Load your dataset\ndata = koch[['country', 'region', 'Arrival_Status']]\n\n# Use catplot to create a count plot on 'region' with hue 'Arrival_Status', facetted by 'country'\ng = sns.catplot(data=data, kind='count', x='region', hue='Arrival_Status', col='country', col_wrap=4, height=4, aspect=1, sharex=False, sharey=False)\n\n# Rotate x-axis labels for readability if needed\ng.set_xticklabels(rotation=90)\n\n# Adjust spacing and layout\ng.fig.subplots_adjust(top=0.9)  # adjust the Figure in g\ng.fig.suptitle('Arrival Status by Region and Country')\n\n# Show plot\ng.fig.show()\n\nC:\\Users\\Dann_\\AppData\\Local\\Temp\\ipykernel_31276\\911448519.py:15: UserWarning: Matplotlib is currently using module://matplotlib_inline.backend_inline, which is a non-GUI backend, so cannot show the figure.\n  g.fig.show()\n\n\n\n\n\n\n\n\n\n\nkoch.shape\n\n(959086, 25)\n\n\n\nkoch.isna().sum()\n\nPURCHASE_DOCUMENT_ID                           0\nCREATE_DATE                                    0\nCOMPANY_CODE_ID                                0\nVENDOR_ID                                 182957\nPOSTAL_CD                                 183927\nRELEASE_DATE                              945562\nPURCHASE_DOCUMENT_ITEM_ID                      0\nMATERIAL_ID                               437382\nSUB_COMMODITY_DESC                           190\nMRP_TYPE_ID                               437382\nMRP_TYPE_DESC_E                           437382\nSHORT_TEXT                                     2\nPLANT_ID                                       4\nPOR_DELIVERY_DATE                         636604\nFIRST_GR_POSTING_DATE                          0\nDELIVERY_DATE                                  0\nREQUESTED_DELIVERY_DATE                       31\nINBOUND_DELIVERY_ID                       501397\nINBOUND_DELIVERY_ITEM_ID                       0\nPLANNED_DELIVERY_DAYS                          0\nBI_LAST_UPDATED_PURCHASE_DOCUMENT_ITEM         4\nTime_Difference                                0\nArrival_Status                                 0\ncountry                                        0\nregion                                         0\ndtype: int64\n\n\n\nimport seaborn as sns\nsns.countplot(koch, x='country', hue=\"Arrival_Status\")\n\n\n\n\n\n\n\n\n\nkoch.corr()\n\nC:\\Users\\Dann_\\AppData\\Local\\Temp\\ipykernel_31276\\3775191888.py:1: FutureWarning: The default value of numeric_only in DataFrame.corr is deprecated. In a future version, it will default to False. Select only valid columns or specify the value of numeric_only to silence this warning.\n  koch.corr()\n\n\n\n\n\n\n\n\n\nPURCHASE_DOCUMENT_ID\nCREATE_DATE\nRELEASE_DATE\nPURCHASE_DOCUMENT_ITEM_ID\nMATERIAL_ID\nMRP_TYPE_ID\nMRP_TYPE_DESC_E\nPLANT_ID\nINBOUND_DELIVERY_ID\nINBOUND_DELIVERY_ITEM_ID\nTime_Difference\n\n\n\n\nPURCHASE_DOCUMENT_ID\n1.000000\n-0.447598\n0.973145\n-0.067820\n-0.026805\n-0.089837\n-0.089837\n-0.116759\n0.067220\n0.137625\n0.013045\n\n\nCREATE_DATE\n-0.447598\n1.000000\n0.978268\n0.019132\n-0.151960\n0.057505\n0.057505\n0.126165\n0.862824\n-0.037265\n-0.003790\n\n\nRELEASE_DATE\n0.973145\n0.978268\n1.000000\n0.058318\n-0.428607\n0.129329\n0.129329\n0.390877\n0.978144\n0.015586\n-0.039788\n\n\nPURCHASE_DOCUMENT_ITEM_ID\n-0.067820\n0.019132\n0.058318\n1.000000\n-0.047315\n-0.025699\n-0.025699\n0.065652\n0.039911\n-0.079710\n0.007318\n\n\nMATERIAL_ID\n-0.026805\n-0.151960\n-0.428607\n-0.047315\n1.000000\n0.053123\n0.053123\n-0.099694\n-0.108986\n0.072350\n0.031020\n\n\nMRP_TYPE_ID\n-0.089837\n0.057505\n0.129329\n-0.025699\n0.053123\n1.000000\n1.000000\n-0.089920\n0.074652\n0.015430\n0.003995\n\n\nMRP_TYPE_DESC_E\n-0.089837\n0.057505\n0.129329\n-0.025699\n0.053123\n1.000000\n1.000000\n-0.089920\n0.074652\n0.015430\n0.003995\n\n\nPLANT_ID\n-0.116759\n0.126165\n0.390877\n0.065652\n-0.099694\n-0.089920\n-0.089920\n1.000000\n-0.017856\n-0.125070\n-0.054153\n\n\nINBOUND_DELIVERY_ID\n0.067220\n0.862824\n0.978144\n0.039911\n-0.108986\n0.074652\n0.074652\n-0.017856\n1.000000\n0.030787\n0.035110\n\n\nINBOUND_DELIVERY_ITEM_ID\n0.137625\n-0.037265\n0.015586\n-0.079710\n0.072350\n0.015430\n0.015430\n-0.125070\n0.030787\n1.000000\n0.030571\n\n\nTime_Difference\n0.013045\n-0.003790\n-0.039788\n0.007318\n0.031020\n0.003995\n0.003995\n-0.054153\n0.035110\n0.030571\n1.000000\n\n\n\n\n\n\n\n\nkoch['Time_Difference'].isnull().sum()\n\n180321\n\n\n\ned = 1139407/(i*100)\ned\n\n0.06318770414982171\n\n\n\n=koch.shape\n\n(1139407, 22)\n\n\n\nkoch.shape\n\n(1139407, 23)\n\n\n\n\n\n\n\n\n\n\n\n\nPURCHASE_DOCUMENT_ID\nCREATE_DATE\nCOMPANY_CODE_ID\nVENDOR_ID\nPOSTAL_CD\nRELEASE_DATE\nPURCHASE_DOCUMENT_ITEM_ID\nMATERIAL_ID\nSUB_COMMODITY_DESC\nMRP_TYPE_ID\n...\nSHORT_TEXT\nPLANT_ID\nPOR_DELIVERY_DATE\nFIRST_GR_POSTING_DATE\nDELIVERY_DATE\nREQUESTED_DELIVERY_DATE\nINBOUND_DELIVERY_ID\nINBOUND_DELIVERY_ITEM_ID\nPLANNED_DELIVERY_DAYS\nBI_LAST_UPDATED_PURCHASE_DOCUMENT_ITEM\n\n\n\n\n0\n2000008134\n20210726\nCA10\n8010005836\nN2C 0B7\nNaN\n30.0\nNaN\nMachinery & Equipment\nNaN\n...\n1/2\" OD x 0.063\" Wall 20' Long ASTM A51\n4036.0\nNaN\n20210812.0\n20210806.0\n20210806.0\nNaN\n0.0\n0.0\n2023-05-30 23:07:43.640\n\n\n1\n5100146560\n20210726\nUS10\nNone\nNone\nNaN\n130.0\n2.100008e+09\nCustom Manufacturing\n1.0\n...\n994 45 9951A TU E 4.99-8.62 3X0X 4014\n4016.0\n20210914.0\n20210914.0\n20210910.0\n20210910.0\n185979544.0\n10.0\n3.0\n2021-11-08 03:52:05.270\n\n\n2\n5501410919\n20210726\nCA10\n8010003139\nL6H 5T5\nNaN\n10.0\nNaN\nInfrastructure Services\nNaN\n...\nB66 RECHARGE PORTABLE SAFETY SHOWER 2021\n4036.0\nNaN\n0.0\n20210802.0\n20210802.0\nNaN\n0.0\n0.0\n2023-09-07 10:07:29.323\n\n\n3\n2000008138\n20210726\nCA10\n8010005836\nN2C 0B7\nNaN\n20.0\nNaN\nPiping & Tubing\nNaN\n...\n5/8\" x 50 ft Black Rubber ProFitter[REG]\n4036.0\nNaN\n20210803.0\n20210827.0\n20210827.0\nNaN\n0.0\n0.0\n2023-05-30 23:07:43.640\n\n\n4\n2000008136\n20210726\nCA10\n8010005836\nN2C 0B7\nNaN\n40.0\nNaN\nMaterial Handling\nNaN\n...\n4XL - HI-VIZ SAFETY LONG-SLEEVED SHIRT -\n4036.0\nNaN\n20210730.0\n20210805.0\n20210805.0\nNaN\n0.0\n0.0\n2023-05-30 23:07:43.640\n\n\n\n\n5 rows × 21 columns\n\n\n\n\n\n\n\n\n\nFirst, let’s force headers to uppercase using Snowpark DataFrame operations for standardization when columns are later written to a Snowflake table.\n\n# Force headers to uppercase\nfor colname in diamonds_df.columns:\n    if colname == '\"table\"':\n       new_colname = \"TABLE_PCT\"\n    else:\n        new_colname = str.upper(colname)\n    diamonds_df = diamonds_df.with_column_renamed(colname, new_colname)\n\ndiamonds_df.show()\n\n----------------------------------------------------------------------------------------------------\n|\"CARAT\"  |\"CUT\"      |\"COLOR\"  |\"CLARITY\"  |\"DEPTH\"  |\"TABLE_PCT\"  |\"PRICE\"  |\"X\"   |\"Y\"   |\"Z\"   |\n----------------------------------------------------------------------------------------------------\n|0.23     |Ideal      |E        |SI2        |61.5     |55.0         |326      |3.95  |3.98  |2.43  |\n|0.21     |Premium    |E        |SI1        |59.8     |61.0         |326      |3.89  |3.84  |2.31  |\n|0.23     |Good       |E        |VS1        |56.9     |65.0         |327      |4.05  |4.07  |2.31  |\n|0.29     |Premium    |I        |VS2        |62.4     |58.0         |334      |4.20  |4.23  |2.63  |\n|0.31     |Good       |J        |SI2        |63.3     |58.0         |335      |4.34  |4.35  |2.75  |\n|0.24     |Very Good  |J        |VVS2       |62.8     |57.0         |336      |3.94  |3.96  |2.48  |\n|0.24     |Very Good  |I        |VVS1       |62.3     |57.0         |336      |3.95  |3.98  |2.47  |\n|0.26     |Very Good  |H        |SI1        |61.9     |55.0         |337      |4.07  |4.11  |2.53  |\n|0.22     |Fair       |E        |VS2        |65.1     |61.0         |337      |3.87  |3.78  |2.49  |\n|0.23     |Very Good  |H        |VS1        |59.4     |61.0         |338      |4.00  |4.05  |2.39  |\n----------------------------------------------------------------------------------------------------\n\n\n\nNext, we standardize the category formatting for CUT using Snowpark DataFrame operations.\nThis way, when we write to a Snowflake table, there will be no inconsistencies in how the Snowpark DataFrame will read in the category names. Secondly, the feature transformations on categoricals will be easier to encode.\n\ndef fix_values(columnn):\n    return F.upper(F.regexp_replace(F.col(columnn), '[^a-zA-Z0-9]+', '_'))\n\nfor col in [\"CUT\"]:\n    diamonds_df = diamonds_df.with_column(col, fix_values(col))\n\ndiamonds_df.show()\n\n----------------------------------------------------------------------------------------------------\n|\"CARAT\"  |\"COLOR\"  |\"CLARITY\"  |\"DEPTH\"  |\"TABLE_PCT\"  |\"PRICE\"  |\"X\"   |\"Y\"   |\"Z\"   |\"CUT\"      |\n----------------------------------------------------------------------------------------------------\n|0.23     |E        |SI2        |61.5     |55.0         |326      |3.95  |3.98  |2.43  |IDEAL      |\n|0.21     |E        |SI1        |59.8     |61.0         |326      |3.89  |3.84  |2.31  |PREMIUM    |\n|0.23     |E        |VS1        |56.9     |65.0         |327      |4.05  |4.07  |2.31  |GOOD       |\n|0.29     |I        |VS2        |62.4     |58.0         |334      |4.20  |4.23  |2.63  |PREMIUM    |\n|0.31     |J        |SI2        |63.3     |58.0         |335      |4.34  |4.35  |2.75  |GOOD       |\n|0.24     |J        |VVS2       |62.8     |57.0         |336      |3.94  |3.96  |2.48  |VERY_GOOD  |\n|0.24     |I        |VVS1       |62.3     |57.0         |336      |3.95  |3.98  |2.47  |VERY_GOOD  |\n|0.26     |H        |SI1        |61.9     |55.0         |337      |4.07  |4.11  |2.53  |VERY_GOOD  |\n|0.22     |E        |VS2        |65.1     |61.0         |337      |3.87  |3.78  |2.49  |FAIR       |\n|0.23     |H        |VS1        |59.4     |61.0         |338      |4.00  |4.05  |2.39  |VERY_GOOD  |\n----------------------------------------------------------------------------------------------------\n\n\n\nCheck the schema.\n\nlist(diamonds_df.schema)\n\n[StructField('CARAT', DecimalType(3, 2), nullable=True),\n StructField('COLOR', StringType(16777216), nullable=True),\n StructField('CLARITY', StringType(16777216), nullable=True),\n StructField('DEPTH', DecimalType(3, 1), nullable=True),\n StructField('TABLE_PCT', DecimalType(3, 1), nullable=True),\n StructField('PRICE', LongType(), nullable=True),\n StructField('X', DecimalType(4, 2), nullable=True),\n StructField('Y', DecimalType(4, 2), nullable=True),\n StructField('Z', DecimalType(4, 2), nullable=True),\n StructField('CUT', StringType(16777216), nullable=True)]\n\n\nFinally, let’s cast the decimal types to DoubleType() since DecimalType() isn’t support by Snowpark ML at the moment.\n\nfor colname in [\"CARAT\", \"X\", \"Y\", \"Z\", \"DEPTH\", \"TABLE_PCT\"]:\n    diamonds_df = diamonds_df.with_column(colname, diamonds_df[colname].cast(DoubleType()))\n\ndiamonds_df.show()\n\n----------------------------------------------------------------------------------------------------\n|\"COLOR\"  |\"CLARITY\"  |\"PRICE\"  |\"CUT\"      |\"CARAT\"  |\"X\"   |\"Y\"   |\"Z\"   |\"DEPTH\"  |\"TABLE_PCT\"  |\n----------------------------------------------------------------------------------------------------\n|E        |SI2        |326      |IDEAL      |0.23     |3.95  |3.98  |2.43  |61.5     |55.0         |\n|E        |SI1        |326      |PREMIUM    |0.21     |3.89  |3.84  |2.31  |59.8     |61.0         |\n|E        |VS1        |327      |GOOD       |0.23     |4.05  |4.07  |2.31  |56.9     |65.0         |\n|I        |VS2        |334      |PREMIUM    |0.29     |4.2   |4.23  |2.63  |62.4     |58.0         |\n|J        |SI2        |335      |GOOD       |0.31     |4.34  |4.35  |2.75  |63.3     |58.0         |\n|J        |VVS2       |336      |VERY_GOOD  |0.24     |3.94  |3.96  |2.48  |62.8     |57.0         |\n|I        |VVS1       |336      |VERY_GOOD  |0.24     |3.95  |3.98  |2.47  |62.3     |57.0         |\n|H        |SI1        |337      |VERY_GOOD  |0.26     |4.07  |4.11  |2.53  |61.9     |55.0         |\n|E        |VS2        |337      |FAIR       |0.22     |3.87  |3.78  |2.49  |65.1     |61.0         |\n|H        |VS1        |338      |VERY_GOOD  |0.23     |4.0   |4.05  |2.39  |59.4     |61.0         |\n----------------------------------------------------------------------------------------------------\n\n\n\n\n\n\n\ndiamonds_df.write.mode('overwrite').save_as_table('diamonds')\n\n\nsession.close()"
  },
  {
    "objectID": "Projects/1_snowpark_ml_data_ingest.html#data-ingestion",
    "href": "Projects/1_snowpark_ml_data_ingest.html#data-ingestion",
    "title": "1. Data Ingestion",
    "section": "",
    "text": "The diamonds dataset has been widely used in data science and machine learning. We will use it to demonstrate Snowflake’s native data science transformers in terms of database functionality and Spark & Pandas comportablity, using non-synthetic and statistically appropriate data that is well known to the ML community.\n\n\nOther connection options include Username/Password, MFA, OAuth, Okta, SSO. For more information, refer to the Python Connector documentation.\n\n\n\n\n# Snowpark for Python\nfrom snowflake.snowpark import Session\nfrom snowflake.snowpark.version import VERSION\nfrom snowflake.snowpark.types import StructType, StructField, DoubleType, StringType\nimport snowflake.snowpark.functions as F\n\n# data science libs\nimport numpy as np\n\n# misc\nimport json\n\n\nimport pandas as pd\n\n\n# Make a Snowpark Connection\n\n################################################################################################################\n#  You can also use the SnowSQL Client to configure your connection params:\n#  https://docs.snowflake.com/en/user-guide/snowsql-install-config.html\n#\n#  &gt;&gt;&gt; from snowflake.ml.utils import connection_params\n#  &gt;&gt;&gt; session = Session.builder.configs(connection_params.SnowflakeLoginOptions()\n#  &gt;&gt;&gt; ).create()   \n#\n#  NOTE: If you have named connection params then specify the connection name\n#  Example:\n#  \n#  &gt;&gt;&gt; session = Session.builder.configs(\n#  &gt;&gt;&gt; connection_params.SnowflakeLoginOptions(connection_name='connections.snowml')\n#  &gt;&gt;&gt; ).create()\n#\n#################################################################################################################\n\n# Edit the connection.json before creating the session object below\n# Create Snowflake Session object\nconnection_parameters = json.load(open('connection.json'))\nsession = Session.builder.configs(connection_parameters).create()\nsession.sql_simplifier_enabled = True\n\nsnowflake_environment = session.sql('SELECT current_user(), current_version()').collect()\nsnowpark_version = VERSION\n\n# Current Environment Details\nprint('\\nConnection Established with the following parameters:')\nprint('User                        : {}'.format(snowflake_environment[0][0]))\nprint('Role                        : {}'.format(session.get_current_role()))\nprint('Database                    : {}'.format(session.get_current_database()))\nprint('Schema                      : {}'.format(session.get_current_schema()))\nprint('Warehouse                   : {}'.format(session.get_current_warehouse()))\nprint('Snowflake version           : {}'.format(snowflake_environment[0][1]))\nprint('Snowpark for Python version : {}.{}.{}'.format(snowpark_version[0],snowpark_version[1],snowpark_version[2]))\n\n\nConnection Established with the following parameters:\nUser                        : LION\nRole                        : \"TRAINING_ROLE\"\nDatabase                    : \"HACKATHON_ALGORITHM_AVENGERS\"\nSchema                      : \"PROCUREMENT_ON_TIME_DELIVERY\"\nWarehouse                   : \"U\"\nSnowflake version           : 8.9.1\nSnowpark for Python version : 1.13.0\n\n\n\n# Show the file before loading\nsession.sql(\"DESCRIBE TABLE PROCUREMENT_ON_TIME_DELIVERY.PURCHASE_ORDER_HISTORY;\").show()\n\ndf = session.read.options({\"field_delimiter\": \",\",\n                                    \"field_optionally_enclosed_by\": '\"',\n                                    \"infer_schema\": True,\n                                    \"parse_header\": True}).table(\"PROCUREMENT_ON_TIME_DELIVERY.PURCHASE_ORDER_HISTORY\")\n\n# Select only three columns from the DataFrame\n#selected_df = df.select(\"PURCHASE_DOCUMENT_ID\", \"CREATE_DATE\", \"COMPANY_CODE_ID\")\n\n# Show the selected columns\n#selected_df.show()\n\nkoch = df.to_pandas()\n\n---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n|\"name\"                     |\"type\"             |\"kind\"  |\"null?\"  |\"default\"  |\"primary key\"  |\"unique key\"  |\"check\"  |\"expression\"  |\"comment\"  |\"policy name\"  |\"privacy domain\"  |\n---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n|PURCHASE_DOCUMENT_ID       |NUMBER(38,0)       |COLUMN  |Y        |NULL       |N              |N             |NULL     |NULL          |NULL       |NULL           |NULL              |\n|CREATE_DATE                |NUMBER(38,0)       |COLUMN  |Y        |NULL       |N              |N             |NULL     |NULL          |NULL       |NULL           |NULL              |\n|COMPANY_CODE_ID            |VARCHAR(16777216)  |COLUMN  |Y        |NULL       |N              |N             |NULL     |NULL          |NULL       |NULL           |NULL              |\n|VENDOR_ID                  |VARCHAR(16777216)  |COLUMN  |Y        |NULL       |N              |N             |NULL     |NULL          |NULL       |NULL           |NULL              |\n|POSTAL_CD                  |VARCHAR(16777216)  |COLUMN  |Y        |NULL       |N              |N             |NULL     |NULL          |NULL       |NULL           |NULL              |\n|RELEASE_DATE               |NUMBER(38,0)       |COLUMN  |Y        |NULL       |N              |N             |NULL     |NULL          |NULL       |NULL           |NULL              |\n|PURCHASE_DOCUMENT_ITEM_ID  |NUMBER(38,0)       |COLUMN  |Y        |NULL       |N              |N             |NULL     |NULL          |NULL       |NULL           |NULL              |\n|MATERIAL_ID                |NUMBER(38,0)       |COLUMN  |Y        |NULL       |N              |N             |NULL     |NULL          |NULL       |NULL           |NULL              |\n|SUB_COMMODITY_DESC         |VARCHAR(16777216)  |COLUMN  |Y        |NULL       |N              |N             |NULL     |NULL          |NULL       |NULL           |NULL              |\n|MRP_TYPE_ID                |NUMBER(38,0)       |COLUMN  |Y        |NULL       |N              |N             |NULL     |NULL          |NULL       |NULL           |NULL              |\n---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n\n\n\n\nkoch.to_csv(\"koch.csv\")\n\n\nimport pandas as pd\nimport numpy as np\n\ndate_columns = ['POR_DELIVERY_DATE', 'DELIVERY_DATE', 'REQUESTED_DELIVERY_DATE', 'FIRST_GR_POSTING_DATE','CREATE_DATE']\n\nfor column in date_columns:\n    temp_series = koch[column].fillna(0).astype(int).astype(str)\n    \n    # Replace '0' string back to NaN to avoid incorrect date conversion\n    temp_series = temp_series.replace('0', np.nan)\n    \n    # Convert to datetime\n    koch[column] = pd.to_datetime(temp_series, format='%Y%m%d', errors='coerce')\n\n#Creating target column\nkoch['Time_Difference'] = koch['FIRST_GR_POSTING_DATE'] - koch['DELIVERY_DATE']\n\nkoch['Time_Difference'] = koch['Time_Difference'].dt.days\n\nfallback_diff = (koch['REQUESTED_DELIVERY_DATE'] - koch['DELIVERY_DATE']).dt.days\n\n# Fill in Time_Difference where it's NaN with the fallback difference\nkoch['Time_Difference'] = np.where(koch['Time_Difference'].isna(), fallback_diff, koch['Time_Difference'])\n\ndef determine_status(days):\n    if days &lt; 0:\n        return 'Early'\n    elif days == 0:\n        return 'On Time'\n    else:\n        return 'Late'\n\n# Assuming 'Time_Difference' is your column with the date difference in days\nkoch['Arrival_Status'] = koch['Time_Difference'].apply(determine_status)\n\n\nrows = [22641, 115701, 358807, 484118, 489462, 524500, 524553, 524570,\n        524667, 524670, 524698, 592204, 614500, 745948, 947446]\n\n# Drop rows based on indices\nkoch = koch.drop(rows)\n\n\nkoch['country'] = koch['COMPANY_CODE_ID'].str.slice(0, 2)\nkoch['region'] = koch['COMPANY_CODE_ID'].str.slice(2, 4)\n\nkoch = koch[koch['DELIVERY_DATE'] &gt;= koch['CREATE_DATE']]\n\nfrom datetime import datetime, timedelta\n\n# Get tomorrow's date for comparison\ntomorrow = datetime.today().date() + timedelta(days=1)\n\n# Filter rows where 'DELIVERY_DATE' is tomorrow or in the future\nkoch = koch[koch['DELIVERY_DATE'].dt.date &lt;= tomorrow]\n\n\n\n\n\ndef determine_status(days):\n    if days &lt; 0:\n        return 'Early'\n    elif days == 0:\n        return 'On Time'\n    else:\n        return 'Late'\n\n# Assuming 'Time_Difference' is your column with the date difference in days\nkoch['Arrival_Status'] = koch['Time_Difference'].apply(determine_status)\n\n\nrows = [22641, 115701, 358807, 484118, 489462, 524500, 524553, 524570,\n        524667, 524670, 524698, 592204, 614500, 745948, 947446]\n\n# Drop rows based on indices\nkoch = koch.drop(rows)\n\n\nkoch['country'] = koch['COMPANY_CODE_ID'].str.slice(0, 2)\nkoch['region'] = koch['COMPANY_CODE_ID'].str.slice(2, 4)\n\n\nkoch.shape\n\n(1139392, 25)\n\n\n\nkoch = koch[koch['DELIVERY_DATE'] &gt;= koch['CREATE_DATE']]\n\nfrom datetime import datetime, timedelta\n\n# Get tomorrow's date for comparison\ntomorrow = datetime.today().date() + timedelta(days=1)\n\n# Filter rows where 'DELIVERY_DATE' is tomorrow or in the future\nkoch = koch[koch['DELIVERY_DATE'].dt.date &lt;= tomorrow]\n\n\nweiurdkoch[koch['DELIVERY_DATE'] &gt;= koch['CREATE_DATE']]\n\n\ngraph1 = koch.groupby(['DELIVERY_DATE', 'Arrival_Status']).size().reset_index(name='Count')\n\n# Sort the resulting DataFrame by 'DELIVERY_DATE'\ngraph1 = graph1.sort_values(by='DELIVERY_DATE', ascending=False)\n\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Assuming 'graph1' is your DataFrame\n\n# Create a FacetGrid with 'ARRIVAL_STATUS' for creating separate subplots for each unique value in 'ARRIVAL_STATUS'\ngraph_1 = sns.FacetGrid(graph1, col=\"Arrival_Status\", col_wrap=4, sharex=False, sharey=False)  # Adjust 'col_wrap' as needed\n\n# Map a lineplot onto each subplot by specifying the x and y axis and the 'hue' for coloring different 'Arrival_Status' within each subplot\ngraph_1.map(sns.lineplot, \"DELIVERY_DATE\", \"Count\", \"Arrival_Status\", marker=\"o\")\n\n# Adding a legend and adjusting subplot titles\ngraph_1.add_legend()\ngraph_1.set_titles(\"{col_name}\")\n\n# Optionally, adjust the layout so plots don't overlap\nplt.tight_layout()\n\n# Show the plot\nplt.show()\n\n\ngraph1 = koch.groupby(['DELIVERY_DATE', 'Arrival_Status']).size().reset_index(name='Count')\n\n# Sort the resulting DataFrame by 'DELIVERY_DATE'\ngraph1 = graph1.sort_values(by='DELIVERY_DATE', ascending=False)\n\ngraph1\n\n\n\n\n\n\n\n\nDELIVERY_DATE\nArrival_Status\nCount\n\n\n\n\n5821\n2024-03-03\nOn Time\n1\n\n\n5820\n2024-03-02\nEarly\n1\n\n\n5819\n2024-03-01\nOn Time\n1\n\n\n5818\n2024-02-29\nOn Time\n24\n\n\n5817\n2024-02-29\nEarly\n1\n\n\n...\n...\n...\n...\n\n\n5\n2018-09-09\nOn Time\n1\n\n\n2\n2018-09-08\nLate\n120\n\n\n3\n2018-09-08\nOn Time\n2\n\n\n1\n2018-09-07\nOn Time\n34\n\n\n0\n2018-09-07\nLate\n8\n\n\n\n\n5822 rows × 3 columns\n\n\n\n\n\noriginal_count = len(koch)\n\nweird_dates_df = koch[koch['DELIVERY_DATE'] &gt; koch['CREATE_DATE']]\nfiltered_count = len(weird_dates_df)\n\n# Calculate the number of dropped rows\ndropped_count = original_count - filtered_count\n\nprint(f\"Number of rows dropped: {dropped_count}\")\n\n\nNumber of rows dropped: 90962\n\n\n\n\noriginal_count = len(koch)\n# Get tomorrow's date for comparison\ntomorrow = datetime.today().date() + timedelta(days=1)\n\n# Filter rows where 'DELIVERY_DATE' is tomorrow or in the future\nmore_weird = koch[koch['DELIVERY_DATE'].dt.date &lt;= tomorrow]\n\n# Get the count of the filtered rows\nmore_weird_count = len(more_weird)\n\n# Calculate the number of rows dropped\ndropped_count_2 = original_count - more_weird_count\n\nprint(f\"Number of rows dropped: {dropped_count_2}\")\n\nNumber of rows dropped: 0\n\n\n\n# Group by 'DELIVERY_DATE' and 'Arrival_Status', and count the occurrences\ngraph_data = koch.groupby(['DELIVERY_DATE', 'Arrival_Status']).size().reset_index(name='Count')\n\n# Sort the resulting DataFrame by 'DELIVERY_DATE'\ngraph_data = graph_data.sort_values(by='DELIVERY_DATE')\n\ngraph_data\n\n\n\n\n\n\n\n\nDELIVERY_DATE\nArrival_Status\nCount\n\n\n\n\n0\n2018-09-07\nLate\n8\n\n\n1\n2018-09-07\nOn Time\n34\n\n\n2\n2018-09-08\nLate\n120\n\n\n3\n2018-09-08\nOn Time\n2\n\n\n4\n2018-09-09\nLate\n6\n\n\n...\n...\n...\n...\n\n\n5818\n2024-02-29\nOn Time\n24\n\n\n5817\n2024-02-29\nEarly\n1\n\n\n5819\n2024-03-01\nOn Time\n1\n\n\n5820\n2024-03-02\nEarly\n1\n\n\n5821\n2024-03-03\nOn Time\n1\n\n\n\n\n5822 rows × 3 columns\n\n\n\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Assuming 'graph1' is your DataFrame\n\n# Create a FacetGrid with 'ARRIVAL_STATUS' for creating separate subplots for each unique value in 'ARRIVAL_STATUS'\ngraph_1 = sns.FacetGrid(graph1, col=\"Arrival_Status\", col_wrap=4, sharex=False, sharey=False)  # Adjust 'col_wrap' as needed\n\n# Map a lineplot onto each subplot by specifying the x and y axis and the 'hue' for coloring different 'Arrival_Status' within each subplot\ngraph_1.map(sns.lineplot, \"DELIVERY_DATE\", \"Count\", \"Arrival_Status\", marker=\"o\")\n\n# Adding a legend and adjusting subplot titles\ngraph_1.add_legend()\ngraph_1.set_titles(\"{col_name}\")\n\n# Optionally, adjust the layout so plots don't overlap\nplt.tight_layout()\n\n# Show the plot\nplt.show()\n\n\n\n\n\n\n\n\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Define your custom color palette for each 'Arrival_Status'\ncolor_palette = {\n    \"On Time\": \"green\",  # Ensure this matches exactly with your DataFrame's 'Arrival_Status' values\n    \"Late\": \"red\",\n    \"Early\": \"blue\"  # Corrected 'golden' to 'gold', which is a valid color name\n}\n\n# Create a FacetGrid\ng = sns.FacetGrid(graph1, col=\"Arrival_Status\", col_wrap=4, sharex=False, sharey=False)\n\n# Use map_dataframe to apply sns.lineplot\ng.map_dataframe(sns.lineplot, x=\"DELIVERY_DATE\", y=\"Count\", hue=\"Arrival_Status\", palette=color_palette, alpha =.7)\n\n# Add a legend and adjust subplot titles\ng.add_legend(title=\"Arrival Status\")\ng.set_titles(\"{col_name}\")\n\n# Set custom axis labels\ng.set_axis_labels(\"Delivery Date\", \"Count of packages received\")\n\n# Set the overall title for the figure\ng.fig.suptitle('Distribution of number of packages delivered by the status', fontsize=16)\n\n# Optionally, adjust the layout\nplt.tight_layout()\n\n# Show the plot\nplt.show()\n\n\n\n\n\n\n\n\n\nimport pandas as pd\n\n# Ensure that 'CREATE_DATE' is a datetime column\nkoch['CREATE_DATE'] = pd.to_datetime(koch['CREATE_DATE'])\n# Extract the year from 'CREATE_DATE'\nkoch['Year'] = koch['CREATE_DATE'].dt.year\n\n# Calculate the average 'Time_Difference' per 'MATERIAL_ID' and 'Year'\nmaterial_delays = koch.groupby(['MATERIAL_ID', 'Year'])['Time_Difference'].mean().reset_index()\n\n# Get top 5 materials with the highest average delay per year\ntop_materials = material_delays.sort_values('Time_Difference', ascending=False).groupby('Year').head(5)\n\n# Get bottom 5 materials with the lowest average delay per year\nbottom_materials = material_delays.sort_values('Time_Difference', ascending=True).groupby('Year').head(5)\n\n\ntop_materials.head(5)\n\n\n\n\n\n\n\n\nMATERIAL_ID\nYear\nTime_Difference\n\n\n\n\n61623\n2.100009e+09\n2018\n1627.0\n\n\n63904\n2.100031e+09\n2019\n1489.0\n\n\n60666\n2.100008e+09\n2019\n1321.0\n\n\n60088\n2.100008e+09\n2020\n1175.5\n\n\n61849\n2.100009e+09\n2020\n1066.0\n\n\n\n\n\n\n\n\nbottom_materials.head(5)\n\n\n\n\n\n\n\n\nMATERIAL_ID\nYear\nTime_Difference\n\n\n\n\n44804\n1.100261e+09\n2021\n-585.0\n\n\n54072\n1.100275e+09\n2022\n-516.0\n\n\n54045\n1.100275e+09\n2022\n-516.0\n\n\n41686\n1.100255e+09\n2022\n-488.0\n\n\n41357\n1.100254e+09\n2018\n-480.5\n\n\n\n\n\n\n\n\n# Convert 'Delivery_Date' to datetime if it's not already\ngraph2 = graph1\n\n\n\n# Extract month name and year from 'Delivery_Date'\ngraph2['Month'] = graph2['DELIVERY_DATE2'].dt.month_name()\ngraph2['Year'] = graph2['DELIVERY_DATE2'].dt.year.astype(str)  # Convert year to string for categorical plotting\n\n\ngraph2\n\n\n\n\n\n\n\n\nDELIVERY_DATE\nArrival_Status\nCount\nDELIVERY_DATE2\nMonth\nYear\nMonth_Name\n\n\n\n\n5821\n2024-03-03\nOn Time\n1\n2024-03-03\nMarch\n2024\nMarch\n\n\n5820\n2024-03-02\nEarly\n1\n2024-03-02\nMarch\n2024\nMarch\n\n\n5819\n2024-03-01\nOn Time\n1\n2024-03-01\nMarch\n2024\nMarch\n\n\n5818\n2024-02-29\nOn Time\n24\n2024-02-29\nFebruary\n2024\nFebruary\n\n\n5817\n2024-02-29\nEarly\n1\n2024-02-29\nFebruary\n2024\nFebruary\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n5\n2018-09-09\nOn Time\n1\n2018-09-09\nSeptember\n2018\nSeptember\n\n\n2\n2018-09-08\nLate\n120\n2018-09-08\nSeptember\n2018\nSeptember\n\n\n3\n2018-09-08\nOn Time\n2\n2018-09-08\nSeptember\n2018\nSeptember\n\n\n1\n2018-09-07\nOn Time\n34\n2018-09-07\nSeptember\n2018\nSeptember\n\n\n0\n2018-09-07\nLate\n8\n2018-09-07\nSeptember\n2018\nSeptember\n\n\n\n\n5822 rows × 7 columns\n\n\n\n\n\n# Assuming 'df' is your DataFrame\n\n# Create a FacetGrid, with rows for each 'Arrival_Status'\ng = sns.FacetGrid(graph2, row='Arrival_Status', height=5, aspect=3, margin_titles=True)\n\n# Map the data to lineplots\ng.map(sns.lineplot, 'Month_Name', 'Count', 'Year', marker=\"o\", palette='viridis', ci=None)\n\ng.set_axis_labels('Months', 'Number of orders Delivered')\ng.set_titles('{row_name}')\ng.add_legend(title='Years')\ng.set_xticklabels(rotation=45)\nfor ax, status in zip(g.axes.flatten(), graph2['Arrival_Status'].unique()):\n    ax.set_title(f'Your Custom Subtitle Here for {status}', y=1.02) # You can adjust the y value as needed\n\n# Show plot\nplt.show()\n\n\n# Show plot\nplt.show()\n\n\n\nC:\\Users\\Dann_\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\seaborn\\axisgrid.py:848: FutureWarning: \n\nThe `ci` parameter is deprecated. Use `errorbar=None` for the same effect.\n\n  func(*plot_args, **plot_kwargs)\nC:\\Users\\Dann_\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\seaborn\\axisgrid.py:848: FutureWarning: \n\nThe `ci` parameter is deprecated. Use `errorbar=None` for the same effect.\n\n  func(*plot_args, **plot_kwargs)\nC:\\Users\\Dann_\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\seaborn\\axisgrid.py:848: FutureWarning: \n\nThe `ci` parameter is deprecated. Use `errorbar=None` for the same effect.\n\n  func(*plot_args, **plot_kwargs)\n\n\n\n\n\n\n\n\n\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Create the main graph\nplt.figure(figsize=(12, 6))\ng = sns.catplot(x='Month', y='Count', hue='Year', data=graph1, kind='bar', height=6, aspect=2, palette='muted', order=['January', 'February', 'March', 'April', 'May', 'June', 'July', 'August', 'September', 'October', 'November', 'December'])\ng.set_axis_labels(\"Month\", \"Count\")\ng.fig.suptitle('Monthly Shipments by Year')\n\n# Rotate x-axis labels for readability if needed\nplt.xticks(rotation=45)\n\n# Show plot\nplt.show()\n\n&lt;Figure size 1200x600 with 0 Axes&gt;\n\n\n\n\n\n\n\n\n\n\nimport pandas as pd\n\n# Assuming 'Delivery_Date' is in datetime format. If not, convert it first:\n# graph1['Delivery_Date'] = pd.to_datetime(graph1['Delivery_Date'])\n\n# Extract month and year from 'Delivery_Date'\ngraph2['Month'] = graph2['DELIVERY_DATE'].dt.month\ngraph1['Year'] = graph1['DELIVERY_DATE'].dt.year\n\n# Optionally, convert 'Month' from numbers to names for better readability in the plots\ngraph1['Month_Name'] = graph1['DELIVERY_DATE'].dt.strftime('%B')\n\n\n# Create mini multiple plots\ng = sns.FacetGrid(graph2, col=\"Arrival_Status\", col_wrap=1, height=5, aspect=3, margin_titles=True)\n\ng.map_dataframe(sns.lineplot, x='Month_Name', y='Count', hue='Year', palette='muted', marker='o', ci=None)\n\n# Adjust the mini plots\ng.set_xticklabels(rotation=45)  # Rotate x-axis labels for readability\ng.add_legend()  # Add legend\ng.set_titles(\"{col_name}\")  # Set title for each subplot\ng.fig.subplots_adjust(top=0.9)\ng.fig.suptitle('Monthly Count by Year for Each Arrival Status')\n\norder = ['January', 'February', 'March', 'April', 'May', 'June', 'July', 'August', 'September', 'October', 'November', 'December']\nplt.xticks(range(len(order)), order, rotation=45)\n\n# Show the plot\nplt.show()\n\nC:\\Users\\Dann_\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\seaborn\\axisgrid.py:848: FutureWarning: \n\nThe `ci` parameter is deprecated. Use `errorbar=None` for the same effect.\n\n  func(*plot_args, **plot_kwargs)\nC:\\Users\\Dann_\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\seaborn\\axisgrid.py:848: FutureWarning: \n\nThe `ci` parameter is deprecated. Use `errorbar=None` for the same effect.\n\n  func(*plot_args, **plot_kwargs)\nC:\\Users\\Dann_\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\seaborn\\axisgrid.py:848: FutureWarning: \n\nThe `ci` parameter is deprecated. Use `errorbar=None` for the same effect.\n\n  func(*plot_args, **plot_kwargs)\n\n\n\n\n\n\n\n\n\n\nkoch_1.shape\n\n(180306, 23)\n\n\n\nkoch_2 = koch[koch['DELIVERY_DATE'].isna()]\n\n\nkoch_2.shape\n\n(45636, 23)\n\n\n\n# Filter rows where both 'FIRST_GR_POSTING_DATE' and 'SECOND_COLUMN' are null\nkoch_3 = koch[koch['FIRST_GR_POSTING_DATE'].isna() & koch['DELIVERY_DATE'].isna()]\n\n\n### THIS WE DON'T KNOW\n# Attempt to convert 'PLANNED_DELIVERY_DAYS' to numeric, forcing non-convertible values to NaN\nkoch_3['PLANNED_DELIVERY_DAYS_numeric'] = pd.to_numeric(koch_3['PLANNED_DELIVERY_DAYS'], errors='coerce')\n\nrows = [ 22641, 115701, 358807, 484118, 489462, 524500, 524553, 524570,\n            524667, 524670, 524698, 592204, 614500, 745948, 947446]\n\nfiltered_rows = koch_3.query(\"index in @rows\")\n\n\nC:\\Users\\Dann_\\AppData\\Local\\Temp\\ipykernel_31276\\2660678657.py:4: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  koch_3['PLANNED_DELIVERY_DAYS_numeric'] = pd.to_numeric(koch_3['PLANNED_DELIVERY_DAYS'], errors='coerce')\n\n\n\nrows = [22641, 115701, 358807, 484118, 489462, 524500, 524553, 524570,\n        524667, 524670, 524698, 592204, 614500, 745948, 947446]\n\n# Drop rows based on indices\nkoch = koch.drop(rows)\n\n\n\nsns.countplot(koch, x=\"country\", hue=\"country\")\n\n\n\n\n\n\n\n\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nkoch = koch.dropna(subset=['Time_Difference'])\n\nsns.countplot(koch, x=\"region\", hue=\"Arrival_Status\")\n\n\n\n\n\n\n\n\n\nkoch.shape\n\n(959086, 25)\n\n\n\n# Load your dataset\ndata = koch[['country', 'region', 'Arrival_Status']]\n\n# Use catplot to create a count plot on 'region' with hue 'Arrival_Status', facetted by 'country'\ng = sns.catplot(data=data, kind='count', x='region', hue='Arrival_Status', col='country', col_wrap=4, height=4, aspect=1, sharex=False, sharey=False)\n\n# Rotate x-axis labels for readability if needed\ng.set_xticklabels(rotation=90)\n\n# Adjust spacing and layout\ng.fig.subplots_adjust(top=0.9)  # adjust the Figure in g\ng.fig.suptitle('Arrival Status by Region and Country')\n\n# Show plot\ng.fig.show()\n\nC:\\Users\\Dann_\\AppData\\Local\\Temp\\ipykernel_31276\\911448519.py:15: UserWarning: Matplotlib is currently using module://matplotlib_inline.backend_inline, which is a non-GUI backend, so cannot show the figure.\n  g.fig.show()\n\n\n\n\n\n\n\n\n\n\nkoch.shape\n\n(959086, 25)\n\n\n\nkoch.isna().sum()\n\nPURCHASE_DOCUMENT_ID                           0\nCREATE_DATE                                    0\nCOMPANY_CODE_ID                                0\nVENDOR_ID                                 182957\nPOSTAL_CD                                 183927\nRELEASE_DATE                              945562\nPURCHASE_DOCUMENT_ITEM_ID                      0\nMATERIAL_ID                               437382\nSUB_COMMODITY_DESC                           190\nMRP_TYPE_ID                               437382\nMRP_TYPE_DESC_E                           437382\nSHORT_TEXT                                     2\nPLANT_ID                                       4\nPOR_DELIVERY_DATE                         636604\nFIRST_GR_POSTING_DATE                          0\nDELIVERY_DATE                                  0\nREQUESTED_DELIVERY_DATE                       31\nINBOUND_DELIVERY_ID                       501397\nINBOUND_DELIVERY_ITEM_ID                       0\nPLANNED_DELIVERY_DAYS                          0\nBI_LAST_UPDATED_PURCHASE_DOCUMENT_ITEM         4\nTime_Difference                                0\nArrival_Status                                 0\ncountry                                        0\nregion                                         0\ndtype: int64\n\n\n\nimport seaborn as sns\nsns.countplot(koch, x='country', hue=\"Arrival_Status\")\n\n\n\n\n\n\n\n\n\nkoch.corr()\n\nC:\\Users\\Dann_\\AppData\\Local\\Temp\\ipykernel_31276\\3775191888.py:1: FutureWarning: The default value of numeric_only in DataFrame.corr is deprecated. In a future version, it will default to False. Select only valid columns or specify the value of numeric_only to silence this warning.\n  koch.corr()\n\n\n\n\n\n\n\n\n\nPURCHASE_DOCUMENT_ID\nCREATE_DATE\nRELEASE_DATE\nPURCHASE_DOCUMENT_ITEM_ID\nMATERIAL_ID\nMRP_TYPE_ID\nMRP_TYPE_DESC_E\nPLANT_ID\nINBOUND_DELIVERY_ID\nINBOUND_DELIVERY_ITEM_ID\nTime_Difference\n\n\n\n\nPURCHASE_DOCUMENT_ID\n1.000000\n-0.447598\n0.973145\n-0.067820\n-0.026805\n-0.089837\n-0.089837\n-0.116759\n0.067220\n0.137625\n0.013045\n\n\nCREATE_DATE\n-0.447598\n1.000000\n0.978268\n0.019132\n-0.151960\n0.057505\n0.057505\n0.126165\n0.862824\n-0.037265\n-0.003790\n\n\nRELEASE_DATE\n0.973145\n0.978268\n1.000000\n0.058318\n-0.428607\n0.129329\n0.129329\n0.390877\n0.978144\n0.015586\n-0.039788\n\n\nPURCHASE_DOCUMENT_ITEM_ID\n-0.067820\n0.019132\n0.058318\n1.000000\n-0.047315\n-0.025699\n-0.025699\n0.065652\n0.039911\n-0.079710\n0.007318\n\n\nMATERIAL_ID\n-0.026805\n-0.151960\n-0.428607\n-0.047315\n1.000000\n0.053123\n0.053123\n-0.099694\n-0.108986\n0.072350\n0.031020\n\n\nMRP_TYPE_ID\n-0.089837\n0.057505\n0.129329\n-0.025699\n0.053123\n1.000000\n1.000000\n-0.089920\n0.074652\n0.015430\n0.003995\n\n\nMRP_TYPE_DESC_E\n-0.089837\n0.057505\n0.129329\n-0.025699\n0.053123\n1.000000\n1.000000\n-0.089920\n0.074652\n0.015430\n0.003995\n\n\nPLANT_ID\n-0.116759\n0.126165\n0.390877\n0.065652\n-0.099694\n-0.089920\n-0.089920\n1.000000\n-0.017856\n-0.125070\n-0.054153\n\n\nINBOUND_DELIVERY_ID\n0.067220\n0.862824\n0.978144\n0.039911\n-0.108986\n0.074652\n0.074652\n-0.017856\n1.000000\n0.030787\n0.035110\n\n\nINBOUND_DELIVERY_ITEM_ID\n0.137625\n-0.037265\n0.015586\n-0.079710\n0.072350\n0.015430\n0.015430\n-0.125070\n0.030787\n1.000000\n0.030571\n\n\nTime_Difference\n0.013045\n-0.003790\n-0.039788\n0.007318\n0.031020\n0.003995\n0.003995\n-0.054153\n0.035110\n0.030571\n1.000000\n\n\n\n\n\n\n\n\nkoch['Time_Difference'].isnull().sum()\n\n180321\n\n\n\ned = 1139407/(i*100)\ned\n\n0.06318770414982171\n\n\n\n=koch.shape\n\n(1139407, 22)\n\n\n\nkoch.shape\n\n(1139407, 23)\n\n\n\n\n\n\n\n\n\n\n\n\nPURCHASE_DOCUMENT_ID\nCREATE_DATE\nCOMPANY_CODE_ID\nVENDOR_ID\nPOSTAL_CD\nRELEASE_DATE\nPURCHASE_DOCUMENT_ITEM_ID\nMATERIAL_ID\nSUB_COMMODITY_DESC\nMRP_TYPE_ID\n...\nSHORT_TEXT\nPLANT_ID\nPOR_DELIVERY_DATE\nFIRST_GR_POSTING_DATE\nDELIVERY_DATE\nREQUESTED_DELIVERY_DATE\nINBOUND_DELIVERY_ID\nINBOUND_DELIVERY_ITEM_ID\nPLANNED_DELIVERY_DAYS\nBI_LAST_UPDATED_PURCHASE_DOCUMENT_ITEM\n\n\n\n\n0\n2000008134\n20210726\nCA10\n8010005836\nN2C 0B7\nNaN\n30.0\nNaN\nMachinery & Equipment\nNaN\n...\n1/2\" OD x 0.063\" Wall 20' Long ASTM A51\n4036.0\nNaN\n20210812.0\n20210806.0\n20210806.0\nNaN\n0.0\n0.0\n2023-05-30 23:07:43.640\n\n\n1\n5100146560\n20210726\nUS10\nNone\nNone\nNaN\n130.0\n2.100008e+09\nCustom Manufacturing\n1.0\n...\n994 45 9951A TU E 4.99-8.62 3X0X 4014\n4016.0\n20210914.0\n20210914.0\n20210910.0\n20210910.0\n185979544.0\n10.0\n3.0\n2021-11-08 03:52:05.270\n\n\n2\n5501410919\n20210726\nCA10\n8010003139\nL6H 5T5\nNaN\n10.0\nNaN\nInfrastructure Services\nNaN\n...\nB66 RECHARGE PORTABLE SAFETY SHOWER 2021\n4036.0\nNaN\n0.0\n20210802.0\n20210802.0\nNaN\n0.0\n0.0\n2023-09-07 10:07:29.323\n\n\n3\n2000008138\n20210726\nCA10\n8010005836\nN2C 0B7\nNaN\n20.0\nNaN\nPiping & Tubing\nNaN\n...\n5/8\" x 50 ft Black Rubber ProFitter[REG]\n4036.0\nNaN\n20210803.0\n20210827.0\n20210827.0\nNaN\n0.0\n0.0\n2023-05-30 23:07:43.640\n\n\n4\n2000008136\n20210726\nCA10\n8010005836\nN2C 0B7\nNaN\n40.0\nNaN\nMaterial Handling\nNaN\n...\n4XL - HI-VIZ SAFETY LONG-SLEEVED SHIRT -\n4036.0\nNaN\n20210730.0\n20210805.0\n20210805.0\nNaN\n0.0\n0.0\n2023-05-30 23:07:43.640\n\n\n\n\n5 rows × 21 columns\n\n\n\n\n\n\n\n\n\nFirst, let’s force headers to uppercase using Snowpark DataFrame operations for standardization when columns are later written to a Snowflake table.\n\n# Force headers to uppercase\nfor colname in diamonds_df.columns:\n    if colname == '\"table\"':\n       new_colname = \"TABLE_PCT\"\n    else:\n        new_colname = str.upper(colname)\n    diamonds_df = diamonds_df.with_column_renamed(colname, new_colname)\n\ndiamonds_df.show()\n\n----------------------------------------------------------------------------------------------------\n|\"CARAT\"  |\"CUT\"      |\"COLOR\"  |\"CLARITY\"  |\"DEPTH\"  |\"TABLE_PCT\"  |\"PRICE\"  |\"X\"   |\"Y\"   |\"Z\"   |\n----------------------------------------------------------------------------------------------------\n|0.23     |Ideal      |E        |SI2        |61.5     |55.0         |326      |3.95  |3.98  |2.43  |\n|0.21     |Premium    |E        |SI1        |59.8     |61.0         |326      |3.89  |3.84  |2.31  |\n|0.23     |Good       |E        |VS1        |56.9     |65.0         |327      |4.05  |4.07  |2.31  |\n|0.29     |Premium    |I        |VS2        |62.4     |58.0         |334      |4.20  |4.23  |2.63  |\n|0.31     |Good       |J        |SI2        |63.3     |58.0         |335      |4.34  |4.35  |2.75  |\n|0.24     |Very Good  |J        |VVS2       |62.8     |57.0         |336      |3.94  |3.96  |2.48  |\n|0.24     |Very Good  |I        |VVS1       |62.3     |57.0         |336      |3.95  |3.98  |2.47  |\n|0.26     |Very Good  |H        |SI1        |61.9     |55.0         |337      |4.07  |4.11  |2.53  |\n|0.22     |Fair       |E        |VS2        |65.1     |61.0         |337      |3.87  |3.78  |2.49  |\n|0.23     |Very Good  |H        |VS1        |59.4     |61.0         |338      |4.00  |4.05  |2.39  |\n----------------------------------------------------------------------------------------------------\n\n\n\nNext, we standardize the category formatting for CUT using Snowpark DataFrame operations.\nThis way, when we write to a Snowflake table, there will be no inconsistencies in how the Snowpark DataFrame will read in the category names. Secondly, the feature transformations on categoricals will be easier to encode.\n\ndef fix_values(columnn):\n    return F.upper(F.regexp_replace(F.col(columnn), '[^a-zA-Z0-9]+', '_'))\n\nfor col in [\"CUT\"]:\n    diamonds_df = diamonds_df.with_column(col, fix_values(col))\n\ndiamonds_df.show()\n\n----------------------------------------------------------------------------------------------------\n|\"CARAT\"  |\"COLOR\"  |\"CLARITY\"  |\"DEPTH\"  |\"TABLE_PCT\"  |\"PRICE\"  |\"X\"   |\"Y\"   |\"Z\"   |\"CUT\"      |\n----------------------------------------------------------------------------------------------------\n|0.23     |E        |SI2        |61.5     |55.0         |326      |3.95  |3.98  |2.43  |IDEAL      |\n|0.21     |E        |SI1        |59.8     |61.0         |326      |3.89  |3.84  |2.31  |PREMIUM    |\n|0.23     |E        |VS1        |56.9     |65.0         |327      |4.05  |4.07  |2.31  |GOOD       |\n|0.29     |I        |VS2        |62.4     |58.0         |334      |4.20  |4.23  |2.63  |PREMIUM    |\n|0.31     |J        |SI2        |63.3     |58.0         |335      |4.34  |4.35  |2.75  |GOOD       |\n|0.24     |J        |VVS2       |62.8     |57.0         |336      |3.94  |3.96  |2.48  |VERY_GOOD  |\n|0.24     |I        |VVS1       |62.3     |57.0         |336      |3.95  |3.98  |2.47  |VERY_GOOD  |\n|0.26     |H        |SI1        |61.9     |55.0         |337      |4.07  |4.11  |2.53  |VERY_GOOD  |\n|0.22     |E        |VS2        |65.1     |61.0         |337      |3.87  |3.78  |2.49  |FAIR       |\n|0.23     |H        |VS1        |59.4     |61.0         |338      |4.00  |4.05  |2.39  |VERY_GOOD  |\n----------------------------------------------------------------------------------------------------\n\n\n\nCheck the schema.\n\nlist(diamonds_df.schema)\n\n[StructField('CARAT', DecimalType(3, 2), nullable=True),\n StructField('COLOR', StringType(16777216), nullable=True),\n StructField('CLARITY', StringType(16777216), nullable=True),\n StructField('DEPTH', DecimalType(3, 1), nullable=True),\n StructField('TABLE_PCT', DecimalType(3, 1), nullable=True),\n StructField('PRICE', LongType(), nullable=True),\n StructField('X', DecimalType(4, 2), nullable=True),\n StructField('Y', DecimalType(4, 2), nullable=True),\n StructField('Z', DecimalType(4, 2), nullable=True),\n StructField('CUT', StringType(16777216), nullable=True)]\n\n\nFinally, let’s cast the decimal types to DoubleType() since DecimalType() isn’t support by Snowpark ML at the moment.\n\nfor colname in [\"CARAT\", \"X\", \"Y\", \"Z\", \"DEPTH\", \"TABLE_PCT\"]:\n    diamonds_df = diamonds_df.with_column(colname, diamonds_df[colname].cast(DoubleType()))\n\ndiamonds_df.show()\n\n----------------------------------------------------------------------------------------------------\n|\"COLOR\"  |\"CLARITY\"  |\"PRICE\"  |\"CUT\"      |\"CARAT\"  |\"X\"   |\"Y\"   |\"Z\"   |\"DEPTH\"  |\"TABLE_PCT\"  |\n----------------------------------------------------------------------------------------------------\n|E        |SI2        |326      |IDEAL      |0.23     |3.95  |3.98  |2.43  |61.5     |55.0         |\n|E        |SI1        |326      |PREMIUM    |0.21     |3.89  |3.84  |2.31  |59.8     |61.0         |\n|E        |VS1        |327      |GOOD       |0.23     |4.05  |4.07  |2.31  |56.9     |65.0         |\n|I        |VS2        |334      |PREMIUM    |0.29     |4.2   |4.23  |2.63  |62.4     |58.0         |\n|J        |SI2        |335      |GOOD       |0.31     |4.34  |4.35  |2.75  |63.3     |58.0         |\n|J        |VVS2       |336      |VERY_GOOD  |0.24     |3.94  |3.96  |2.48  |62.8     |57.0         |\n|I        |VVS1       |336      |VERY_GOOD  |0.24     |3.95  |3.98  |2.47  |62.3     |57.0         |\n|H        |SI1        |337      |VERY_GOOD  |0.26     |4.07  |4.11  |2.53  |61.9     |55.0         |\n|E        |VS2        |337      |FAIR       |0.22     |3.87  |3.78  |2.49  |65.1     |61.0         |\n|H        |VS1        |338      |VERY_GOOD  |0.23     |4.0   |4.05  |2.39  |59.4     |61.0         |\n----------------------------------------------------------------------------------------------------\n\n\n\n\n\n\n\ndiamonds_df.write.mode('overwrite').save_as_table('diamonds')\n\n\nsession.close()"
  },
  {
    "objectID": "Projects/project2.html",
    "href": "Projects/project2.html",
    "title": "Senior Project",
    "section": "",
    "text": "A data product in Data Science is a product that facilitates an end goal through the use of data. It is essentially a technical asset that makes use of data as its core component to solve a specific problem or fulfill a particular need for its users. Data products can be as simple as a report generated from data analyses, or as complex as a machine learning model that powers a recommendation system in an application.\nData products are the bridge between raw data and practical applications. They are essential for applying the theoretical aspects of data science to solve real-world problems. By turning data into actionable insights, data products enable businesses and organizations to make informed decisions, optimize operations, and enhance user experiences. As such, the development and management of data products is a crucial aspect of the data science field, requiring a blend of technical skills, domain expertise, and an understanding of user needs.\n\n\n\nData-Driven: At its core, a data product uses data to function. This data can be static or dynamic, and the product’s effectiveness often depends on the quality and relevance of this data.\nActionable Insights: Unlike raw data or basic analyses, a data product provides insights that are actionable. This means that users can make decisions or take actions based on the output of the data product.\nUser-Focused: Data products are designed with the end-user in mind, ensuring that they are accessible, understandable, and valuable to the intended audience. This often involves user interface design, data visualization, and user experience optimization.\nAutomated: Many data products automate processes that would otherwise require manual data analysis, saving time and reducing the potential for human error.\nScalable: As data grows or as the number of users increases, a well-designed data product can scale to accommodate these changes without losing performance or reliability.\n\n\n\n\nDiving into SQL (Structured Query Language) is like learning the grammar of data speak. It’s the go to language for communicate to databases, asking them about the information they hold. Whether you’re inquiring about customer behaviors, financial transactions, or anything in between, SQL helps you pose the questions and understand the responses. With SQL, you can sift through mountains of data to find the nuggets of insight that drive strategies and decisions, all of this is always done by a few well-crafted queries.\n\n\n\nThe key parts of an SQL query provides a deeper understanding of how data is manipulated and retrieved from a database. Each component plays a crucial role in shaping the output of the query.\n\n\nThe SELECT statement specifies the columns of data you want to retrieve from one or more tables in a database. It’s the starting point of most queries and defines which fields should be included in the output. You can use SELECT * to retrieve all columns or specify individual column names separated by commas for a more tailored output.\nExample:\nSELECT FirstName, LastName  FROM Employees; \nThis query retrieves the FirstName and LastName columns from the Employees table.\n\n\n\nThe FROM clause specifies the table(s) from which to retrieve data. When data needs to be combined from multiple tables, the JOIN clause is used. There are several types of joins, including INNER JOIN, LEFT JOIN, RIGHT JOIN, and FULL JOIN, each serving different purposes in terms of how tables are merged based on matching data in specified columns.\nExample:\nSELECT Orders.OrderID, Customers.CustomerName  FROM Orders  INNER JOIN Customers ON Orders.CustomerID = Customers.CustomerID; \nThis query retrieves order IDs from the Orders table and the corresponding customer names by joining the Orders table with the Customers table on the CustomerID column.\n\n\n\nThe WHERE clause filters records to include only those that meet certain conditions. It helps in narrowing down the results based on specified criteria, making the query output more relevant.\nExample:\nSELECT *  FROM Employees  WHERE Department = ‘Marketing’;\nThis query retrieves all columns for employees who work in the Marketing department.\n\n\n\nThe HAVING clause is used to filter groups of rows after they’ve been grouped by a GROUP BY clause. It’s similar to the WHERE clause but is applicable to aggregated data. HAVING is commonly used with functions like SUM(), AVG(), COUNT(), etc.\nExample:\nSELECT Department, COUNT(EmployeeID) AS NumberOfEmployees  FROM Employees  GROUP BY Department  HAVING COUNT(EmployeeID) &gt; 10; \nThis query counts the number of employees in each department and only includes those departments with more than 10 employees.\n\n\n\nThe GROUP BY clause groups rows that have the same values in specified columns into summary rows, like “find the number of employees in each department”. It is often used with aggregate functions (COUNT(), MAX(), SUM(), AVG()).\nExample:\nSELECT Department, AVG(Salary) AS AverageSalary  FROM Employees  GROUP BY Department; \nThis query calculates the average salary within each department.\n\n\n\nThe ORDER BY clause is used to sort the result set of a query by one or more columns. It can sort the results in ascending (ASC) or descending (DESC) order. If no direction is specified, ASC is assumed by default.\nExample:\nSELECT FirstName, LastName, Salary  FROM Employees  ORDER BY Salary DESC; \nThis query retrieves employee names and salaries, sorted by salary in descending order.\n\n\n\nThe LIMIT clause is used to constrain the number of rows returned by a query. This is useful in large datasets to retrieve only a subset of rows. The OFFSET clause is used in conjunction with LIMIT to skip a specific number of rows before starting to return rows from the query.\nExample:\nSELECT *  FROM Employees  LIMIT 10 OFFSET 5;  This query skips the first 5 rows and then retrieves the next 10 rows from the Employees table.\n\n\n\n\nA subquery is a query embedded within another query. It can be used in various parts of the main query, including the SELECT, FROM, and WHERE clauses. Subqueries are typically enclosed in parentheses and can return scalar values, single columns, single rows, or tables, depending on their placement and purpose in the main query.\nExample of a subquery in a WHERE clause:\nSELECT *  FROM Employees  WHERE DepartmentID IN (SELECT DepartmentID FROM Departments WHERE Name = ‘Research’);\nIn this example, the subquery selects DepartmentIDs from the Departments table where the department name is ‘Research’. The main query then uses these IDs to filter employees from those specific departments.\n\n\n\nA nested query refers to a broader category of queries that contain other queries, and it can encompass subqueries as one of its forms. The term “nested query” can also specifically refer to queries nested within FROM clauses, where a subquery is used to create a temporary table that the outer query can then join to or operate on.\nExample of a nested query in a FROM clause:\nSELECT AvgSalaries.Department, Employee.Name, Employee.Salary  FROM Employees AS Employee  JOIN (  SELECT DepartmentID, AVG(Salary) AS AverageSalary  FROM Employees  GROUP BY DepartmentID  ) AS AvgSalaries ON Employee.DepartmentID = AvgSalaries.DepartmentID  WHERE Employee.Salary &gt; AvgSalaries.AverageSalary; \nIn this example, the nested query calculates the average salary per department and creates a temporary table (AvgSalaries) with this information. The outer query then selects employees who earn more than the average salary in their respective departments.\n\n\n\n\nScope: Subqueries can be considered a specific type of nested query. While all subqueries are nested queries, not all nested queries are subqueries, especially when considering the broader use of nesting in SQL queries.\nUsage: Subqueries are often used for returning specific values that the main query can use for comparison or filtering, whereas nested queries, particularly those in the FROM clause, are used to create temporary tables for the main query to interact with.\nComplexity: Nested queries, especially those used as temporary tables, can be more complex and might involve aggregation, grouping, or additional joins within the nested part, whereas subqueries might be simpler, returning a single value or a set of values for the main query to use.\nIn summary, while the terms subquery and nested query are often used interchangeably, the distinction usually lies in the specific use case and complexity. Subqueries are a subset of nested queries, primarily used within WHERE, SELECT, and FROM clauses for filtering, selection, or temporary table creation.\n\n\n\n\nApplication Programming Interfaces (APIs) serve as the conduit through which different software systems interact, based on a set of defined protocols and rules. APIs facilitate the seamless exchange of data and functionality between disparate systems, making them integral to modern software development.\n\n\nEndpoint: The specific URL where the API can be accessed. It represents the specific function or resource you wish to interact with. Headers: Contain metadata for the API call, such as content type, response format, and authentication tokens (API keys). Body: Essential for POST or PUT requests, the body contains the data to be sent to the API. Example: Consider a weather forecasting application that uses a third-party API to fetch weather data. The application might make a GET request to https://api.weather.com/v2/forecast?location=NewYork with headers containing an API key. The response could include data like temperature, humidity, and forecasts, which the application then displays to the user.\n\n\n\nIntegration: APIs enable seamless integration of third-party services or data, enriching your application’s capabilities without the need for extensive development. Automation: Tasks can be automated by scripting interactions with APIs, saving time and reducing the potential for human error. Flexibility: APIs allow for the extension of an application’s functionality, enabling customization and scalability. The Pipeline from the API or Server A data pipeline is an orchestrated set of operations designed to move and transform data from its source to a destination, such as a database or data warehouse. It typically involves stages like data extraction, transformation, and loading (ETL).\n\n\n\n\nExtraction: Data is gathered from various sources, which could include databases, APIs, or flat files. Transformation: The raw data is cleaned, enriched, and transformed into a format suitable for analysis. This might involve filtering out irrelevant data, converting data types, or aggregating information. Loading: The transformed data is stored in a structured form in a target system for further analysis or reporting. Example: A retail company might extract sales data from its online and physical stores (via APIs or direct database access), transform the data to calculate total sales by region and category, and load the results into a data warehouse for analysis.\n\n\n\nEfficiency: Automating the flow of data reduces manual tasks and speeds up data processing.\nConsistency: Ensures that all data is processed in a uniform manner, improving data quality and reliability.\nScalability: Can handle increasing volumes of data by scaling resources up or down as needed.\n\n\n\n\n\nAfter processing and analysis, data often needs to be stored in a structured and accessible form. This could involve:\n\nUpdating an operational database to reflect new insights or changes.\nStoring aggregated or analyzed data in a data warehouse, making it available for complex queries and strategic decision-making.\nCreating data marts, which are subsets of data warehouses tailored to the specific needs of different departments or functions.\n\nExample: After analyzing sales data, a business might update its inventory database to reflect items that need restocking. It might also store detailed sales analysis in a data warehouse for use in strategic planning and create a marketing-specific data mart focusing on customer purchase patterns.\nAdvantages:\n\nAccessibility: Storing processed data makes it readily accessible for decision-making, reporting, and further analysis.\nPerformance: Separating operational databases from analytical stores (data warehouses) improves performance in both systems.\nSecurity: Data warehouses and marts can implement specific security measures appropriate for the sensitivity of the stored data.\n\n\n\n\n\n\nData Enrichment is the process of augmenting your existing dataset with additional data from external sources to provide more context and depth. For instance, adding socioeconomic data to customer profiles can offer more insights into customer behavior and preferences.\nAdvantages:\nEnhanced Insights: Provides a more rounded view of the data subjects, leading to more accurate analyses and predictions. Improved Decision Making: Deeper data context can lead to better-informed strategic decisions.\n\n\n\nFeature Engineering involves creating new features from raw data to improve the performance of machine learning models. Techniques might include aggregating data points, decomposing features (e.g., splitting dates into day and month), or transforming variables (e.g., log transformations).\nAdvantages:\nModel Accuracy: Properly engineered features can significantly improve model performance. Interpretability: Well-chosen features can make models more understandable by highlighting the underlying factors that drive predictions.\n\n\n\nWhile both processes aim to improve data quality, data enrichment broadens the dataset with new data, providing more dimensions for analysis. Feature engineering, on the other hand, works on the existing dataset, transforming it into a more model-friendly format. The choice between them depends on whether the need is for more data or for transforming existing data into a more useful form. Both are crucial in different stages of the data preparation process for machine learning.",
    "crumbs": [
      "Senior Project",
      "Data Products"
    ]
  },
  {
    "objectID": "Projects/project2.html#the-key-characteristics-of-a-data-product-include",
    "href": "Projects/project2.html#the-key-characteristics-of-a-data-product-include",
    "title": "Senior Project",
    "section": "",
    "text": "Data-Driven: At its core, a data product uses data to function. This data can be static or dynamic, and the product’s effectiveness often depends on the quality and relevance of this data.\nActionable Insights: Unlike raw data or basic analyses, a data product provides insights that are actionable. This means that users can make decisions or take actions based on the output of the data product.\nUser-Focused: Data products are designed with the end-user in mind, ensuring that they are accessible, understandable, and valuable to the intended audience. This often involves user interface design, data visualization, and user experience optimization.\nAutomated: Many data products automate processes that would otherwise require manual data analysis, saving time and reducing the potential for human error.\nScalable: As data grows or as the number of users increases, a well-designed data product can scale to accommodate these changes without losing performance or reliability.",
    "crumbs": [
      "Senior Project",
      "Data Products"
    ]
  },
  {
    "objectID": "Projects/project2.html#sql",
    "href": "Projects/project2.html#sql",
    "title": "Senior Project",
    "section": "",
    "text": "Diving into SQL (Structured Query Language) is like learning the grammar of data speak. It’s the go to language for communicate to databases, asking them about the information they hold. Whether you’re inquiring about customer behaviors, financial transactions, or anything in between, SQL helps you pose the questions and understand the responses. With SQL, you can sift through mountains of data to find the nuggets of insight that drive strategies and decisions, all of this is always done by a few well-crafted queries.",
    "crumbs": [
      "Senior Project",
      "Data Products"
    ]
  },
  {
    "objectID": "Projects/project2.html#a-little-visit-down-memory-lane-with-sql",
    "href": "Projects/project2.html#a-little-visit-down-memory-lane-with-sql",
    "title": "Senior Project",
    "section": "",
    "text": "The key parts of an SQL query provides a deeper understanding of how data is manipulated and retrieved from a database. Each component plays a crucial role in shaping the output of the query.\n\n\nThe SELECT statement specifies the columns of data you want to retrieve from one or more tables in a database. It’s the starting point of most queries and defines which fields should be included in the output. You can use SELECT * to retrieve all columns or specify individual column names separated by commas for a more tailored output.\nExample:\nSELECT FirstName, LastName  FROM Employees; \nThis query retrieves the FirstName and LastName columns from the Employees table.\n\n\n\nThe FROM clause specifies the table(s) from which to retrieve data. When data needs to be combined from multiple tables, the JOIN clause is used. There are several types of joins, including INNER JOIN, LEFT JOIN, RIGHT JOIN, and FULL JOIN, each serving different purposes in terms of how tables are merged based on matching data in specified columns.\nExample:\nSELECT Orders.OrderID, Customers.CustomerName  FROM Orders  INNER JOIN Customers ON Orders.CustomerID = Customers.CustomerID; \nThis query retrieves order IDs from the Orders table and the corresponding customer names by joining the Orders table with the Customers table on the CustomerID column.\n\n\n\nThe WHERE clause filters records to include only those that meet certain conditions. It helps in narrowing down the results based on specified criteria, making the query output more relevant.\nExample:\nSELECT *  FROM Employees  WHERE Department = ‘Marketing’;\nThis query retrieves all columns for employees who work in the Marketing department.\n\n\n\nThe HAVING clause is used to filter groups of rows after they’ve been grouped by a GROUP BY clause. It’s similar to the WHERE clause but is applicable to aggregated data. HAVING is commonly used with functions like SUM(), AVG(), COUNT(), etc.\nExample:\nSELECT Department, COUNT(EmployeeID) AS NumberOfEmployees  FROM Employees  GROUP BY Department  HAVING COUNT(EmployeeID) &gt; 10; \nThis query counts the number of employees in each department and only includes those departments with more than 10 employees.\n\n\n\nThe GROUP BY clause groups rows that have the same values in specified columns into summary rows, like “find the number of employees in each department”. It is often used with aggregate functions (COUNT(), MAX(), SUM(), AVG()).\nExample:\nSELECT Department, AVG(Salary) AS AverageSalary  FROM Employees  GROUP BY Department; \nThis query calculates the average salary within each department.\n\n\n\nThe ORDER BY clause is used to sort the result set of a query by one or more columns. It can sort the results in ascending (ASC) or descending (DESC) order. If no direction is specified, ASC is assumed by default.\nExample:\nSELECT FirstName, LastName, Salary  FROM Employees  ORDER BY Salary DESC; \nThis query retrieves employee names and salaries, sorted by salary in descending order.\n\n\n\nThe LIMIT clause is used to constrain the number of rows returned by a query. This is useful in large datasets to retrieve only a subset of rows. The OFFSET clause is used in conjunction with LIMIT to skip a specific number of rows before starting to return rows from the query.\nExample:\nSELECT *  FROM Employees  LIMIT 10 OFFSET 5;  This query skips the first 5 rows and then retrieves the next 10 rows from the Employees table.",
    "crumbs": [
      "Senior Project",
      "Data Products"
    ]
  },
  {
    "objectID": "Projects/project2.html#subqueries",
    "href": "Projects/project2.html#subqueries",
    "title": "Senior Project",
    "section": "",
    "text": "A subquery is a query embedded within another query. It can be used in various parts of the main query, including the SELECT, FROM, and WHERE clauses. Subqueries are typically enclosed in parentheses and can return scalar values, single columns, single rows, or tables, depending on their placement and purpose in the main query.\nExample of a subquery in a WHERE clause:\nSELECT *  FROM Employees  WHERE DepartmentID IN (SELECT DepartmentID FROM Departments WHERE Name = ‘Research’);\nIn this example, the subquery selects DepartmentIDs from the Departments table where the department name is ‘Research’. The main query then uses these IDs to filter employees from those specific departments.",
    "crumbs": [
      "Senior Project",
      "Data Products"
    ]
  },
  {
    "objectID": "Projects/project2.html#nested-queries",
    "href": "Projects/project2.html#nested-queries",
    "title": "Senior Project",
    "section": "",
    "text": "A nested query refers to a broader category of queries that contain other queries, and it can encompass subqueries as one of its forms. The term “nested query” can also specifically refer to queries nested within FROM clauses, where a subquery is used to create a temporary table that the outer query can then join to or operate on.\nExample of a nested query in a FROM clause:\nSELECT AvgSalaries.Department, Employee.Name, Employee.Salary  FROM Employees AS Employee  JOIN (  SELECT DepartmentID, AVG(Salary) AS AverageSalary  FROM Employees  GROUP BY DepartmentID  ) AS AvgSalaries ON Employee.DepartmentID = AvgSalaries.DepartmentID  WHERE Employee.Salary &gt; AvgSalaries.AverageSalary; \nIn this example, the nested query calculates the average salary per department and creates a temporary table (AvgSalaries) with this information. The outer query then selects employees who earn more than the average salary in their respective departments.",
    "crumbs": [
      "Senior Project",
      "Data Products"
    ]
  },
  {
    "objectID": "Projects/project2.html#comparison",
    "href": "Projects/project2.html#comparison",
    "title": "Senior Project",
    "section": "",
    "text": "Scope: Subqueries can be considered a specific type of nested query. While all subqueries are nested queries, not all nested queries are subqueries, especially when considering the broader use of nesting in SQL queries.\nUsage: Subqueries are often used for returning specific values that the main query can use for comparison or filtering, whereas nested queries, particularly those in the FROM clause, are used to create temporary tables for the main query to interact with.\nComplexity: Nested queries, especially those used as temporary tables, can be more complex and might involve aggregation, grouping, or additional joins within the nested part, whereas subqueries might be simpler, returning a single value or a set of values for the main query to use.\nIn summary, while the terms subquery and nested query are often used interchangeably, the distinction usually lies in the specific use case and complexity. Subqueries are a subset of nested queries, primarily used within WHERE, SELECT, and FROM clauses for filtering, selection, or temporary table creation.",
    "crumbs": [
      "Senior Project",
      "Data Products"
    ]
  },
  {
    "objectID": "Projects/project2.html#apis",
    "href": "Projects/project2.html#apis",
    "title": "Senior Project",
    "section": "",
    "text": "Application Programming Interfaces (APIs) serve as the conduit through which different software systems interact, based on a set of defined protocols and rules. APIs facilitate the seamless exchange of data and functionality between disparate systems, making them integral to modern software development.\n\n\nEndpoint: The specific URL where the API can be accessed. It represents the specific function or resource you wish to interact with. Headers: Contain metadata for the API call, such as content type, response format, and authentication tokens (API keys). Body: Essential for POST or PUT requests, the body contains the data to be sent to the API. Example: Consider a weather forecasting application that uses a third-party API to fetch weather data. The application might make a GET request to https://api.weather.com/v2/forecast?location=NewYork with headers containing an API key. The response could include data like temperature, humidity, and forecasts, which the application then displays to the user.\n\n\n\nIntegration: APIs enable seamless integration of third-party services or data, enriching your application’s capabilities without the need for extensive development. Automation: Tasks can be automated by scripting interactions with APIs, saving time and reducing the potential for human error. Flexibility: APIs allow for the extension of an application’s functionality, enabling customization and scalability. The Pipeline from the API or Server A data pipeline is an orchestrated set of operations designed to move and transform data from its source to a destination, such as a database or data warehouse. It typically involves stages like data extraction, transformation, and loading (ETL).",
    "crumbs": [
      "Senior Project",
      "Data Products"
    ]
  },
  {
    "objectID": "Projects/project2.html#stages-of-a-data-pipeline",
    "href": "Projects/project2.html#stages-of-a-data-pipeline",
    "title": "Senior Project",
    "section": "",
    "text": "Extraction: Data is gathered from various sources, which could include databases, APIs, or flat files. Transformation: The raw data is cleaned, enriched, and transformed into a format suitable for analysis. This might involve filtering out irrelevant data, converting data types, or aggregating information. Loading: The transformed data is stored in a structured form in a target system for further analysis or reporting. Example: A retail company might extract sales data from its online and physical stores (via APIs or direct database access), transform the data to calculate total sales by region and category, and load the results into a data warehouse for analysis.\n\n\n\nEfficiency: Automating the flow of data reduces manual tasks and speeds up data processing.\nConsistency: Ensures that all data is processed in a uniform manner, improving data quality and reliability.\nScalability: Can handle increasing volumes of data by scaling resources up or down as needed.",
    "crumbs": [
      "Senior Project",
      "Data Products"
    ]
  },
  {
    "objectID": "Projects/project2.html#pushing-back-data-to-a-server-data-warehouse-or-data-mart",
    "href": "Projects/project2.html#pushing-back-data-to-a-server-data-warehouse-or-data-mart",
    "title": "Senior Project",
    "section": "",
    "text": "After processing and analysis, data often needs to be stored in a structured and accessible form. This could involve:\n\nUpdating an operational database to reflect new insights or changes.\nStoring aggregated or analyzed data in a data warehouse, making it available for complex queries and strategic decision-making.\nCreating data marts, which are subsets of data warehouses tailored to the specific needs of different departments or functions.\n\nExample: After analyzing sales data, a business might update its inventory database to reflect items that need restocking. It might also store detailed sales analysis in a data warehouse for use in strategic planning and create a marketing-specific data mart focusing on customer purchase patterns.\nAdvantages:\n\nAccessibility: Storing processed data makes it readily accessible for decision-making, reporting, and further analysis.\nPerformance: Separating operational databases from analytical stores (data warehouses) improves performance in both systems.\nSecurity: Data warehouses and marts can implement specific security measures appropriate for the sensitivity of the stored data.",
    "crumbs": [
      "Senior Project",
      "Data Products"
    ]
  },
  {
    "objectID": "Projects/project2.html#important-parts-of-machine-learning",
    "href": "Projects/project2.html#important-parts-of-machine-learning",
    "title": "Senior Project",
    "section": "",
    "text": "Data Enrichment is the process of augmenting your existing dataset with additional data from external sources to provide more context and depth. For instance, adding socioeconomic data to customer profiles can offer more insights into customer behavior and preferences.\nAdvantages:\nEnhanced Insights: Provides a more rounded view of the data subjects, leading to more accurate analyses and predictions. Improved Decision Making: Deeper data context can lead to better-informed strategic decisions.\n\n\n\nFeature Engineering involves creating new features from raw data to improve the performance of machine learning models. Techniques might include aggregating data points, decomposing features (e.g., splitting dates into day and month), or transforming variables (e.g., log transformations).\nAdvantages:\nModel Accuracy: Properly engineered features can significantly improve model performance. Interpretability: Well-chosen features can make models more understandable by highlighting the underlying factors that drive predictions.\n\n\n\nWhile both processes aim to improve data quality, data enrichment broadens the dataset with new data, providing more dimensions for analysis. Feature engineering, on the other hand, works on the existing dataset, transforming it into a more model-friendly format. The choice between them depends on whether the need is for more data or for transforming existing data into a more useful form. Both are crucial in different stages of the data preparation process for machine learning.",
    "crumbs": [
      "Senior Project",
      "Data Products"
    ]
  },
  {
    "objectID": "Projects/project4.html",
    "href": "Projects/project4.html",
    "title": "USU Hackathon in Snowflake and KOCH",
    "section": "",
    "text": "paste your elevator pitch here A SHORT (4-5 SENTENCES) PARAGRAPH THAT DESCRIBES KEY INSIGHTS TAKEN FROM METRICS IN THE PROJECT RESULTS THINK TOP OR MOST IMPORTANT RESULTS.\n\n\nRead and format project data\n# Include and execute your code here\ndf = pd.read_csv(\"https://github.com/byuidatascience/data4names/raw/master/data-raw/names_year/names_year.csv\")\n\n\nHighlight the Questions and Tasks"
  },
  {
    "objectID": "Projects/project4.html#elevator-pitch",
    "href": "Projects/project4.html#elevator-pitch",
    "title": "USU Hackathon in Snowflake and KOCH",
    "section": "",
    "text": "paste your elevator pitch here A SHORT (4-5 SENTENCES) PARAGRAPH THAT DESCRIBES KEY INSIGHTS TAKEN FROM METRICS IN THE PROJECT RESULTS THINK TOP OR MOST IMPORTANT RESULTS.\n\n\nRead and format project data\n# Include and execute your code here\ndf = pd.read_csv(\"https://github.com/byuidatascience/data4names/raw/master/data-raw/names_year/names_year.csv\")\n\n\nHighlight the Questions and Tasks"
  },
  {
    "objectID": "Projects/project4.html#questiontask-1",
    "href": "Projects/project4.html#questiontask-1",
    "title": "USU Hackathon in Snowflake and KOCH",
    "section": "QUESTION|TASK 1",
    "text": "QUESTION|TASK 1\nCOPY PASTE QUESTION|TASK 1 FROM THE PROJECT HERE\ntype your results and analysis here\n\n\nRead and format data\n# Include and execute your code here\n\n\ninclude figures in chunks and discuss your findings in the figure.\n::: {#cell-Q1 chart .cell execution_count=4}\n\nplot example\n# Include and execute your code here\nchart = px.bar(df.head(200),\n    x=\"name\", \n    y=\"AK\"\n)\nchart.show()\n\n\n                                                \nMy useless chart\n\n:::\n::: {#cell-Q1 table .cell .tbl-cap-location-top tbl-cap=‘Not much of a table’ execution_count=5}\n\ntable example\n# Include and execute your code here\nmydat = df.head(1000)\\\n    .groupby('year')\\\n    .sum()\\\n    .reset_index()\\\n    .tail(10)\\\n    .filter([\"year\", \"AK\",\"AR\"])\n\ndisplay(mydat)\n\n\n\n\n\n\n\n\n\nyear\nAK\nAR\n\n\n\n\n96\n2006\n21.0\n183.0\n\n\n97\n2007\n28.0\n153.0\n\n\n98\n2008\n36.0\n212.0\n\n\n99\n2009\n34.0\n179.0\n\n\n100\n2010\n22.0\n196.0\n\n\n101\n2011\n41.0\n148.0\n\n\n102\n2012\n28.0\n140.0\n\n\n103\n2013\n26.0\n134.0\n\n\n104\n2014\n20.0\n114.0\n\n\n105\n2015\n28.0\n121.0\n\n\n\n\n\n\n:::"
  },
  {
    "objectID": "Projects/project4.html#questiontask-2",
    "href": "Projects/project4.html#questiontask-2",
    "title": "USU Hackathon in Snowflake and KOCH",
    "section": "QUESTION|TASK 2",
    "text": "QUESTION|TASK 2\nCOPY PASTE QUESTION|TASK 2 FROM THE PROJECT HERE\ntype your results and analysis here\n\n\nRead and format data\n# Include and execute your code here\n\n\ninclude figures in chunks and discuss your findings in the figure.\n::: {#cell-Q2 chart .cell execution_count=7}\n\nplot example\n# Include and execute your code here\nchart = px.bar(df.head(200),\n    x=\"name\", \n    y=\"AK\"\n)\nchart.show()\n\n\n                                                \nMy useless chart\n\n:::\n::: {#cell-Q2 table .cell .tbl-cap-location-top tbl-cap=‘Not much of a table’ execution_count=8}\n\ntable example\n# Include and execute your code here\nmydat = df.head(1000)\\\n    .groupby('year')\\\n    .sum()\\\n    .reset_index()\\\n    .tail(10)\\\n    .filter([\"year\", \"AK\",\"AR\"])\n\ndisplay(mydat)\n\n\n\n\n\n\n\n\n\nyear\nAK\nAR\n\n\n\n\n96\n2006\n21.0\n183.0\n\n\n97\n2007\n28.0\n153.0\n\n\n98\n2008\n36.0\n212.0\n\n\n99\n2009\n34.0\n179.0\n\n\n100\n2010\n22.0\n196.0\n\n\n101\n2011\n41.0\n148.0\n\n\n102\n2012\n28.0\n140.0\n\n\n103\n2013\n26.0\n134.0\n\n\n104\n2014\n20.0\n114.0\n\n\n105\n2015\n28.0\n121.0\n\n\n\n\n\n\n:::"
  },
  {
    "objectID": "Projects/project4.html#questiontask-3",
    "href": "Projects/project4.html#questiontask-3",
    "title": "USU Hackathon in Snowflake and KOCH",
    "section": "QUESTION|TASK 3",
    "text": "QUESTION|TASK 3\nCOPY PASTE QUESTION|TASK 3 FROM THE PROJECT HERE\ntype your results and analysis here\n\n\nRead and format data\n# Include and execute your code here\n\n\ninclude figures in chunks and discuss your findings in the figure.\n::: {#cell-Q3 chart .cell execution_count=10}\n\nplot example\n# Include and execute your code here\nchart = px.bar(df.head(200),\n    x=\"name\", \n    y=\"AK\"\n)\nchart.show()\n\n\n                                                \nMy useless chart\n\n:::\n::: {#cell-Q3 table .cell .tbl-cap-location-top tbl-cap=‘Not much of a table’ execution_count=11}\n\ntable example\n# Include and execute your code here\nmydat = df.head(1000)\\\n    .groupby('year')\\\n    .sum()\\\n    .reset_index()\\\n    .tail(10)\\\n    .filter([\"year\", \"AK\",\"AR\"])\n\ndisplay(mydat)\n\n\n\n\n\n\n\n\n\nyear\nAK\nAR\n\n\n\n\n96\n2006\n21.0\n183.0\n\n\n97\n2007\n28.0\n153.0\n\n\n98\n2008\n36.0\n212.0\n\n\n99\n2009\n34.0\n179.0\n\n\n100\n2010\n22.0\n196.0\n\n\n101\n2011\n41.0\n148.0\n\n\n102\n2012\n28.0\n140.0\n\n\n103\n2013\n26.0\n134.0\n\n\n104\n2014\n20.0\n114.0\n\n\n105\n2015\n28.0\n121.0\n\n\n\n\n\n\n:::"
  },
  {
    "objectID": "Projects/pycaret.html",
    "href": "Projects/pycaret.html",
    "title": "Daniel Dominguez Portfolio",
    "section": "",
    "text": "import pandas as pd\nimport pyarrow as pa\nimport pyarrow.parquet as pq\n\nimport pycaret\n\n\nkoch = pd.read_csv(\"koch.csv\")\n\n: \n\n\n\n# Convert all values to numeric, coercing errors to NaN\nkoch['PLANNED_DELIVERY_DAYS'] = pd.to_numeric(koch['PLANNED_DELIVERY_DAYS'], errors='coerce')\n\n# Drop rows with NaN in 'PLANNED_DELIVERY_DAYS'\nkoch.dropna(subset=['PLANNED_DELIVERY_DAYS'], inplace=True)\n\n# Optionally, convert 'PLANNED_DELIVERY_DAYS' to int if you're sure they're all integers\nkoch['PLANNED_DELIVERY_DAYS'] = koch['PLANNED_DELIVERY_DAYS'].astype(int)\n\n\nkoch['TARGET'] = koch['FIRST_GR_POSTING_DATE']-koch['DELIVERY_DATE']\n\n\nkoch.columns\n\nIndex(['Unnamed: 0', 'PURCHASE_DOCUMENT_ID', 'CREATE_DATE', 'COMPANY_CODE_ID',\n       'VENDOR_ID', 'POSTAL_CD', 'RELEASE_DATE', 'PURCHASE_DOCUMENT_ITEM_ID',\n       'MATERIAL_ID', 'SUB_COMMODITY_DESC', 'MRP_TYPE_ID', 'MRP_TYPE_DESC_E',\n       'SHORT_TEXT', 'PLANT_ID', 'POR_DELIVERY_DATE', 'FIRST_GR_POSTING_DATE',\n       'DELIVERY_DATE', 'REQUESTED_DELIVERY_DATE', 'INBOUND_DELIVERY_ID',\n       'INBOUND_DELIVERY_ITEM_ID', 'PLANNED_DELIVERY_DAYS',\n       'BI_LAST_UPDATED_PURCHASE_DOCUMENT_ITEM', 'TARGET'],\n      dtype='object')\n\n\n\nkoch1 = koch.filter(['PURCHASE_DOCUMENT_ID', 'CREATE_DATE', 'COMPANY_CODE_ID',\n       'VENDOR_ID', 'POSTAL_CD', 'RELEASE_DATE', 'PURCHASE_DOCUMENT_ITEM_ID',\n       'MATERIAL_ID', 'SUB_COMMODITY_DESC', 'MRP_TYPE_ID', 'MRP_TYPE_DESC_E',\n       'SHORT_TEXT', 'PLANT_ID', 'POR_DELIVERY_DATE', 'FIRST_GR_POSTING_DATE',\n       'DELIVERY_DATE', 'REQUESTED_DELIVERY_DATE', 'INBOUND_DELIVERY_ID',\n       'INBOUND_DELIVERY_ITEM_ID', 'PLANNED_DELIVERY_DAYS',\n       'BI_LAST_UPDATED_PURCHASE_DOCUMENT_ITEM', 'TARGET'])\n\n\nkoch1 = koch1.dropna(subset=['TARGET','FIRST_GR_POSTING_DATE','DELIVERY_DATE'])\n\n\nfrom pycaret.regression import *\n\n# Assuming 'koch' is your DataFrame and 'TARGET' is your continuous target variable\ns = setup(koch1, target='TARGET', session_id=123)\n\n\n\n\n\n\n \nDescription\nValue\n\n\n\n\n0\nSession id\n123\n\n\n1\nTarget\nTARGET\n\n\n2\nTarget type\nRegression\n\n\n3\nOriginal data shape\n(1093752, 22)\n\n\n4\nTransformed data shape\n(1093752, 22)\n\n\n5\nTransformed train set shape\n(765626, 22)\n\n\n6\nTransformed test set shape\n(328126, 22)\n\n\n7\nNumeric features\n15\n\n\n8\nCategorical features\n6\n\n\n9\nRows with missing values\n99.9%\n\n\n10\nPreprocess\nTrue\n\n\n11\nImputation type\nsimple\n\n\n12\nNumeric imputation\nmean\n\n\n13\nCategorical imputation\nmode\n\n\n14\nMaximum one-hot encoding\n25\n\n\n15\nEncoding method\nNone\n\n\n16\nFold Generator\nKFold\n\n\n17\nFold Number\n10\n\n\n18\nCPU Jobs\n-1\n\n\n19\nUse GPU\nFalse\n\n\n20\nLog Experiment\nFalse\n\n\n21\nExperiment Name\nreg-default-name\n\n\n22\nUSI\n0f34\n\n\n\n\n\n\nbest = compare_models()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nInitiated\n. . . . . . . . . . . . . . . . . .\n11:25:22\n\n\nStatus\n. . . . . . . . . . . . . . . . . .\nFitting 10 Folds\n\n\nEstimator\n. . . . . . . . . . . . . . . . . .\nRandom Forest Regressor\n\n\n\n\n\n\n\n\n\n\n\n\n \nModel\nMAE\nMSE\nRMSE\nR2\nRMSLE\nMAPE\nTT (Sec)\n\n\n\n\nlr\nLinear Regression\n0.5961\n0.7773\n0.8817\n1.0000\n0.2698\n0.2449\n8.2710\n\n\nlasso\nLasso Regression\n1.0541\n2.9354\n1.7128\n1.0000\n0.3039\n0.2967\n42.1430\n\n\nridge\nRidge Regression\n0.5961\n0.7773\n0.8817\n1.0000\n0.2698\n0.2449\n5.4280\n\n\nen\nElastic Net\n1.0557\n2.9488\n1.7168\n1.0000\n0.3041\n0.2969\n40.3720\n\n\nlar\nLeast Angle Regression\n204.2237\n796141.6000\n474.5954\n1.0000\n1.3089\n39.1111\n5.2380\n\n\nllar\nLasso Least Angle Regression\n0.5959\n0.7773\n0.8817\n1.0000\n0.2699\n0.2449\n5.2900\n\n\nomp\nOrthogonal Matching Pursuit\n11022.0914\n177384917.4567\n13318.5579\n1.0000\n6.7042\n2856.8189\n5.5100\n\n\nknn\nK Neighbors Regressor\n1156.8678\n33234381.8000\n4594.7141\n1.0000\n2.2560\n104.7954\n139.0290\n\n\ndt\nDecision Tree Regressor\n13.1305\n83160.7853\n270.8086\n1.0000\n0.7952\n1.7258\n17.1310\n\n\npar\nPassive Aggressive Regressor\n58965.1916\n5959836809.1409\n74264.7995\n0.9999\n8.1041\n16304.4583\n32.1670\n\n\nhuber\nHuber Regressor\n2494557.2634\n50415582117965.3438\n7100352.0510\n-0.1408\n6.4081\n0.9934\n6.3120\n\n\nbr\nBayesian Ridge\n5986988.4998\n741106270732429.8750\n9100441.0089\n-15.8950\n2.7935\n1263079.0680\n8.2760\n\n\n\n\n\n\n\n\n: \n\n\n\nexp.setup(data, target = 'species', session_id = 123)\n\npycaret.classification.oop.ClassificationExperiment\n\n\n\ndef convert_csv_to_parquet(input_file_path, output_file_path):\n    # Read CSV file into a Pandas DataFrame\n    df = pd.read_csv(input_file_path)\n    \n    # Convert Pandas DataFrame to PyArrow Table\n    table = pa.Table.from_pandas(df)\n\n    # Write PyArrow Table to Parquet file\n    pq.write_table(table, output_file_path)\n\n# Input and output file paths\ninput_file_path = 'koch1.csv'\noutput_file_path = 'koch.parquet'\n\n# Call the function\nconvert_csv_to_parquet(input_file_path, output_file_path)\n\n\nimport pyarrow.parquet as pq\n\n# Path to your Parquet file\nparquet_file_path = 'koch.parquet'\n\n# Load the Parquet file into a PyArrow Table\ntable = pq.read_table(parquet_file_path)\n\n# Print the PyArrow Table\nprint(table)\n\nOSError: Could not open Parquet input source 'koch.parquet': Couldn't deserialize thrift: TProtocolException: Invalid data\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "resume.html",
    "href": "resume.html",
    "title": "Daniel Dominguez",
    "section": "",
    "text": "385.328.1955 | danndch@gmail.com |  LinkedIn | Data Science Program"
  },
  {
    "objectID": "resume.html#work-experience.",
    "href": "resume.html#work-experience.",
    "title": "Daniel Dominguez",
    "section": "Work Experience.",
    "text": "Work Experience.\n\nData Analyst.\nBYUI Communications (Rexburg, ID)\nOct 2021- April 2024\n\nDesigned and implemented a GA4-based analytics funnel to analyze the University Admissions process, understanding the story and identifying key stages where applicants face delays or drop out. Utilized insights to propose strategic improvements, significantly enhancing process efficiency and applicant experience.\n\nEnabled data driven-budget reporting and campaign monitoring across 15+ departments by implementing a Power BI that processes 30K+ text-messages monthly through a pipeline from an API using Python.\n\nDeveloped and maintained a dynamic dashboard in Power BI for analysis of 90K+ web-scraped pages, on the renovated website, leading to continuous improvements in UX, SEO by identifying 404 errors, obsolete links, and tracking tags.\n\nGenerated and maintain Looker reports based on Google Analytics 4 GA4 to monitor and evaluate engagement data for 60,000+ students, 200,000+ alumni, and other users, translating insights into meaningful visual reports for 33 different University departments.\n\n\n\nData Science Tutor.\nBYUI Math Department (Rexburg, ID)\nSep 2023 - April 2024\n\nAid students with their questions to meet rubrics on assignments from different courses such as: Data Science Programing, Data Wrangling, Big Data, Machine Learning. The technologies addressed at the Data Science Lab was: R, Python, Pyspark, Pytorch."
  },
  {
    "objectID": "resume.html#projects-and-consulting.",
    "href": "resume.html#projects-and-consulting.",
    "title": "Daniel Dominguez",
    "section": "Projects and Consulting.",
    "text": "Projects and Consulting.\n\nSEPROCESA S.A. DE C.V. (Consulting).\nMay 2024 - Present\n\n\nTransformed and optimized their database to create maintenance Shiny reports with sensors in money-counting machines, and software reducing downtime and extending lifespan on more than 200,000 equipment pieces of equipment.\n\nCustomized sales trend reports and developed a regression model (84% accuracy) for demand forecasting across 250 different clients.\n\n\n\n\nSpecial Topics SQL Class for Data Science.\nJan - Apr 2024\n\n\nDesigned a curriculum for Senior class, blending Data Science and Engineering to teach ETL/ELT processes using SQL, incorporating real-world industry projects.\n\n\n\nKOCH with Utah State University Hackathon.\nMarch 2024\n\n\nBuilt a predictive dashboard with Streamlit and Snowflake to forecast shipment delivery times for KOCH, applying SnowSpark, SQL, and Python to wrangle large datasets with an 86% accuracy on 1M rows of data.\n\n\n\nCentenario Rent-a-Car (BYU-I Data Science Society)\nJan - Apr 2024\n* Automated a Power BI dashboard to analyze income sources, car usage, driver data, and destinations for a car rental company. Implemented ETL to transform Excel data into a standardized format for comprehensive analysis.\n\n\niWorq Systems (Consulting Project).\nApr-Jul 2023\n\n\nDeveloped a Power BI dashboard to forecast client service cancellations, integrating R and AWS-S3 to embed a 3-month risk forecast.\n\n\n\n\nWhat’s That Fish.\nApr-Jul 2023\n\n\nDeveloped a fish identification app utilizing Convolutional Neural Networks (92% accuracy) to recognize trout species from photos. Deployed the app on Google Firebase, enabling seamless photo storage and database integration for user interactions. \n\n\n\nSinclair Tire Dealership.\n(BYUI Data Science Society) Apr-Jul 2023\n\n\nDeveloped on Streamlit an analytics tool to Web-scrape a given URL to aid a tire dealership from a given URL and retrieve information to help establish retail price according to the competition.\n\n\n\n\nJohn Deer - Stotz.\n(BYUI Data Science Society) Jan-Apr 2023\n\n\nEstablished a data pipeline to Power BI from John Deere’s API, enabling strategic agricultural analysis for data-driven decision-making in equipment management.\n\n\n\n\nBeehive Credit Union.\n(BYUI Data Science Society) Sep-Dec 2022\n\n\nEmployed Python, Spark, and TensorFlow to create a machine learning model, enabling Beehive Credit Union to make informed decisions about new possible store locations by analyzing National Census Data and institutional data."
  },
  {
    "objectID": "resume.html#skills",
    "href": "resume.html#skills",
    "title": "Daniel Dominguez",
    "section": "Skills",
    "text": "Skills\nData Analytics, Wrangling, and Modeling: SQL, Python (Pandas, Numpy, PySpark, Polars), R (CAR, Mosaic, Tidyverse), Machine Learning (TensorFlow, PyTorch, PyCaret).\nData Visualization and BI Tools: Power BI (DAX and M), Tableau, Python (Seaborn, Matplotlib, Streamlit), R (ggplot, shiny).\nDatabase Design and Administration: SQL, MySQL, Relational and Non-relational databases.\nProgramming: Python, and C#.\nOther Technical Skills: AWS (S3 and Athena), GA4, Google Big Query, Looker, GitHub, Jupyter, DataBricks, SnowFlake, APIs."
  },
  {
    "objectID": "resume.html#education.",
    "href": "resume.html#education.",
    "title": "Daniel Dominguez",
    "section": "Education.",
    "text": "Education.\n\nB.Sc. Data Science\nBrigham Young University Idaho (Rexburg, ID) Jan 2020 July 2024 * Minor: Statistics, and Business Analytics. * Certificates: Computer Programming."
  },
  {
    "objectID": "resume.html#volunteer",
    "href": "resume.html#volunteer",
    "title": "Daniel Dominguez",
    "section": "Volunteer",
    "text": "Volunteer\n\nData Science Society President Jan-April 2024\nBYU-I Data Science Society Rexburg, ID\n\n\nLed 15 project managers and supervised 11 data science projects, connecting students with hands-on experiences in data science competitions and real-world projects."
  }
]