{
  "hash": "230cf2c275abc173e02b56f79ccfcb35",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"Senior Project Post 1\"\nsubtitle: \"The Data Science Engineer\"\nauthor: \"Daniel Dominguez\"\nformat:\n  html:\n    self-contained: true\n    page-layout: full\n    title-block-banner: true\n    toc: true\n    toc-depth: 3\n    toc-location: body\n    number-sections: false\n    html-math-method: katex\n    code-fold: true\n    code-summary: \"Show the code\"\n    code-overflow: wrap\n    code-copy: hover\n    code-tools:\n        source: false\n        toggle: true\n        caption: See code\nexecute: \n  warning: false\n    \n---\n\n\n# Data Science delveig into Data Engineering \n\nIn the rapidly evolving world of Data, where the ability to turn vast amounts of raw data into actionable insights is highly prized. You may know Python, you may know R, but mastering the language of data and understanding the architecture that supports it are not just an advantage but it is essential. SQL and data engineering, is a realm that it is close to Data Science and there are some skills that will set you apart in your future career.\n\n## Why SQL?\n\nSQL, or Structured Query Language, is the cornerstone of data manipulation and retrieval. It's the bridge between the questions you seek to answer and the data that contains those answers. Whether it's understanding customer behavior, predicting market trends, or improving operational efficiency, SQL empowers you to:\n\n* Access Critical Data Quickly: Navigate through complex databases with ease, ensuring that no source of insight is beyond your reach.\n* Communicate Effectively with Data: Speak the universal language understood by databases worldwide, making your analyses more versatile and impactful.\n* Refine Your Analytical Precision: Hone your ability to ask nuanced questions of your data, enabling more precise, targeted analyses.\n\n\n## The Value of Data Engineering Skills\n\nAs Data Scientists, our vision is only as clear as the data at our disposal. Data engineering skills could ensure that the data we rely on is accurate, accessible, and ready for analysis. By understanding data pipelines, storage solutions, and data processing, we will:\n\n* Enhance Data Quality and Accessibility: Ensure that the data you use for your analyses is clean, well-structured, and readily available, reducing the time from question to insight.\n* Scale Your Data Efforts: Tackle larger datasets and more complex problems with confidence, knowing that your data infrastructure is robust and scalable.\n* Bridge the Gap Between Data and Decision-Making: Play that role in translating data insights into strategic actions, making your contributions invaluable to any team.\n* You don't want to sit and wait: If you provide to your employer a solution more than waiting for the Data Engineer to give you the data you need to work, what if you could provide that solution as well, not having to depend a 100% on someone else gives you freedome and a higher value in the market.\n\n\n## Your Path to Becoming a Data Science Hybrid with Data Engineering \n\nEmbracing SQL and and some other data engineering concepts is not just about enlarging your technical toolbox; it's about expanding your problem-solving skillset. In a data-driven world, these skills enable you to navigate and make sense of the multiple situations, turning data into information which will become decisions that drive progress.\n\n# Now where is this being used?\n\n## Top industry\n\nLets take a look at two positions open at TESLA for an internship, they are in the same job post but they ask for two different positions.\n\n![image](C:/Users/Dann_/OneDrive - BYU-Idaho/BYU Idaho/11th Semester W24/Senior Project/ddominguez_portfolio/Images/data_science.png)\n![image](C:/Users/Dann_/OneDrive - BYU-Idaho/BYU Idaho/11th Semester W24/Senior Project/ddominguez_portfolio/Images/capture.png)\n\n\nIf we were to look into a Ven Diagram we could picture Data Science on one of the sides and Data Engineering in the other, this is it could be taken from out the descriptions and put it into the Data Science or Data Engineering side\n\n### Skills and Tasks Specific to a Data Scientist:\n\nStatistical Learning and Bayesian Models: The mention of statistical learning and Bayesian models is more aligned with the data scientist role, where a deep understanding of statistical methods and the ability to apply these methods to extract insights from data is crucial.\nAnswering Complex Questions on Fleet Usage and Behavior: This involves not just accessing and processing data but also interpreting it in a way that provides actionable insights, which is a core part of a data scientist's role.\nMLOps Skills: While MLOps can be relevant to both roles, in the context of deploying and managing machine learning models, it leans more towards the data scientist side, especially when it involves model development, validation, and iteration based on statistical principles.\n\n\n### Skills and Tasks Specific to a Data Engineer:\n\nBuilding Scalable Data Pipelines: The emphasis on building scalable data pipelines for deploying fleet health monitoring models is a essential data engineering task, focusing on the infrastructure that allows for efficient data flow and processing.\nData ELT Pipelines Using Airflow: Creating and maintaining data Extract, Load, Transform (ELT) pipelines, especially with tools like Airflow, is a key data engineering responsibility, ensuring that data is regularly and reliably prepared for analysis.\nQuery Optimization and Advanced SQL: While SQL is used by both data scientists and data engineers, the focus on optimizing and undertaking advanced SQL queries, especially on massive datasets, is more characteristic of data engineering, which often deals with the optimization of data access and processing.\n\n### Skills and Tasks Shared Between Both Roles:\n\nData Analytics and Infrastructure: Both roles involve a level of responsibility for employing data to drive insights and actions, which includes the infrastructure that supports data analytics.\nUse of Tableau and Similar Tools for Dashboards: The creation and maintenance of analytics dashboards are common to both roles, although the focus might differ; data scientists would be more involved in the interpretation of the data, while data engineers would ensure the data is accurately and efficiently presented.\nWorking with Cross-Functional Teams: Collaboration with cross-functional teams to support design cycles, provide insights, and support decision-making is essential for both roles, though the nature of the support and insights provided may vary.\n\n### The difference\n\nThe job description establishes a range for the payment, it goes from $20-60 an hour. Just imagine that for being able to understand skills like ETL, stablishing a pipeline (Which is ETL), and sharp your SQL could give you 3 times more money, and you could fix your problems in less time than waiting for someone to complement this tasks, if you are hired of course.\n\n\n\n# Material\n\n## Difference on the Data Structures\n\n### Data Lake\n#### Attributes:\n\nA Data Lake is a vast pool of raw, unstructured, and structured data stored in its native format.\nIt is designed to store a large amount of data without a predefined schema, allowing you to keep all your data in one place without having to structure it first.\n\n#### Benefits:\n\n* Flexibility: You can store data of all types and structures, making it highly adaptable to changes.\n* Scalability: Capable of handling massive volumes of data, from gigabytes to petabytes.\n* Cost-Effectiveness: Often built on inexpensive storage solutions, making it economical for storing vast amounts of data.\n\n\n#### Uses:\n\n* Ideal for big data and real-time analytics projects.\n* Useful for organizations that want to store all their data without initially knowing how they will use it.\n\n\n### Data Warehouse\n#### Attributes:\n\nA Data Warehouse is a centralized repository for structured, filtered data that has already been processed for a specific purpose.\nIt is highly structured and uses a schema-on-write approach, meaning the data is organized and formatted at the time of entry.\n\n#### Benefits:\n\n* Improved Data Quality and Consistency: Due to its structured nature, data is cleaned and transformed, ensuring reliability.\n* Performance: Optimized for complex queries and data analysis, providing fast access to insights.\n* Historical Intelligence: Ideal for storing historical data, enabling trend analysis over time.\n\n#### Uses:\n\n* Suitable for business intelligence, reporting, and data analysis purposes.\n* Used by organizations that need to analyze their data comprehensively to make informed decisions.\n\n\n### Data Mart\n#### Attributes:\n\nA Data Mart is a subset of a data warehouse designed to focus on a specific line of business, department, or subject area.\nIt is more tailored and subject-oriented, containing only the data relevant to a particular group or purpose.\n\n#### Benefits:\n\n* Increased Accessibility: More accessible to users due to its focus on a specific domain, making it easier to retrieve relevant information.\n* Faster Query Performance: Smaller size leads to quicker data retrieval, improving efficiency.\n* User-Friendly: Easier for non-technical users to interact with and understand since it's focused on a specific area.\n\n#### Uses:\n\n* Ideal for department-specific reports and analyses, such as sales, finance, or marketing data.\n* Used by departments within organizations that need quick, easy access to specific, relevant data.\n\n\n### Data Fabric\n#### Attributes:\n\n* Data Fabric provides a unified layer of data and connectivity across different platforms and environments, using various technologies and approaches (like metadata).\n* It is designed to provide consistent capabilities across endpoints in a hybrid and multi-cloud environment.\n\n#### Benefits:\n\n* Agility: Enables quick access to data across the organization, regardless of its location.\n* Interoperability: Facilitates seamless data sharing and integration across diverse systems and platforms.\n\n#### Uses:\n\n* Ideal for organizations with complex data ecosystems looking to streamline data access, integration, and management across multiple environments.\n\n\n# ETL VS ELT\n\n## ETL (Extract, Transform, Load)\n\n* Extract: Data is sourced from various origins, such as databases, CRM systems, flat files, or APIs. This stage involves querying data sources, often using SQL for structured databases, and aggregating the data for further processing.\n* Transform: This critical phase involves a series of operations such as cleansing (removing or correcting data errors), deduplication (eliminating duplicate records), normalization (structuring data to reduce redundancy), and aggregation (summarizing detailed data). Complex transformations might include pivoting data formats, merging datasets, or applying business logic to derive new calculated fields. This step is usually performed in a dedicated staging area or a transformation engine, using tools and languages suitable for data manipulation, including SQL, Python, or specialized ETL tools.\n* Load: The final step involves writing the transformed data into a target data warehouse or database. This phase is carefully managed to maintain data integrity and optimize for query performance, often involving SQL operations to insert data into structured schemas designed for analysis.\n\n## Advantages and Challenges:\n\n* Control and Quality: The ETL process allows for extensive control over the transformation logic, ensuring data quality and consistency. This is particularly important in regulated industries or where data accuracy is critical.\n* Performance Consideration: ETL can relieve the target data warehouse from the processing load of transformations. However, the transformation phase can become a bottleneck if not well-optimized, especially with large volumes of data.\n* Tooling and Complexity: While ETL offers robustness and control, it can also introduce complexity, requiring specialized tools and expertise to manage the transformation logic and workflows.\n\n\n## When to Use ETL:\n\n* When data quality and transformation logic are complex and need to be managed meticulously.\n* In environments where the computational capabilities of the target data warehouse are limited or costly.\n* When data processing needs to be done before loading to ensure compliance, security, or data privacy.\n\n\n# ELT (Extract, Load, Transform)\n\n* Extract: Similar to ETL, data is extracted from various sources. The focus here is on speed and efficiency, getting data into the target system as quickly as possible.\n* Load: The extracted data is loaded into the data warehouse in its raw form. Modern data warehouses are designed to handle large volumes of unstructured or semi-structured data, making them well-suited for this approach.\n* Transform: Transformation happens after the data is already in the target system, leveraging the powerful processing capabilities of modern data warehouses. SQL is often used for transformation within the warehouse, allowing data scientists and analysts to apply transformations using familiar query languages. This approach offers the flexibility to transform data as needed for different analyses without pre-defining those transformations.\n\n\n## Advantages and Challenges:\n\n* Scalability and Flexibility: ELT leverages the scalable compute resources of modern cloud-based data warehouses, handling vast amounts of data efficiently. The flexibility to transform data on-the-fly supports agile analytics and data exploration.\n* Speed: By loading data directly into the warehouse, ELT can make data available for analysis more quickly. This is advantageous in fast-paced environments where timely insights are critical.\n* Simplification and Cost: ELT can simplify the data pipeline by reducing the need for a separate transformation layer, potentially lowering infrastructure and maintenance costs. However, this approach requires a robust data warehouse capable of handling intensive compute tasks.\n\n\n## When to Use ELT:\n\n* In big data scenarios where the volume, velocity, and variety of data exceed traditional database capabilities.\n* When using cloud-based data warehouses with high processing power, where the cost of compute resources is outweighed by the need for flexibility and speed.\n* In agile environments where the requirements for data models and analyses change frequently, necessitating a flexible approach to data transformation.\n\n# SQL's Role in ETL and ELT\nSQL plays a crucial role in both ETL and ELT processes, particularly in the transformation phase. In ETL, SQL is used within the transformation engine or staging area to manipulate and prepare data for loading. In ELT, SQL is utilized within the data warehouse itself to transform the data after it has been loaded, taking advantage of the warehouse's compute capabilities.\n\n## The Ongoing Debate\nThe debate between ETL and ELT often centers on trade-offs between control and flexibility, scalability and complexity, and the upfront costs versus long-term benefits. The right choice depends on specific project requirements, data characteristics, available infrastructure, and strategic priorities.\n\nETL is traditionally preferred for scenarios where data integrity, privacy, and compliance are paramount, and where transformations need to be tightly controlled and monitored. It's ideal for environments where the cleanliness and structure of the data are critical before it can be stored or analyzed, such as in financial services or healthcare sectors where data accuracy and privacy are non-negotiable.\n\nELT, on the other hand, is favored in contexts where the agility and flexibility of data analysis are prioritized. The ability to store raw data and transform it as needed allows for a more explorative and iterative approach to data analytics. This is particularly beneficial in dynamic industries like e-commerce or social media, where business needs can change rapidly, and the ability to pivot and explore new data models quickly is a competitive advantage.\n\n\n## Practical Examples\n\n\nETL Example: A financial institution might use ETL to integrate customer transaction data from various sources. The transformation stage would include validating transaction codes, converting currencies, and anonymizing personal information to comply with data protection regulations before loading the clean, transformed data into a data warehouse for analysis and reporting.\n\nELT Example: A digital marketing firm might use ELT to analyze web traffic data. Raw clickstream data is ingested directly into a cloud-based data warehouse, and SQL queries are used to transform this data on-demand, creating different views for analyzing user behavior, campaign performance, and website engagement.\n\n\n\n\n\n\n\n# Sources\n\nhttps://learning.oreilly.com/playlists/102dbc35-5c2c-47a2-9a9d-6711d369832e/\n\n",
    "supporting": [
      "project1_files"
    ],
    "filters": [],
    "includes": {}
  }
}