[
  {
    "objectID": "story_telling.html",
    "href": "story_telling.html",
    "title": "about me",
    "section": "",
    "text": "MarkDown Basics"
  },
  {
    "objectID": "story_telling.html#title-2-header",
    "href": "story_telling.html#title-2-header",
    "title": "about me",
    "section": "",
    "text": "MarkDown Basics"
  },
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "Senior Project",
    "section": "",
    "text": "The objective of this project is to design and construct an educational framework that helps Data Science students develop skills typically used by Data Engineers. This project aims to introduce students to scenarios and a pace of research and learning that keeps a scholarly mindset while incorporating aspects of a professional environment.\n\n\nThis outline has been put together from different sources including academic literature, industry job postings, and IT articles. The mix of this different skills it might be relevant and could boost the value for future students in the job market, thereby enhancing the employability of future graduates. The proposed topics are the following:\n\nData Science vs. Data Engineering\nTEL, ELT, ETL, ETLT?\nData Pipelines and Workflows\nNoSQL Databases?\nData Transformation Techniques with SQL\nCloud-based Data Services\nPerformance Optimization\nData Quality, Testing, and Monitoring\nBig Data Technologies\nData Ingestion Tools\nContainerization and Virtualization\nData Governance and Ethics\nAdvanced Analytics and Machine Learning Operations (MLOps)\nReal-time Data Processing and Analytics\nData Visualization and Reporting\nAPIs for Data Science\nCollaborative Data Science\nData Science vs. Data Engineering\nData Engineering for DS\nSQL Work\nData Maturity",
    "crumbs": [
      "Senior Project"
    ]
  },
  {
    "objectID": "projects.html#potential-topics",
    "href": "projects.html#potential-topics",
    "title": "Senior Project",
    "section": "",
    "text": "This outline has been put together from different sources including academic literature, industry job postings, and IT articles. The mix of this different skills it might be relevant and could boost the value for future students in the job market, thereby enhancing the employability of future graduates. The proposed topics are the following:\n\nData Science vs. Data Engineering\nTEL, ELT, ETL, ETLT?\nData Pipelines and Workflows\nNoSQL Databases?\nData Transformation Techniques with SQL\nCloud-based Data Services\nPerformance Optimization\nData Quality, Testing, and Monitoring\nBig Data Technologies\nData Ingestion Tools\nContainerization and Virtualization\nData Governance and Ethics\nAdvanced Analytics and Machine Learning Operations (MLOps)\nReal-time Data Processing and Analytics\nData Visualization and Reporting\nAPIs for Data Science\nCollaborative Data Science\nData Science vs. Data Engineering\nData Engineering for DS\nSQL Work\nData Maturity",
    "crumbs": [
      "Senior Project"
    ]
  },
  {
    "objectID": "Projects/project4.html",
    "href": "Projects/project4.html",
    "title": "Client Report - [Insert Project Title]",
    "section": "",
    "text": "paste your elevator pitch here A SHORT (4-5 SENTENCES) PARAGRAPH THAT DESCRIBES KEY INSIGHTS TAKEN FROM METRICS IN THE PROJECT RESULTS THINK TOP OR MOST IMPORTANT RESULTS.\n\n\nRead and format project data\n# Include and execute your code here\ndf = pd.read_csv(\"https://github.com/byuidatascience/data4names/raw/master/data-raw/names_year/names_year.csv\")\n\n\nHighlight the Questions and Tasks"
  },
  {
    "objectID": "Projects/project4.html#elevator-pitch",
    "href": "Projects/project4.html#elevator-pitch",
    "title": "Client Report - [Insert Project Title]",
    "section": "",
    "text": "paste your elevator pitch here A SHORT (4-5 SENTENCES) PARAGRAPH THAT DESCRIBES KEY INSIGHTS TAKEN FROM METRICS IN THE PROJECT RESULTS THINK TOP OR MOST IMPORTANT RESULTS.\n\n\nRead and format project data\n# Include and execute your code here\ndf = pd.read_csv(\"https://github.com/byuidatascience/data4names/raw/master/data-raw/names_year/names_year.csv\")\n\n\nHighlight the Questions and Tasks"
  },
  {
    "objectID": "Projects/project4.html#questiontask-1",
    "href": "Projects/project4.html#questiontask-1",
    "title": "Client Report - [Insert Project Title]",
    "section": "QUESTION|TASK 1",
    "text": "QUESTION|TASK 1\nCOPY PASTE QUESTION|TASK 1 FROM THE PROJECT HERE\ntype your results and analysis here\n\n\nRead and format data\n# Include and execute your code here\n\n\ninclude figures in chunks and discuss your findings in the figure.\n::: {#cell-Q1 chart .cell execution_count=4}\n\nplot example\n# Include and execute your code here\nchart = px.bar(df.head(200),\n    x=\"name\", \n    y=\"AK\"\n)\nchart.show()\n\n\n                                                \nMy useless chart\n\n:::\n::: {#cell-Q1 table .cell .tbl-cap-location-top tbl-cap=‘Not much of a table’ execution_count=5}\n\ntable example\n# Include and execute your code here\nmydat = df.head(1000)\\\n    .groupby('year')\\\n    .sum()\\\n    .reset_index()\\\n    .tail(10)\\\n    .filter([\"year\", \"AK\",\"AR\"])\n\ndisplay(mydat)\n\n\n\n\n\n\n\n\n\nyear\nAK\nAR\n\n\n\n\n96\n2006\n21.0\n183.0\n\n\n97\n2007\n28.0\n153.0\n\n\n98\n2008\n36.0\n212.0\n\n\n99\n2009\n34.0\n179.0\n\n\n100\n2010\n22.0\n196.0\n\n\n101\n2011\n41.0\n148.0\n\n\n102\n2012\n28.0\n140.0\n\n\n103\n2013\n26.0\n134.0\n\n\n104\n2014\n20.0\n114.0\n\n\n105\n2015\n28.0\n121.0\n\n\n\n\n\n\n:::"
  },
  {
    "objectID": "Projects/project4.html#questiontask-2",
    "href": "Projects/project4.html#questiontask-2",
    "title": "Client Report - [Insert Project Title]",
    "section": "QUESTION|TASK 2",
    "text": "QUESTION|TASK 2\nCOPY PASTE QUESTION|TASK 2 FROM THE PROJECT HERE\ntype your results and analysis here\n\n\nRead and format data\n# Include and execute your code here\n\n\ninclude figures in chunks and discuss your findings in the figure.\n::: {#cell-Q2 chart .cell execution_count=7}\n\nplot example\n# Include and execute your code here\nchart = px.bar(df.head(200),\n    x=\"name\", \n    y=\"AK\"\n)\nchart.show()\n\n\n                                                \nMy useless chart\n\n:::\n::: {#cell-Q2 table .cell .tbl-cap-location-top tbl-cap=‘Not much of a table’ execution_count=8}\n\ntable example\n# Include and execute your code here\nmydat = df.head(1000)\\\n    .groupby('year')\\\n    .sum()\\\n    .reset_index()\\\n    .tail(10)\\\n    .filter([\"year\", \"AK\",\"AR\"])\n\ndisplay(mydat)\n\n\n\n\n\n\n\n\n\nyear\nAK\nAR\n\n\n\n\n96\n2006\n21.0\n183.0\n\n\n97\n2007\n28.0\n153.0\n\n\n98\n2008\n36.0\n212.0\n\n\n99\n2009\n34.0\n179.0\n\n\n100\n2010\n22.0\n196.0\n\n\n101\n2011\n41.0\n148.0\n\n\n102\n2012\n28.0\n140.0\n\n\n103\n2013\n26.0\n134.0\n\n\n104\n2014\n20.0\n114.0\n\n\n105\n2015\n28.0\n121.0\n\n\n\n\n\n\n:::"
  },
  {
    "objectID": "Projects/project4.html#questiontask-3",
    "href": "Projects/project4.html#questiontask-3",
    "title": "Client Report - [Insert Project Title]",
    "section": "QUESTION|TASK 3",
    "text": "QUESTION|TASK 3\nCOPY PASTE QUESTION|TASK 3 FROM THE PROJECT HERE\ntype your results and analysis here\n\n\nRead and format data\n# Include and execute your code here\n\n\ninclude figures in chunks and discuss your findings in the figure.\n::: {#cell-Q3 chart .cell execution_count=10}\n\nplot example\n# Include and execute your code here\nchart = px.bar(df.head(200),\n    x=\"name\", \n    y=\"AK\"\n)\nchart.show()\n\n\n                                                \nMy useless chart\n\n:::\n::: {#cell-Q3 table .cell .tbl-cap-location-top tbl-cap=‘Not much of a table’ execution_count=11}\n\ntable example\n# Include and execute your code here\nmydat = df.head(1000)\\\n    .groupby('year')\\\n    .sum()\\\n    .reset_index()\\\n    .tail(10)\\\n    .filter([\"year\", \"AK\",\"AR\"])\n\ndisplay(mydat)\n\n\n\n\n\n\n\n\n\nyear\nAK\nAR\n\n\n\n\n96\n2006\n21.0\n183.0\n\n\n97\n2007\n28.0\n153.0\n\n\n98\n2008\n36.0\n212.0\n\n\n99\n2009\n34.0\n179.0\n\n\n100\n2010\n22.0\n196.0\n\n\n101\n2011\n41.0\n148.0\n\n\n102\n2012\n28.0\n140.0\n\n\n103\n2013\n26.0\n134.0\n\n\n104\n2014\n20.0\n114.0\n\n\n105\n2015\n28.0\n121.0\n\n\n\n\n\n\n:::"
  },
  {
    "objectID": "Projects/project2.html",
    "href": "Projects/project2.html",
    "title": "Senior Project",
    "section": "",
    "text": "A data product in Data Science is a product that facilitates an end goal through the use of data. It is essentially a technical asset that makes use of data as its core component to solve a specific problem or fulfill a particular need for its users. Data products can be as simple as a report generated from data analyses, or as complex as a machine learning model that powers a recommendation system in an application.\nData products are the bridge between raw data and practical applications. They are essential for applying the theoretical aspects of data science to solve real-world problems. By turning data into actionable insights, data products enable businesses and organizations to make informed decisions, optimize operations, and enhance user experiences. As such, the development and management of data products is a crucial aspect of the data science field, requiring a blend of technical skills, domain expertise, and an understanding of user needs.\n\n\n\nData-Driven: At its core, a data product uses data to function. This data can be static or dynamic, and the product’s effectiveness often depends on the quality and relevance of this data.\nActionable Insights: Unlike raw data or basic analyses, a data product provides insights that are actionable. This means that users can make decisions or take actions based on the output of the data product.\nUser-Focused: Data products are designed with the end-user in mind, ensuring that they are accessible, understandable, and valuable to the intended audience. This often involves user interface design, data visualization, and user experience optimization.\nAutomated: Many data products automate processes that would otherwise require manual data analysis, saving time and reducing the potential for human error.\nScalable: As data grows or as the number of users increases, a well-designed data product can scale to accommodate these changes without losing performance or reliability.\n\n\n\n\nDiving into SQL (Structured Query Language) is like learning the grammar of data speak. It’s the go to language for communicate to databases, asking them about the information they hold. Whether you’re inquiring about customer behaviors, financial transactions, or anything in between, SQL helps you pose the questions and understand the responses. With SQL, you can sift through mountains of data to find the nuggets of insight that drive strategies and decisions, all of this is always done by a few well-crafted queries.\n\n\n\nThe key parts of an SQL query provides a deeper understanding of how data is manipulated and retrieved from a database. Each component plays a crucial role in shaping the output of the query.\n\n\nThe SELECT statement specifies the columns of data you want to retrieve from one or more tables in a database. It’s the starting point of most queries and defines which fields should be included in the output. You can use SELECT * to retrieve all columns or specify individual column names separated by commas for a more tailored output.\nExample:\nSELECT FirstName, LastName  FROM Employees; \nThis query retrieves the FirstName and LastName columns from the Employees table.\n\n\n\nThe FROM clause specifies the table(s) from which to retrieve data. When data needs to be combined from multiple tables, the JOIN clause is used. There are several types of joins, including INNER JOIN, LEFT JOIN, RIGHT JOIN, and FULL JOIN, each serving different purposes in terms of how tables are merged based on matching data in specified columns.\nExample:\nSELECT Orders.OrderID, Customers.CustomerName  FROM Orders  INNER JOIN Customers ON Orders.CustomerID = Customers.CustomerID; \nThis query retrieves order IDs from the Orders table and the corresponding customer names by joining the Orders table with the Customers table on the CustomerID column.\n\n\n\nThe WHERE clause filters records to include only those that meet certain conditions. It helps in narrowing down the results based on specified criteria, making the query output more relevant.\nExample:\nSELECT *  FROM Employees  WHERE Department = ‘Marketing’;\nThis query retrieves all columns for employees who work in the Marketing department.\n\n\n\nThe HAVING clause is used to filter groups of rows after they’ve been grouped by a GROUP BY clause. It’s similar to the WHERE clause but is applicable to aggregated data. HAVING is commonly used with functions like SUM(), AVG(), COUNT(), etc.\nExample:\nSELECT Department, COUNT(EmployeeID) AS NumberOfEmployees  FROM Employees  GROUP BY Department  HAVING COUNT(EmployeeID) &gt; 10; \nThis query counts the number of employees in each department and only includes those departments with more than 10 employees.\n\n\n\nThe GROUP BY clause groups rows that have the same values in specified columns into summary rows, like “find the number of employees in each department”. It is often used with aggregate functions (COUNT(), MAX(), SUM(), AVG()).\nExample:\nSELECT Department, AVG(Salary) AS AverageSalary  FROM Employees  GROUP BY Department; \nThis query calculates the average salary within each department.\n\n\n\nThe ORDER BY clause is used to sort the result set of a query by one or more columns. It can sort the results in ascending (ASC) or descending (DESC) order. If no direction is specified, ASC is assumed by default.\nExample:\nSELECT FirstName, LastName, Salary  FROM Employees  ORDER BY Salary DESC; \nThis query retrieves employee names and salaries, sorted by salary in descending order.\n\n\n\nThe LIMIT clause is used to constrain the number of rows returned by a query. This is useful in large datasets to retrieve only a subset of rows. The OFFSET clause is used in conjunction with LIMIT to skip a specific number of rows before starting to return rows from the query.\nExample:\nSELECT *  FROM Employees  LIMIT 10 OFFSET 5;  This query skips the first 5 rows and then retrieves the next 10 rows from the Employees table.\n\n\n\n\nA subquery is a query embedded within another query. It can be used in various parts of the main query, including the SELECT, FROM, and WHERE clauses. Subqueries are typically enclosed in parentheses and can return scalar values, single columns, single rows, or tables, depending on their placement and purpose in the main query.\nExample of a subquery in a WHERE clause:\nSELECT *  FROM Employees  WHERE DepartmentID IN (SELECT DepartmentID FROM Departments WHERE Name = ‘Research’);\nIn this example, the subquery selects DepartmentIDs from the Departments table where the department name is ‘Research’. The main query then uses these IDs to filter employees from those specific departments.\n\n\n\nA nested query refers to a broader category of queries that contain other queries, and it can encompass subqueries as one of its forms. The term “nested query” can also specifically refer to queries nested within FROM clauses, where a subquery is used to create a temporary table that the outer query can then join to or operate on.\nExample of a nested query in a FROM clause:\nSELECT AvgSalaries.Department, Employee.Name, Employee.Salary  FROM Employees AS Employee  JOIN (  SELECT DepartmentID, AVG(Salary) AS AverageSalary  FROM Employees  GROUP BY DepartmentID  ) AS AvgSalaries ON Employee.DepartmentID = AvgSalaries.DepartmentID  WHERE Employee.Salary &gt; AvgSalaries.AverageSalary; \nIn this example, the nested query calculates the average salary per department and creates a temporary table (AvgSalaries) with this information. The outer query then selects employees who earn more than the average salary in their respective departments.\n\n\n\n\nScope: Subqueries can be considered a specific type of nested query. While all subqueries are nested queries, not all nested queries are subqueries, especially when considering the broader use of nesting in SQL queries.\nUsage: Subqueries are often used for returning specific values that the main query can use for comparison or filtering, whereas nested queries, particularly those in the FROM clause, are used to create temporary tables for the main query to interact with.\nComplexity: Nested queries, especially those used as temporary tables, can be more complex and might involve aggregation, grouping, or additional joins within the nested part, whereas subqueries might be simpler, returning a single value or a set of values for the main query to use.\nIn summary, while the terms subquery and nested query are often used interchangeably, the distinction usually lies in the specific use case and complexity. Subqueries are a subset of nested queries, primarily used within WHERE, SELECT, and FROM clauses for filtering, selection, or temporary table creation.\n\n\n\n\nApplication Programming Interfaces (APIs) serve as the conduit through which different software systems interact, based on a set of defined protocols and rules. APIs facilitate the seamless exchange of data and functionality between disparate systems, making them integral to modern software development.\n\n\nEndpoint: The specific URL where the API can be accessed. It represents the specific function or resource you wish to interact with. Headers: Contain metadata for the API call, such as content type, response format, and authentication tokens (API keys). Body: Essential for POST or PUT requests, the body contains the data to be sent to the API. Example: Consider a weather forecasting application that uses a third-party API to fetch weather data. The application might make a GET request to https://api.weather.com/v2/forecast?location=NewYork with headers containing an API key. The response could include data like temperature, humidity, and forecasts, which the application then displays to the user.\n\n\n\nIntegration: APIs enable seamless integration of third-party services or data, enriching your application’s capabilities without the need for extensive development. Automation: Tasks can be automated by scripting interactions with APIs, saving time and reducing the potential for human error. Flexibility: APIs allow for the extension of an application’s functionality, enabling customization and scalability. The Pipeline from the API or Server A data pipeline is an orchestrated set of operations designed to move and transform data from its source to a destination, such as a database or data warehouse. It typically involves stages like data extraction, transformation, and loading (ETL).\n\n\n\n\nExtraction: Data is gathered from various sources, which could include databases, APIs, or flat files. Transformation: The raw data is cleaned, enriched, and transformed into a format suitable for analysis. This might involve filtering out irrelevant data, converting data types, or aggregating information. Loading: The transformed data is stored in a structured form in a target system for further analysis or reporting. Example: A retail company might extract sales data from its online and physical stores (via APIs or direct database access), transform the data to calculate total sales by region and category, and load the results into a data warehouse for analysis.\n\n\n\nEfficiency: Automating the flow of data reduces manual tasks and speeds up data processing.\nConsistency: Ensures that all data is processed in a uniform manner, improving data quality and reliability.\nScalability: Can handle increasing volumes of data by scaling resources up or down as needed.\n\n\n\n\n\nAfter processing and analysis, data often needs to be stored in a structured and accessible form. This could involve:\n\nUpdating an operational database to reflect new insights or changes.\nStoring aggregated or analyzed data in a data warehouse, making it available for complex queries and strategic decision-making.\nCreating data marts, which are subsets of data warehouses tailored to the specific needs of different departments or functions.\n\nExample: After analyzing sales data, a business might update its inventory database to reflect items that need restocking. It might also store detailed sales analysis in a data warehouse for use in strategic planning and create a marketing-specific data mart focusing on customer purchase patterns.\nAdvantages:\n\nAccessibility: Storing processed data makes it readily accessible for decision-making, reporting, and further analysis.\nPerformance: Separating operational databases from analytical stores (data warehouses) improves performance in both systems.\nSecurity: Data warehouses and marts can implement specific security measures appropriate for the sensitivity of the stored data.\n\n\n\n\n\n\nData Enrichment is the process of augmenting your existing dataset with additional data from external sources to provide more context and depth. For instance, adding socioeconomic data to customer profiles can offer more insights into customer behavior and preferences.\nAdvantages:\nEnhanced Insights: Provides a more rounded view of the data subjects, leading to more accurate analyses and predictions. Improved Decision Making: Deeper data context can lead to better-informed strategic decisions.\n\n\n\nFeature Engineering involves creating new features from raw data to improve the performance of machine learning models. Techniques might include aggregating data points, decomposing features (e.g., splitting dates into day and month), or transforming variables (e.g., log transformations).\nAdvantages:\nModel Accuracy: Properly engineered features can significantly improve model performance. Interpretability: Well-chosen features can make models more understandable by highlighting the underlying factors that drive predictions.\n\n\n\nWhile both processes aim to improve data quality, data enrichment broadens the dataset with new data, providing more dimensions for analysis. Feature engineering, on the other hand, works on the existing dataset, transforming it into a more model-friendly format. The choice between them depends on whether the need is for more data or for transforming existing data into a more useful form. Both are crucial in different stages of the data preparation process for machine learning.",
    "crumbs": [
      "Senior Project",
      "Data Products"
    ]
  },
  {
    "objectID": "Projects/project2.html#the-key-characteristics-of-a-data-product-include",
    "href": "Projects/project2.html#the-key-characteristics-of-a-data-product-include",
    "title": "Senior Project",
    "section": "",
    "text": "Data-Driven: At its core, a data product uses data to function. This data can be static or dynamic, and the product’s effectiveness often depends on the quality and relevance of this data.\nActionable Insights: Unlike raw data or basic analyses, a data product provides insights that are actionable. This means that users can make decisions or take actions based on the output of the data product.\nUser-Focused: Data products are designed with the end-user in mind, ensuring that they are accessible, understandable, and valuable to the intended audience. This often involves user interface design, data visualization, and user experience optimization.\nAutomated: Many data products automate processes that would otherwise require manual data analysis, saving time and reducing the potential for human error.\nScalable: As data grows or as the number of users increases, a well-designed data product can scale to accommodate these changes without losing performance or reliability.",
    "crumbs": [
      "Senior Project",
      "Data Products"
    ]
  },
  {
    "objectID": "Projects/project2.html#sql",
    "href": "Projects/project2.html#sql",
    "title": "Senior Project",
    "section": "",
    "text": "Diving into SQL (Structured Query Language) is like learning the grammar of data speak. It’s the go to language for communicate to databases, asking them about the information they hold. Whether you’re inquiring about customer behaviors, financial transactions, or anything in between, SQL helps you pose the questions and understand the responses. With SQL, you can sift through mountains of data to find the nuggets of insight that drive strategies and decisions, all of this is always done by a few well-crafted queries.",
    "crumbs": [
      "Senior Project",
      "Data Products"
    ]
  },
  {
    "objectID": "Projects/project2.html#a-little-visit-down-memory-lane-with-sql",
    "href": "Projects/project2.html#a-little-visit-down-memory-lane-with-sql",
    "title": "Senior Project",
    "section": "",
    "text": "The key parts of an SQL query provides a deeper understanding of how data is manipulated and retrieved from a database. Each component plays a crucial role in shaping the output of the query.\n\n\nThe SELECT statement specifies the columns of data you want to retrieve from one or more tables in a database. It’s the starting point of most queries and defines which fields should be included in the output. You can use SELECT * to retrieve all columns or specify individual column names separated by commas for a more tailored output.\nExample:\nSELECT FirstName, LastName  FROM Employees; \nThis query retrieves the FirstName and LastName columns from the Employees table.\n\n\n\nThe FROM clause specifies the table(s) from which to retrieve data. When data needs to be combined from multiple tables, the JOIN clause is used. There are several types of joins, including INNER JOIN, LEFT JOIN, RIGHT JOIN, and FULL JOIN, each serving different purposes in terms of how tables are merged based on matching data in specified columns.\nExample:\nSELECT Orders.OrderID, Customers.CustomerName  FROM Orders  INNER JOIN Customers ON Orders.CustomerID = Customers.CustomerID; \nThis query retrieves order IDs from the Orders table and the corresponding customer names by joining the Orders table with the Customers table on the CustomerID column.\n\n\n\nThe WHERE clause filters records to include only those that meet certain conditions. It helps in narrowing down the results based on specified criteria, making the query output more relevant.\nExample:\nSELECT *  FROM Employees  WHERE Department = ‘Marketing’;\nThis query retrieves all columns for employees who work in the Marketing department.\n\n\n\nThe HAVING clause is used to filter groups of rows after they’ve been grouped by a GROUP BY clause. It’s similar to the WHERE clause but is applicable to aggregated data. HAVING is commonly used with functions like SUM(), AVG(), COUNT(), etc.\nExample:\nSELECT Department, COUNT(EmployeeID) AS NumberOfEmployees  FROM Employees  GROUP BY Department  HAVING COUNT(EmployeeID) &gt; 10; \nThis query counts the number of employees in each department and only includes those departments with more than 10 employees.\n\n\n\nThe GROUP BY clause groups rows that have the same values in specified columns into summary rows, like “find the number of employees in each department”. It is often used with aggregate functions (COUNT(), MAX(), SUM(), AVG()).\nExample:\nSELECT Department, AVG(Salary) AS AverageSalary  FROM Employees  GROUP BY Department; \nThis query calculates the average salary within each department.\n\n\n\nThe ORDER BY clause is used to sort the result set of a query by one or more columns. It can sort the results in ascending (ASC) or descending (DESC) order. If no direction is specified, ASC is assumed by default.\nExample:\nSELECT FirstName, LastName, Salary  FROM Employees  ORDER BY Salary DESC; \nThis query retrieves employee names and salaries, sorted by salary in descending order.\n\n\n\nThe LIMIT clause is used to constrain the number of rows returned by a query. This is useful in large datasets to retrieve only a subset of rows. The OFFSET clause is used in conjunction with LIMIT to skip a specific number of rows before starting to return rows from the query.\nExample:\nSELECT *  FROM Employees  LIMIT 10 OFFSET 5;  This query skips the first 5 rows and then retrieves the next 10 rows from the Employees table.",
    "crumbs": [
      "Senior Project",
      "Data Products"
    ]
  },
  {
    "objectID": "Projects/project2.html#subqueries",
    "href": "Projects/project2.html#subqueries",
    "title": "Senior Project",
    "section": "",
    "text": "A subquery is a query embedded within another query. It can be used in various parts of the main query, including the SELECT, FROM, and WHERE clauses. Subqueries are typically enclosed in parentheses and can return scalar values, single columns, single rows, or tables, depending on their placement and purpose in the main query.\nExample of a subquery in a WHERE clause:\nSELECT *  FROM Employees  WHERE DepartmentID IN (SELECT DepartmentID FROM Departments WHERE Name = ‘Research’);\nIn this example, the subquery selects DepartmentIDs from the Departments table where the department name is ‘Research’. The main query then uses these IDs to filter employees from those specific departments.",
    "crumbs": [
      "Senior Project",
      "Data Products"
    ]
  },
  {
    "objectID": "Projects/project2.html#nested-queries",
    "href": "Projects/project2.html#nested-queries",
    "title": "Senior Project",
    "section": "",
    "text": "A nested query refers to a broader category of queries that contain other queries, and it can encompass subqueries as one of its forms. The term “nested query” can also specifically refer to queries nested within FROM clauses, where a subquery is used to create a temporary table that the outer query can then join to or operate on.\nExample of a nested query in a FROM clause:\nSELECT AvgSalaries.Department, Employee.Name, Employee.Salary  FROM Employees AS Employee  JOIN (  SELECT DepartmentID, AVG(Salary) AS AverageSalary  FROM Employees  GROUP BY DepartmentID  ) AS AvgSalaries ON Employee.DepartmentID = AvgSalaries.DepartmentID  WHERE Employee.Salary &gt; AvgSalaries.AverageSalary; \nIn this example, the nested query calculates the average salary per department and creates a temporary table (AvgSalaries) with this information. The outer query then selects employees who earn more than the average salary in their respective departments.",
    "crumbs": [
      "Senior Project",
      "Data Products"
    ]
  },
  {
    "objectID": "Projects/project2.html#comparison",
    "href": "Projects/project2.html#comparison",
    "title": "Senior Project",
    "section": "",
    "text": "Scope: Subqueries can be considered a specific type of nested query. While all subqueries are nested queries, not all nested queries are subqueries, especially when considering the broader use of nesting in SQL queries.\nUsage: Subqueries are often used for returning specific values that the main query can use for comparison or filtering, whereas nested queries, particularly those in the FROM clause, are used to create temporary tables for the main query to interact with.\nComplexity: Nested queries, especially those used as temporary tables, can be more complex and might involve aggregation, grouping, or additional joins within the nested part, whereas subqueries might be simpler, returning a single value or a set of values for the main query to use.\nIn summary, while the terms subquery and nested query are often used interchangeably, the distinction usually lies in the specific use case and complexity. Subqueries are a subset of nested queries, primarily used within WHERE, SELECT, and FROM clauses for filtering, selection, or temporary table creation.",
    "crumbs": [
      "Senior Project",
      "Data Products"
    ]
  },
  {
    "objectID": "Projects/project2.html#apis",
    "href": "Projects/project2.html#apis",
    "title": "Senior Project",
    "section": "",
    "text": "Application Programming Interfaces (APIs) serve as the conduit through which different software systems interact, based on a set of defined protocols and rules. APIs facilitate the seamless exchange of data and functionality between disparate systems, making them integral to modern software development.\n\n\nEndpoint: The specific URL where the API can be accessed. It represents the specific function or resource you wish to interact with. Headers: Contain metadata for the API call, such as content type, response format, and authentication tokens (API keys). Body: Essential for POST or PUT requests, the body contains the data to be sent to the API. Example: Consider a weather forecasting application that uses a third-party API to fetch weather data. The application might make a GET request to https://api.weather.com/v2/forecast?location=NewYork with headers containing an API key. The response could include data like temperature, humidity, and forecasts, which the application then displays to the user.\n\n\n\nIntegration: APIs enable seamless integration of third-party services or data, enriching your application’s capabilities without the need for extensive development. Automation: Tasks can be automated by scripting interactions with APIs, saving time and reducing the potential for human error. Flexibility: APIs allow for the extension of an application’s functionality, enabling customization and scalability. The Pipeline from the API or Server A data pipeline is an orchestrated set of operations designed to move and transform data from its source to a destination, such as a database or data warehouse. It typically involves stages like data extraction, transformation, and loading (ETL).",
    "crumbs": [
      "Senior Project",
      "Data Products"
    ]
  },
  {
    "objectID": "Projects/project2.html#stages-of-a-data-pipeline",
    "href": "Projects/project2.html#stages-of-a-data-pipeline",
    "title": "Senior Project",
    "section": "",
    "text": "Extraction: Data is gathered from various sources, which could include databases, APIs, or flat files. Transformation: The raw data is cleaned, enriched, and transformed into a format suitable for analysis. This might involve filtering out irrelevant data, converting data types, or aggregating information. Loading: The transformed data is stored in a structured form in a target system for further analysis or reporting. Example: A retail company might extract sales data from its online and physical stores (via APIs or direct database access), transform the data to calculate total sales by region and category, and load the results into a data warehouse for analysis.\n\n\n\nEfficiency: Automating the flow of data reduces manual tasks and speeds up data processing.\nConsistency: Ensures that all data is processed in a uniform manner, improving data quality and reliability.\nScalability: Can handle increasing volumes of data by scaling resources up or down as needed.",
    "crumbs": [
      "Senior Project",
      "Data Products"
    ]
  },
  {
    "objectID": "Projects/project2.html#pushing-back-data-to-a-server-data-warehouse-or-data-mart",
    "href": "Projects/project2.html#pushing-back-data-to-a-server-data-warehouse-or-data-mart",
    "title": "Senior Project",
    "section": "",
    "text": "After processing and analysis, data often needs to be stored in a structured and accessible form. This could involve:\n\nUpdating an operational database to reflect new insights or changes.\nStoring aggregated or analyzed data in a data warehouse, making it available for complex queries and strategic decision-making.\nCreating data marts, which are subsets of data warehouses tailored to the specific needs of different departments or functions.\n\nExample: After analyzing sales data, a business might update its inventory database to reflect items that need restocking. It might also store detailed sales analysis in a data warehouse for use in strategic planning and create a marketing-specific data mart focusing on customer purchase patterns.\nAdvantages:\n\nAccessibility: Storing processed data makes it readily accessible for decision-making, reporting, and further analysis.\nPerformance: Separating operational databases from analytical stores (data warehouses) improves performance in both systems.\nSecurity: Data warehouses and marts can implement specific security measures appropriate for the sensitivity of the stored data.",
    "crumbs": [
      "Senior Project",
      "Data Products"
    ]
  },
  {
    "objectID": "Projects/project2.html#important-parts-of-machine-learning",
    "href": "Projects/project2.html#important-parts-of-machine-learning",
    "title": "Senior Project",
    "section": "",
    "text": "Data Enrichment is the process of augmenting your existing dataset with additional data from external sources to provide more context and depth. For instance, adding socioeconomic data to customer profiles can offer more insights into customer behavior and preferences.\nAdvantages:\nEnhanced Insights: Provides a more rounded view of the data subjects, leading to more accurate analyses and predictions. Improved Decision Making: Deeper data context can lead to better-informed strategic decisions.\n\n\n\nFeature Engineering involves creating new features from raw data to improve the performance of machine learning models. Techniques might include aggregating data points, decomposing features (e.g., splitting dates into day and month), or transforming variables (e.g., log transformations).\nAdvantages:\nModel Accuracy: Properly engineered features can significantly improve model performance. Interpretability: Well-chosen features can make models more understandable by highlighting the underlying factors that drive predictions.\n\n\n\nWhile both processes aim to improve data quality, data enrichment broadens the dataset with new data, providing more dimensions for analysis. Feature engineering, on the other hand, works on the existing dataset, transforming it into a more model-friendly format. The choice between them depends on whether the need is for more data or for transforming existing data into a more useful form. Both are crucial in different stages of the data preparation process for machine learning.",
    "crumbs": [
      "Senior Project",
      "Data Products"
    ]
  },
  {
    "objectID": "ml.html",
    "href": "ml.html",
    "title": "about me",
    "section": "",
    "text": "MarkDown Basics"
  },
  {
    "objectID": "ml.html#title-2-header",
    "href": "ml.html#title-2-header",
    "title": "about me",
    "section": "",
    "text": "MarkDown Basics"
  },
  {
    "objectID": "full_stack.html",
    "href": "full_stack.html",
    "title": "about me",
    "section": "",
    "text": "MarkDown Basics"
  },
  {
    "objectID": "full_stack.html#title-2-header",
    "href": "full_stack.html#title-2-header",
    "title": "about me",
    "section": "",
    "text": "MarkDown Basics"
  },
  {
    "objectID": "competition.html",
    "href": "competition.html",
    "title": "about me",
    "section": "",
    "text": "MarkDown Basics"
  },
  {
    "objectID": "competition.html#title-2-header",
    "href": "competition.html#title-2-header",
    "title": "about me",
    "section": "",
    "text": "MarkDown Basics"
  },
  {
    "objectID": "cleansing.html",
    "href": "cleansing.html",
    "title": "about me",
    "section": "",
    "text": "MarkDown Basics"
  },
  {
    "objectID": "cleansing.html#title-2-header",
    "href": "cleansing.html#title-2-header",
    "title": "about me",
    "section": "",
    "text": "MarkDown Basics"
  },
  {
    "objectID": "exploration.html",
    "href": "exploration.html",
    "title": "about me",
    "section": "",
    "text": "MarkDown Basics"
  },
  {
    "objectID": "exploration.html#title-2-header",
    "href": "exploration.html#title-2-header",
    "title": "about me",
    "section": "",
    "text": "MarkDown Basics"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Welcom to my portfolio",
    "section": "",
    "text": "Hey there, I’m Daniel Dominguez, a Data Science major at BYU-Idaho University. As a kid growing up in Mexico City, I was always a keen observer of the world around me. I found that I was able to glean insights and patterns from the things that most people didn’t notice. This natural curiosity and analytical mind led me to the fascinating field of data science.\nTo me, data is like a story waiting to be told. Every set of data has its own narrative, and it’s my job to uncover the hidden stories and connections within it. As someone who is passionate about storytelling, I believe that data analysis is a powerful tool for understanding our world and making better decisions.\nI’m passionate about all things data-related, but what really drives me is the ability to predict scenarios based on past and present trends. It’s like being able to read the next chapter of a gripping story before anyone else! I’m constantly amazed by how the dots connect when we analyze data, and how insights gained from one area can inform and improve other areas.\nAs a problem-solver at heart, I relish the challenge of identifying issues and devising ways to tackle them. I’m always eager to put my skills to the test and find the best possible outcomes.\nSo, if you’re interested in exploring the world of data or just want to chat, feel free to reach out to me. Let’s see what we can create together!\n\ndanndch7@gmail.com | Data Science Program |  LinkedIn"
  },
  {
    "objectID": "index.html#this-still-under-construcion",
    "href": "index.html#this-still-under-construcion",
    "title": "Welcom to my portfolio",
    "section": "",
    "text": "Hey there, I’m Daniel Dominguez, a Data Science major at BYU-Idaho University. As a kid growing up in Mexico City, I was always a keen observer of the world around me. I found that I was able to glean insights and patterns from the things that most people didn’t notice. This natural curiosity and analytical mind led me to the fascinating field of data science.\nTo me, data is like a story waiting to be told. Every set of data has its own narrative, and it’s my job to uncover the hidden stories and connections within it. As someone who is passionate about storytelling, I believe that data analysis is a powerful tool for understanding our world and making better decisions.\nI’m passionate about all things data-related, but what really drives me is the ability to predict scenarios based on past and present trends. It’s like being able to read the next chapter of a gripping story before anyone else! I’m constantly amazed by how the dots connect when we analyze data, and how insights gained from one area can inform and improve other areas.\nAs a problem-solver at heart, I relish the challenge of identifying issues and devising ways to tackle them. I’m always eager to put my skills to the test and find the best possible outcomes.\nSo, if you’re interested in exploring the world of data or just want to chat, feel free to reach out to me. Let’s see what we can create together!\n\ndanndch7@gmail.com | Data Science Program |  LinkedIn"
  },
  {
    "objectID": "Projects/project1.html",
    "href": "Projects/project1.html",
    "title": "Senior Project Post 1",
    "section": "",
    "text": "In the rapidly evolving world of Data, where the ability to turn vast amounts of raw data into actionable insights is highly prized. You may know Python, you may know R, but mastering the language of data and understanding the architecture that supports it are not just an advantage but it is essential. SQL and data engineering, is a realm that it is close to Data Science and there are some skills that will set you apart in your future career.\n\n\nSQL, or Structured Query Language, is the cornerstone of data manipulation and retrieval. It’s the bridge between the questions you seek to answer and the data that contains those answers. Whether it’s understanding customer behavior, predicting market trends, or improving operational efficiency, SQL empowers you to:\n\nAccess Critical Data Quickly: Navigate through complex databases with ease, ensuring that no source of insight is beyond your reach.\nCommunicate Effectively with Data: Speak the universal language understood by databases worldwide, making your analyses more versatile and impactful.\nRefine Your Analytical Precision: Hone your ability to ask nuanced questions of your data, enabling more precise, targeted analyses.\n\n\n\n\nAs Data Scientists, our vision is only as clear as the data at our disposal. Data engineering skills could ensure that the data we rely on is accurate, accessible, and ready for analysis. By understanding data pipelines, storage solutions, and data processing, we will:\n\nEnhance Data Quality and Accessibility: Ensure that the data you use for your analyses is clean, well-structured, and readily available, reducing the time from question to insight.\nScale Your Data Efforts: Tackle larger datasets and more complex problems with confidence, knowing that your data infrastructure is robust and scalable.\nBridge the Gap Between Data and Decision-Making: Play that role in translating data insights into strategic actions, making your contributions invaluable to any team.\nYou don’t want to sit and wait: If you provide to your employer a solution more than waiting for the Data Engineer to give you the data you need to work, what if you could provide that solution as well, not having to depend a 100% on someone else gives you freedome and a higher value in the market.\n\n\n\n\nEmbracing SQL and and some other data engineering concepts is not just about enlarging your technical toolbox; it’s about expanding your problem-solving skillset. In a data-driven world, these skills enable you to navigate and make sense of the multiple situations, turning data into information which will become decisions that drive progress.",
    "crumbs": [
      "Senior Project",
      "Data Engineering for DS"
    ]
  },
  {
    "objectID": "Projects/project1.html#why-sql",
    "href": "Projects/project1.html#why-sql",
    "title": "Senior Project Post 1",
    "section": "",
    "text": "SQL, or Structured Query Language, is the cornerstone of data manipulation and retrieval. It’s the bridge between the questions you seek to answer and the data that contains those answers. Whether it’s understanding customer behavior, predicting market trends, or improving operational efficiency, SQL empowers you to:\n\nAccess Critical Data Quickly: Navigate through complex databases with ease, ensuring that no source of insight is beyond your reach.\nCommunicate Effectively with Data: Speak the universal language understood by databases worldwide, making your analyses more versatile and impactful.\nRefine Your Analytical Precision: Hone your ability to ask nuanced questions of your data, enabling more precise, targeted analyses.",
    "crumbs": [
      "Senior Project",
      "Data Engineering for DS"
    ]
  },
  {
    "objectID": "Projects/project1.html#the-value-of-data-engineering-skills",
    "href": "Projects/project1.html#the-value-of-data-engineering-skills",
    "title": "Senior Project Post 1",
    "section": "",
    "text": "As Data Scientists, our vision is only as clear as the data at our disposal. Data engineering skills could ensure that the data we rely on is accurate, accessible, and ready for analysis. By understanding data pipelines, storage solutions, and data processing, we will:\n\nEnhance Data Quality and Accessibility: Ensure that the data you use for your analyses is clean, well-structured, and readily available, reducing the time from question to insight.\nScale Your Data Efforts: Tackle larger datasets and more complex problems with confidence, knowing that your data infrastructure is robust and scalable.\nBridge the Gap Between Data and Decision-Making: Play that role in translating data insights into strategic actions, making your contributions invaluable to any team.\nYou don’t want to sit and wait: If you provide to your employer a solution more than waiting for the Data Engineer to give you the data you need to work, what if you could provide that solution as well, not having to depend a 100% on someone else gives you freedome and a higher value in the market.",
    "crumbs": [
      "Senior Project",
      "Data Engineering for DS"
    ]
  },
  {
    "objectID": "Projects/project1.html#your-path-to-becoming-a-data-science-hybrid-with-data-engineering",
    "href": "Projects/project1.html#your-path-to-becoming-a-data-science-hybrid-with-data-engineering",
    "title": "Senior Project Post 1",
    "section": "",
    "text": "Embracing SQL and and some other data engineering concepts is not just about enlarging your technical toolbox; it’s about expanding your problem-solving skillset. In a data-driven world, these skills enable you to navigate and make sense of the multiple situations, turning data into information which will become decisions that drive progress.",
    "crumbs": [
      "Senior Project",
      "Data Engineering for DS"
    ]
  },
  {
    "objectID": "Projects/project1.html#top-industry",
    "href": "Projects/project1.html#top-industry",
    "title": "Senior Project Post 1",
    "section": "Top industry",
    "text": "Top industry\nLets take a look at two positions open at TESLA for an internship, they are in the same job post but they ask for two different positions.\n \nIf we were to look into a Ven Diagram we could picture Data Science on one of the sides and Data Engineering in the other, this is it could be taken from out the descriptions and put it into the Data Science or Data Engineering side s ### Skills and Tasks Specific to a Data Scientist:\n\nStatistical Learning and Bayesian Models: The mention of statistical learning and Bayesian models is more aligned with the data scientist role, where a deep understanding of statistical methods and the ability to apply these methods to extract insights from data is crucial.\nAnswering Complex Questions on Fleet Usage and Behavior: This involves not just accessing and processing data but also interpreting it in a way that provides actionable insights, which is a core part of a data scientist’s role.\nMLOps Skills: While MLOps can be relevant to both roles, in the context of deploying and managing machine learning models, it leans more towards the data scientist side, especially when it involves model development, validation, and iteration based on statistical principles.\n\n\nSkills and Tasks Specific to a Data Engineer:\n\nBuilding Scalable Data Pipelines: The emphasis on building scalable data pipelines for deploying fleet health monitoring models is a essential data engineering task, focusing on the infrastructure that allows for efficient data flow and processing.\nData ELT Pipelines Using Airflow: Creating and maintaining data Extract, Load, Transform (ELT) pipelines, especially with tools like Airflow, is a key data engineering responsibility, ensuring that data is regularly and reliably prepared for analysis.\nQuery Optimization and Advanced SQL: While SQL is used by both data scientists and data engineers, the focus on optimizing and undertaking advanced SQL queries, especially on massive datasets, is more characteristic of data engineering, which often deals with the optimization of data access and processing.\n\n\n\nSkills and Tasks Shared Between Both Roles:\n\nData Analytics and Infrastructure: Both roles involve a level of responsibility for employing data to drive insights and actions, which includes the infrastructure that supports data analytics.\nUse of Tableau and Similar Tools for Dashboards: The creation and maintenance of analytics dashboards are common to both roles, although the focus might differ; data scientists would be more involved in the interpretation of the data, while data engineers would ensure the data is accurately and efficiently presented.\nWorking with Cross-Functional Teams: Collaboration with cross-functional teams to support design cycles, provide insights, and support decision-making is essential for both roles, though the nature of the support and insights provided may vary.\n\n\n\nThe difference\nThe job description establishes a range for the payment, it goes from $20-60 an hour. Just imagine that for being able to understand skills like ETL, stablishing a pipeline (Which is ETL), and sharp your SQL could give you 3 times more money, and you could fix your problems in less time than waiting for someone to complement this tasks, if you are hired of course.",
    "crumbs": [
      "Senior Project",
      "Data Engineering for DS"
    ]
  },
  {
    "objectID": "Projects/project1.html#difference-on-the-data-structures",
    "href": "Projects/project1.html#difference-on-the-data-structures",
    "title": "Senior Project Post 1",
    "section": "Difference on the Data Structures",
    "text": "Difference on the Data Structures\n\nData Lake\n\nAttributes:\nA Data Lake is a vast pool of raw, unstructured, and structured data stored in its native format. It is designed to store a large amount of data without a predefined schema, allowing you to keep all your data in one place without having to structure it first.\n\n\nBenefits:\n\nFlexibility: You can store data of all types and structures, making it highly adaptable to changes.\nScalability: Capable of handling massive volumes of data, from gigabytes to petabytes.\nCost-Effectiveness: Often built on inexpensive storage solutions, making it economical for storing vast amounts of data.\n\n\n\nUses:\n\nIdeal for big data and real-time analytics projects.\nUseful for organizations that want to store all their data without initially knowing how they will use it.\n\n\n\n\nData Warehouse\n\nAttributes:\nA Data Warehouse is a centralized repository for structured, filtered data that has already been processed for a specific purpose. It is highly structured and uses a schema-on-write approach, meaning the data is organized and formatted at the time of entry.\n\n\nBenefits:\n\nImproved Data Quality and Consistency: Due to its structured nature, data is cleaned and transformed, ensuring reliability.\nPerformance: Optimized for complex queries and data analysis, providing fast access to insights.\nHistorical Intelligence: Ideal for storing historical data, enabling trend analysis over time.\n\n\n\nUses:\n\nSuitable for business intelligence, reporting, and data analysis purposes.\nUsed by organizations that need to analyze their data comprehensively to make informed decisions.\n\n\n\n\nData Mart\n\nAttributes:\nA Data Mart is a subset of a data warehouse designed to focus on a specific line of business, department, or subject area. It is more tailored and subject-oriented, containing only the data relevant to a particular group or purpose.\n\n\nBenefits:\n\nIncreased Accessibility: More accessible to users due to its focus on a specific domain, making it easier to retrieve relevant information.\nFaster Query Performance: Smaller size leads to quicker data retrieval, improving efficiency.\nUser-Friendly: Easier for non-technical users to interact with and understand since it’s focused on a specific area.\n\n\n\nUses:\n\nIdeal for department-specific reports and analyses, such as sales, finance, or marketing data.\nUsed by departments within organizations that need quick, easy access to specific, relevant data.\n\n\n\n\nData Fabric\n\nAttributes:\n\nData Fabric provides a unified layer of data and connectivity across different platforms and environments, using various technologies and approaches (like metadata).\nIt is designed to provide consistent capabilities across endpoints in a hybrid and multi-cloud environment.\n\n\n\nBenefits:\n\nAgility: Enables quick access to data across the organization, regardless of its location.\nInteroperability: Facilitates seamless data sharing and integration across diverse systems and platforms.\n\n\n\nUses:\n\nIdeal for organizations with complex data ecosystems looking to streamline data access, integration, and management across multiple environments.",
    "crumbs": [
      "Senior Project",
      "Data Engineering for DS"
    ]
  },
  {
    "objectID": "Projects/project1.html#etl-extract-transform-load",
    "href": "Projects/project1.html#etl-extract-transform-load",
    "title": "Senior Project Post 1",
    "section": "ETL (Extract, Transform, Load)",
    "text": "ETL (Extract, Transform, Load)\n\nExtract: Data is sourced from various origins, such as databases, CRM systems, flat files, or APIs. This stage involves querying data sources, often using SQL for structured databases, and aggregating the data for further processing.\nTransform: This critical phase involves a series of operations such as cleansing (removing or correcting data errors), deduplication (eliminating duplicate records), normalization (structuring data to reduce redundancy), and aggregation (summarizing detailed data). Complex transformations might include pivoting data formats, merging datasets, or applying business logic to derive new calculated fields. This step is usually performed in a dedicated staging area or a transformation engine, using tools and languages suitable for data manipulation, including SQL, Python, or specialized ETL tools.\nLoad: The final step involves writing the transformed data into a target data warehouse or database. This phase is carefully managed to maintain data integrity and optimize for query performance, often involving SQL operations to insert data into structured schemas designed for analysis.",
    "crumbs": [
      "Senior Project",
      "Data Engineering for DS"
    ]
  },
  {
    "objectID": "Projects/project1.html#advantages-and-challenges",
    "href": "Projects/project1.html#advantages-and-challenges",
    "title": "Senior Project Post 1",
    "section": "Advantages and Challenges:",
    "text": "Advantages and Challenges:\n\nControl and Quality: The ETL process allows for extensive control over the transformation logic, ensuring data quality and consistency. This is particularly important in regulated industries or where data accuracy is critical.\nPerformance Consideration: ETL can relieve the target data warehouse from the processing load of transformations. However, the transformation phase can become a bottleneck if not well-optimized, especially with large volumes of data.\nTooling and Complexity: While ETL offers robustness and control, it can also introduce complexity, requiring specialized tools and expertise to manage the transformation logic and workflows.",
    "crumbs": [
      "Senior Project",
      "Data Engineering for DS"
    ]
  },
  {
    "objectID": "Projects/project1.html#when-to-use-etl",
    "href": "Projects/project1.html#when-to-use-etl",
    "title": "Senior Project Post 1",
    "section": "When to Use ETL:",
    "text": "When to Use ETL:\n\nWhen data quality and transformation logic are complex and need to be managed meticulously.\nIn environments where the computational capabilities of the target data warehouse are limited or costly.\nWhen data processing needs to be done before loading to ensure compliance, security, or data privacy.",
    "crumbs": [
      "Senior Project",
      "Data Engineering for DS"
    ]
  },
  {
    "objectID": "Projects/project1.html#advantages-and-challenges-1",
    "href": "Projects/project1.html#advantages-and-challenges-1",
    "title": "Senior Project Post 1",
    "section": "Advantages and Challenges:",
    "text": "Advantages and Challenges:\n\nScalability and Flexibility: ELT leverages the scalable compute resources of modern cloud-based data warehouses, handling vast amounts of data efficiently. The flexibility to transform data on-the-fly supports agile analytics and data exploration.\nSpeed: By loading data directly into the warehouse, ELT can make data available for analysis more quickly. This is advantageous in fast-paced environments where timely insights are critical.\nSimplification and Cost: ELT can simplify the data pipeline by reducing the need for a separate transformation layer, potentially lowering infrastructure and maintenance costs. However, this approach requires a robust data warehouse capable of handling intensive compute tasks.",
    "crumbs": [
      "Senior Project",
      "Data Engineering for DS"
    ]
  },
  {
    "objectID": "Projects/project1.html#when-to-use-elt",
    "href": "Projects/project1.html#when-to-use-elt",
    "title": "Senior Project Post 1",
    "section": "When to Use ELT:",
    "text": "When to Use ELT:\n\nIn big data scenarios where the volume, velocity, and variety of data exceed traditional database capabilities.\nWhen using cloud-based data warehouses with high processing power, where the cost of compute resources is outweighed by the need for flexibility and speed.\nIn agile environments where the requirements for data models and analyses change frequently, necessitating a flexible approach to data transformation.",
    "crumbs": [
      "Senior Project",
      "Data Engineering for DS"
    ]
  },
  {
    "objectID": "Projects/project1.html#the-ongoing-debate",
    "href": "Projects/project1.html#the-ongoing-debate",
    "title": "Senior Project Post 1",
    "section": "The Ongoing Debate",
    "text": "The Ongoing Debate\nThe debate between ETL and ELT often centers on trade-offs between control and flexibility, scalability and complexity, and the upfront costs versus long-term benefits. The right choice depends on specific project requirements, data characteristics, available infrastructure, and strategic priorities.\nETL is traditionally preferred for scenarios where data integrity, privacy, and compliance are paramount, and where transformations need to be tightly controlled and monitored. It’s ideal for environments where the cleanliness and structure of the data are critical before it can be stored or analyzed, such as in financial services or healthcare sectors where data accuracy and privacy are non-negotiable.\nELT, on the other hand, is favored in contexts where the agility and flexibility of data analysis are prioritized. The ability to store raw data and transform it as needed allows for a more explorative and iterative approach to data analytics. This is particularly beneficial in dynamic industries like e-commerce or social media, where business needs can change rapidly, and the ability to pivot and explore new data models quickly is a competitive advantage.",
    "crumbs": [
      "Senior Project",
      "Data Engineering for DS"
    ]
  },
  {
    "objectID": "Projects/project1.html#practical-examples",
    "href": "Projects/project1.html#practical-examples",
    "title": "Senior Project Post 1",
    "section": "Practical Examples",
    "text": "Practical Examples\nETL Example: A financial institution might use ETL to integrate customer transaction data from various sources. The transformation stage would include validating transaction codes, converting currencies, and anonymizing personal information to comply with data protection regulations before loading the clean, transformed data into a data warehouse for analysis and reporting.\nELT Example: A digital marketing firm might use ELT to analyze web traffic data. Raw clickstream data is ingested directly into a cloud-based data warehouse, and SQL queries are used to transform this data on-demand, creating different views for analyzing user behavior, campaign performance, and website engagement.",
    "crumbs": [
      "Senior Project",
      "Data Engineering for DS"
    ]
  },
  {
    "objectID": "Projects/project3.html",
    "href": "Projects/project3.html",
    "title": "Final Post",
    "section": "",
    "text": "The rapid growth of Artificial Intelligence (AI) and Machine Learning (ML) in this digital era is pushing many industries to start using these technologies to stay competitive and leverage the significant benefits they offer for making informed decisions. But how do we keep up when there’s confusion about the roles and responsibilities within data-related jobs? Through my experience of applying to over 100 jobs, including positions at Fortune 200 companies, and interacting with employers and recruiters at career fairs, I’ve observed widespread misconceptions about the data industry and its roles, and what data can actually achieve for businesses. This situation reminds me of learning the differences between “Their,” “There,” and “They’re.” A common issue is the interchangeability of titles such as Data Analyst, Data Scientist, and Data Engineer, along with the misconception that Data Scientists work only in IT. These misunderstandings made me wonder: if even mid-to-large-sized companies confuse these terms, how can smaller companies and startups take advantage and structure their data processes for success?\nOne notable conversation with an employer at a small company highlighted a common challenge: they had an Analyst position that, in practice, was not focused on analyzing data in the way one might expect. When asked if they had a dedicated full-time data analyst to create reports, reveal trends, and provide insights for decision-makers, the answer was no. Yet, the idea of having someone to perform in-depth analysis and help understand trends was seen as highly beneficial. This interaction underscores the widespread need for clearer roles and a more structured approach to data analysis within organizations, regardless of size.\nThe concept of Data Maturity is designed to address this gap by providing a comprehensive understanding of the necessary actions and tools organizations need to effectively employ data and generate insights. It guides companies in identifying their specific needs, where they need to invest more effort, and how to establish a reliable process for data analysis. The aim is to lay a solid foundation in the data processes of individuals and organizations, ensuring they have the right information when they need it. By structuring new information with a clear end goal—answering questions and making informed decisions—Data Maturity empowers businesses to leverage their data more effectively and achieve greater success.\nThis approach is particularly crucial in an era dominated by AI and ML, where the ability to navigate the complex data landscape can set companies apart. Data Maturity acts as a map, offering key landmarks and guidelines rather than a rigid, step-by-step recipe. This framework aims to transform the data process, making it more efficient and aligned with organizational goals, thereby facilitating a deeper understanding and better use of data for strategic decision-making.\n\n\n\n\n\n\n\nThe intention of the following text is to help understand different data roles, but as well to help individuals and companies to start applying the data process and make use of tools like Data Science, Data Analysis, Data Engineering, Machine Learning, AI.\nOur purpose as being involved in the field is to facilitate Data Products from the beginning till the end.\n\n\n\nThinking about making well-informed decisions for your business using reliable and meaningful information? Data Maturity is here to guide you through the process, from starting out to achieving your goals. If you’re considering diving into the world of AI and Machine Learning because you’ve heard about the incredible advantages they can bring, you’re on the right track. While we can’t promise every decision will be perfect, AI and ML can significantly improve your chances of success.\nIt all starts with a question – the “why” or “how” behind a problem you’re looking to solve. In the world of data, every question is an opportunity to embark on a new journey of discovery. This is particularly exciting for those who are keen to uncover insights from data.\n\n\nImagine we’re stepping into a forest, looking to handpick the perfect trees for crafting furniture. This is similar to the exploratory phase, where we scope out the landscape to find the data we need. Whether we’re generating new data or sourcing it from elsewhere, this phase is about gathering the raw materials for our project.\n\n\n\nThink of extracting data like selecting and cutting down the right trees from our forest. We’re identifying and collecting the data we believe will be most useful for our end goal. Extracting involves pulling together all this data, whether it’s from documents, files, databases, or other sources, to make it accessible and ready for use.\nTransforming data is akin to milling those trees into usable lumber, shaping it to fit our project’s needs. This crucial step involves cleaning and organizing the data, using tools and programming languages like R, Python, or SQL, to ensure it’s in a usable form.\nLoading the data then involves putting it to use, much like utilizing our prepared lumber to start building. This can lead to two paths: one where data is immediately used to inform decisions through dashboards or reports, and another where it’s stored in a Data Warehouse for future use. Data stored in a warehouse can be organized in a Data Mart, making it easily accessible for specific needs.\n\n\n\nNow that our data (or wood, in our analogy) is prepared and ready, we need to decide what to build with it. In data terms, this means analyzing the information to uncover trends, insights, and potential risks. This stage can already inform some decisions, but there’s more depth to explore for greater confidence.\n\n\n\nWith our data cleaned and relevant, it’s time to dig deeper and find out what’s truly significant. This is where Machine Learning comes in, helping to assess the importance of our findings, much like making sure we’re studying the right material for an exam.\n\n\n\nWouldn’t it be great to know your decisions will turn out well? While we can’t guarantee 100% success, Machine Learning can get us close by predicting outcomes. This allows us to prepare for various scenarios, increasing the likelihood of success.\n\n\n\nThe real test is in the significance of our predictions. This might involve some trial and error, but it’s all about understanding if our predictions hold up in the real world. If they do, our confidence in making decisions grows.\n\n\n\nOnce we have significant results, we can start training AI. AI is all about teaching computers to perform tasks like analyzing data or making recommendations. It’s a big step but can bring incredible benefits to your business.\nIn the end, Data Maturity is about equipping you with the knowledge and tools to make impactful decisions. Whether it’s figuring out the best step forward, where to invest, or how to optimize your processes, AI and Machine Learning are here to help. As a Data Scientist, my goal is to make these powerful tools accessible to you and your business.\n\n\n\n\nI presented this iniciative and teaching material at the Research and Creative Works where I was able to present it to judges and people that were interested.\nThe most commmon question I got from people is What is Data Science and what do you do? It was very nice to have the opportunity to tell some of the people what do I and what solutions can Data Science bring to their business and how can it be implemented in different industries and fields.\nFrom the judges I got 17 points out of 18 with the following feedback:\nThe scoring is broken down as the following: 3 (Outstanding) 2 (Meets Expectations) 1 (Sufficient, needs some improvement)\nMastery of concepts, skills, field: 3,3\nIndependence of thought and work: 3,3\nProfessional presence of presenter: 3,3\nEffectiveness of poster/visuals: 2,3\nUnderstood questions and responded effectively: 3,3\nProgress toward objective: 3,3\nOverall impression/COMMENTS:\n\nGreat job! Graphic was very informative. Would be nice to organize/simplify all the text and maybe add pictures to make it easier to read. Very well presented by Daniel!\nI love your poster. It will be very helpful for students trying to understand their options. The handout was a great idea; I’m going to recommend that everyone do a handout! Well done!",
    "crumbs": [
      "Senior Project",
      "Data Maturity"
    ]
  },
  {
    "objectID": "Projects/project3.html#the-abstract",
    "href": "Projects/project3.html#the-abstract",
    "title": "Final Post",
    "section": "",
    "text": "The rapid growth of Artificial Intelligence (AI) and Machine Learning (ML) in this digital era is pushing many industries to start using these technologies to stay competitive and leverage the significant benefits they offer for making informed decisions. But how do we keep up when there’s confusion about the roles and responsibilities within data-related jobs? Through my experience of applying to over 100 jobs, including positions at Fortune 200 companies, and interacting with employers and recruiters at career fairs, I’ve observed widespread misconceptions about the data industry and its roles, and what data can actually achieve for businesses. This situation reminds me of learning the differences between “Their,” “There,” and “They’re.” A common issue is the interchangeability of titles such as Data Analyst, Data Scientist, and Data Engineer, along with the misconception that Data Scientists work only in IT. These misunderstandings made me wonder: if even mid-to-large-sized companies confuse these terms, how can smaller companies and startups take advantage and structure their data processes for success?\nOne notable conversation with an employer at a small company highlighted a common challenge: they had an Analyst position that, in practice, was not focused on analyzing data in the way one might expect. When asked if they had a dedicated full-time data analyst to create reports, reveal trends, and provide insights for decision-makers, the answer was no. Yet, the idea of having someone to perform in-depth analysis and help understand trends was seen as highly beneficial. This interaction underscores the widespread need for clearer roles and a more structured approach to data analysis within organizations, regardless of size.\nThe concept of Data Maturity is designed to address this gap by providing a comprehensive understanding of the necessary actions and tools organizations need to effectively employ data and generate insights. It guides companies in identifying their specific needs, where they need to invest more effort, and how to establish a reliable process for data analysis. The aim is to lay a solid foundation in the data processes of individuals and organizations, ensuring they have the right information when they need it. By structuring new information with a clear end goal—answering questions and making informed decisions—Data Maturity empowers businesses to leverage their data more effectively and achieve greater success.\nThis approach is particularly crucial in an era dominated by AI and ML, where the ability to navigate the complex data landscape can set companies apart. Data Maturity acts as a map, offering key landmarks and guidelines rather than a rigid, step-by-step recipe. This framework aims to transform the data process, making it more efficient and aligned with organizational goals, thereby facilitating a deeper understanding and better use of data for strategic decision-making.",
    "crumbs": [
      "Senior Project",
      "Data Maturity"
    ]
  },
  {
    "objectID": "Projects/project3.html#an-initiative-for-any-business-and-individuals.",
    "href": "Projects/project3.html#an-initiative-for-any-business-and-individuals.",
    "title": "Final Post",
    "section": "",
    "text": "The intention of the following text is to help understand different data roles, but as well to help individuals and companies to start applying the data process and make use of tools like Data Science, Data Analysis, Data Engineering, Machine Learning, AI.\nOur purpose as being involved in the field is to facilitate Data Products from the beginning till the end.",
    "crumbs": [
      "Senior Project",
      "Data Maturity"
    ]
  },
  {
    "objectID": "Projects/project3.html#the-pitch",
    "href": "Projects/project3.html#the-pitch",
    "title": "Final Post",
    "section": "",
    "text": "Thinking about making well-informed decisions for your business using reliable and meaningful information? Data Maturity is here to guide you through the process, from starting out to achieving your goals. If you’re considering diving into the world of AI and Machine Learning because you’ve heard about the incredible advantages they can bring, you’re on the right track. While we can’t promise every decision will be perfect, AI and ML can significantly improve your chances of success.\nIt all starts with a question – the “why” or “how” behind a problem you’re looking to solve. In the world of data, every question is an opportunity to embark on a new journey of discovery. This is particularly exciting for those who are keen to uncover insights from data.\n\n\nImagine we’re stepping into a forest, looking to handpick the perfect trees for crafting furniture. This is similar to the exploratory phase, where we scope out the landscape to find the data we need. Whether we’re generating new data or sourcing it from elsewhere, this phase is about gathering the raw materials for our project.\n\n\n\nThink of extracting data like selecting and cutting down the right trees from our forest. We’re identifying and collecting the data we believe will be most useful for our end goal. Extracting involves pulling together all this data, whether it’s from documents, files, databases, or other sources, to make it accessible and ready for use.\nTransforming data is akin to milling those trees into usable lumber, shaping it to fit our project’s needs. This crucial step involves cleaning and organizing the data, using tools and programming languages like R, Python, or SQL, to ensure it’s in a usable form.\nLoading the data then involves putting it to use, much like utilizing our prepared lumber to start building. This can lead to two paths: one where data is immediately used to inform decisions through dashboards or reports, and another where it’s stored in a Data Warehouse for future use. Data stored in a warehouse can be organized in a Data Mart, making it easily accessible for specific needs.\n\n\n\nNow that our data (or wood, in our analogy) is prepared and ready, we need to decide what to build with it. In data terms, this means analyzing the information to uncover trends, insights, and potential risks. This stage can already inform some decisions, but there’s more depth to explore for greater confidence.\n\n\n\nWith our data cleaned and relevant, it’s time to dig deeper and find out what’s truly significant. This is where Machine Learning comes in, helping to assess the importance of our findings, much like making sure we’re studying the right material for an exam.\n\n\n\nWouldn’t it be great to know your decisions will turn out well? While we can’t guarantee 100% success, Machine Learning can get us close by predicting outcomes. This allows us to prepare for various scenarios, increasing the likelihood of success.\n\n\n\nThe real test is in the significance of our predictions. This might involve some trial and error, but it’s all about understanding if our predictions hold up in the real world. If they do, our confidence in making decisions grows.\n\n\n\nOnce we have significant results, we can start training AI. AI is all about teaching computers to perform tasks like analyzing data or making recommendations. It’s a big step but can bring incredible benefits to your business.\nIn the end, Data Maturity is about equipping you with the knowledge and tools to make impactful decisions. Whether it’s figuring out the best step forward, where to invest, or how to optimize your processes, AI and Machine Learning are here to help. As a Data Scientist, my goal is to make these powerful tools accessible to you and your business.",
    "crumbs": [
      "Senior Project",
      "Data Maturity"
    ]
  },
  {
    "objectID": "Projects/project3.html#presentation-and-creative-works-conference",
    "href": "Projects/project3.html#presentation-and-creative-works-conference",
    "title": "Final Post",
    "section": "",
    "text": "I presented this iniciative and teaching material at the Research and Creative Works where I was able to present it to judges and people that were interested.\nThe most commmon question I got from people is What is Data Science and what do you do? It was very nice to have the opportunity to tell some of the people what do I and what solutions can Data Science bring to their business and how can it be implemented in different industries and fields.\nFrom the judges I got 17 points out of 18 with the following feedback:\nThe scoring is broken down as the following: 3 (Outstanding) 2 (Meets Expectations) 1 (Sufficient, needs some improvement)\nMastery of concepts, skills, field: 3,3\nIndependence of thought and work: 3,3\nProfessional presence of presenter: 3,3\nEffectiveness of poster/visuals: 2,3\nUnderstood questions and responded effectively: 3,3\nProgress toward objective: 3,3\nOverall impression/COMMENTS:\n\nGreat job! Graphic was very informative. Would be nice to organize/simplify all the text and maybe add pictures to make it easier to read. Very well presented by Daniel!\nI love your poster. It will be very helpful for students trying to understand their options. The handout was a great idea; I’m going to recommend that everyone do a handout! Well done!",
    "crumbs": [
      "Senior Project",
      "Data Maturity"
    ]
  },
  {
    "objectID": "Projects/project5.html",
    "href": "Projects/project5.html",
    "title": "Client Report - [Insert Project Title]",
    "section": "",
    "text": "paste your elevator pitch here A SHORT (4-5 SENTENCES) PARAGRAPH THAT DESCRIBES KEY INSIGHTS TAKEN FROM METRICS IN THE PROJECT RESULTS THINK TOP OR MOST IMPORTANT RESULTS.\n\n\nRead and format project data\n# Include and execute your code here\ndf = pd.read_csv(\"https://github.com/byuidatascience/data4names/raw/master/data-raw/names_year/names_year.csv\")\n\n\nHighlight the Questions and Tasks"
  },
  {
    "objectID": "Projects/project5.html#elevator-pitch",
    "href": "Projects/project5.html#elevator-pitch",
    "title": "Client Report - [Insert Project Title]",
    "section": "",
    "text": "paste your elevator pitch here A SHORT (4-5 SENTENCES) PARAGRAPH THAT DESCRIBES KEY INSIGHTS TAKEN FROM METRICS IN THE PROJECT RESULTS THINK TOP OR MOST IMPORTANT RESULTS.\n\n\nRead and format project data\n# Include and execute your code here\ndf = pd.read_csv(\"https://github.com/byuidatascience/data4names/raw/master/data-raw/names_year/names_year.csv\")\n\n\nHighlight the Questions and Tasks"
  },
  {
    "objectID": "Projects/project5.html#questiontask-1",
    "href": "Projects/project5.html#questiontask-1",
    "title": "Client Report - [Insert Project Title]",
    "section": "QUESTION|TASK 1",
    "text": "QUESTION|TASK 1\nCOPY PASTE QUESTION|TASK 1 FROM THE PROJECT HERE\ntype your results and analysis here\n\n\nRead and format data\n# Include and execute your code here\n\n\ninclude figures in chunks and discuss your findings in the figure.\n::: {#cell-Q1 chart .cell execution_count=4}\n\nplot example\n# Include and execute your code here\nchart = px.bar(df.head(200),\n    x=\"name\", \n    y=\"AK\"\n)\nchart.show()\n\n\n                                                \nMy useless chart\n\n:::\n::: {#cell-Q1 table .cell .tbl-cap-location-top tbl-cap=‘Not much of a table’ execution_count=5}\n\ntable example\n# Include and execute your code here\nmydat = df.head(1000)\\\n    .groupby('year')\\\n    .sum()\\\n    .reset_index()\\\n    .tail(10)\\\n    .filter([\"year\", \"AK\",\"AR\"])\n\ndisplay(mydat)\n\n\n\n\n\n\n\n\n\nyear\nAK\nAR\n\n\n\n\n96\n2006\n21.0\n183.0\n\n\n97\n2007\n28.0\n153.0\n\n\n98\n2008\n36.0\n212.0\n\n\n99\n2009\n34.0\n179.0\n\n\n100\n2010\n22.0\n196.0\n\n\n101\n2011\n41.0\n148.0\n\n\n102\n2012\n28.0\n140.0\n\n\n103\n2013\n26.0\n134.0\n\n\n104\n2014\n20.0\n114.0\n\n\n105\n2015\n28.0\n121.0\n\n\n\n\n\n\n:::"
  },
  {
    "objectID": "Projects/project5.html#questiontask-2",
    "href": "Projects/project5.html#questiontask-2",
    "title": "Client Report - [Insert Project Title]",
    "section": "QUESTION|TASK 2",
    "text": "QUESTION|TASK 2\nCOPY PASTE QUESTION|TASK 2 FROM THE PROJECT HERE\ntype your results and analysis here\n\n\nRead and format data\n# Include and execute your code here\n\n\ninclude figures in chunks and discuss your findings in the figure.\n::: {#cell-Q2 chart .cell execution_count=7}\n\nplot example\n# Include and execute your code here\nchart = px.bar(df.head(200),\n    x=\"name\", \n    y=\"AK\"\n)\nchart.show()\n\n\n                                                \nMy useless chart\n\n:::\n::: {#cell-Q2 table .cell .tbl-cap-location-top tbl-cap=‘Not much of a table’ execution_count=8}\n\ntable example\n# Include and execute your code here\nmydat = df.head(1000)\\\n    .groupby('year')\\\n    .sum()\\\n    .reset_index()\\\n    .tail(10)\\\n    .filter([\"year\", \"AK\",\"AR\"])\n\ndisplay(mydat)\n\n\n\n\n\n\n\n\n\nyear\nAK\nAR\n\n\n\n\n96\n2006\n21.0\n183.0\n\n\n97\n2007\n28.0\n153.0\n\n\n98\n2008\n36.0\n212.0\n\n\n99\n2009\n34.0\n179.0\n\n\n100\n2010\n22.0\n196.0\n\n\n101\n2011\n41.0\n148.0\n\n\n102\n2012\n28.0\n140.0\n\n\n103\n2013\n26.0\n134.0\n\n\n104\n2014\n20.0\n114.0\n\n\n105\n2015\n28.0\n121.0\n\n\n\n\n\n\n:::"
  },
  {
    "objectID": "Projects/project5.html#questiontask-3",
    "href": "Projects/project5.html#questiontask-3",
    "title": "Client Report - [Insert Project Title]",
    "section": "QUESTION|TASK 3",
    "text": "QUESTION|TASK 3\nCOPY PASTE QUESTION|TASK 3 FROM THE PROJECT HERE\ntype your results and analysis here\n\n\nRead and format data\n# Include and execute your code here\n\n\ninclude figures in chunks and discuss your findings in the figure.\n::: {#cell-Q3 chart .cell execution_count=10}\n\nplot example\n# Include and execute your code here\nchart = px.bar(df.head(200),\n    x=\"name\", \n    y=\"AK\"\n)\nchart.show()\n\n\n                                                \nMy useless chart\n\n:::\n::: {#cell-Q3 table .cell .tbl-cap-location-top tbl-cap=‘Not much of a table’ execution_count=11}\n\ntable example\n# Include and execute your code here\nmydat = df.head(1000)\\\n    .groupby('year')\\\n    .sum()\\\n    .reset_index()\\\n    .tail(10)\\\n    .filter([\"year\", \"AK\",\"AR\"])\n\ndisplay(mydat)\n\n\n\n\n\n\n\n\n\nyear\nAK\nAR\n\n\n\n\n96\n2006\n21.0\n183.0\n\n\n97\n2007\n28.0\n153.0\n\n\n98\n2008\n36.0\n212.0\n\n\n99\n2009\n34.0\n179.0\n\n\n100\n2010\n22.0\n196.0\n\n\n101\n2011\n41.0\n148.0\n\n\n102\n2012\n28.0\n140.0\n\n\n103\n2013\n26.0\n134.0\n\n\n104\n2014\n20.0\n114.0\n\n\n105\n2015\n28.0\n121.0\n\n\n\n\n\n\n:::"
  },
  {
    "objectID": "resume.html",
    "href": "resume.html",
    "title": "Daniel Dominguez",
    "section": "",
    "text": "385.328.1955 | danndch@gmail.com |  LinkedIn | Data Science Program"
  },
  {
    "objectID": "resume.html#education.",
    "href": "resume.html#education.",
    "title": "Daniel Dominguez",
    "section": "Education.",
    "text": "Education.\n\nB.Sc. Data Science\nBrigham Young University Idaho (Rexburg, ID)\nJan 2020 - April 2024\n\nMinor: Statistics, Business Analytics\n\nCertificates: Machine Learning, Database, Applied Programming."
  },
  {
    "objectID": "resume.html#skills",
    "href": "resume.html#skills",
    "title": "Daniel Dominguez",
    "section": "Skills",
    "text": "Skills\n\nSkills\n\n\n\n\n\n\n\nData Analysis, Wrangling, and Visualization:\nProgramming:\nDatabase Design and Administration:\n\n\n\n\n• Pyspark and SQL (Big Data)\n• Python\n• SQL\n\n\n• R, Python\n• Pytorch, TensorFlow\n• PosrgreSQL\n\n\n• Statistical Analysis in R\n• C#\n• Relational and Non Relational DB\n\n\n• Tableau, Domo, Power BI"
  },
  {
    "objectID": "resume.html#work-experience.",
    "href": "resume.html#work-experience.",
    "title": "Daniel Dominguez",
    "section": "Work Experience.",
    "text": "Work Experience.\n\nData Analyst.\nBYUI Communications (Rexburg, ID)\nOct 2021- Present\n\nDesigned and implemented a GA4-based analytics funnel to analyze the University Admissions process, understanding the story and identifying key stages where applicants face delays or drop out. Utilized insights to propose strategic improvements, significantly enhancing process efficiency and applicant experience.\n\nEstablished pipeline from Twilio API in Python, retrieving 30K+ messages per month, ETL the data to connect to a dashboard on Power BI with the purpose of recognizing messaging trends, dropout, sentiment analysis, usage by 33 university departments, and billing.\n\nLeveraged web-scraping in R data of 94K pages, to optimize user experiences and drive success, identifying errors and updating SEO content to improve accessibility and usability.\n\nProficiency in Google Analytics and GA4, to monitor and evaluate engagement data for 60,000+ students, 200,000+ alumni, and various stakeholders, translating insights into meaningful visual reports in Looker Studio, for 33 different University departments.\n\n\n\nData Science Tutor.\nBYUI Math Department (Rexburg, ID)\nSep 2023 - Present\n\nAid students with their questions to meet rubrics on assignments from different courses such as: Data Science Programing, Data Wrangling, Big Data, Machine Learning."
  },
  {
    "objectID": "resume.html#projects.",
    "href": "resume.html#projects.",
    "title": "Daniel Dominguez",
    "section": "Projects.",
    "text": "Projects.\n\niWorq Consulting Project (Remote).\nApr-Jul 2023\n\n\nAutomated a live dashboard to seamlessly retrieve data from AWS-S3, working of over 9 million CSV data points into the refined Parquet format using R. This transformative process empowered intricate feature engineering, culminating in early predictions of service cancellations based on client usage patterns.\n\n\n\n\nWhat’s That Fish.\nApr-Jul 2023\n\n\nDeveloped a fish identification app utilizing Convolutional Neural Networks (92% accuracy) to recognize trout species from photos. Deployed the app on Google Firebase, enabling seamless photo storage and database integration for user interactions. \n\n\n\nSinclair Tire Dealership.\n(BYUI Data Science Society) Apr-Jul 2023\n\n\nDeveloped on Streamlit an analytics tool to Web-scrape a given URL to aid a tire dealership from a given URL and retrieve information to help establish retail price according to the competition.\n\n\n\n\nJohn Deer - Stotz.\n(BYUI Data Science Society) Jan-Apr 2023\n\n\nProject manage to establish a data pipeline and do ETL Process working with the John Deere API to feed a dashboard. The scope of the project was to do exploratory analysis to understand the data for future projects.\n\n\n\n\nBeehive Credit Union.\n(BYUI Data Science Society) Sep-Dec 2022\n\n\nEmployed Python, Spark, and TensorFlow to create a machine learning model, enabling Beehive Credit Union to make informed decisions about new possible store locations by analyzing National Census Data and institutional data."
  },
  {
    "objectID": "resume.html#volunteer",
    "href": "resume.html#volunteer",
    "title": "Daniel Dominguez",
    "section": "Volunteer",
    "text": "Volunteer\n\nData Science Society Leadership Apr 2023-Present\nBYU-I Data Science Society Rexburg, ID\n\n\nLed and structured the efforts of 11 projects, maintaining regular contact and meetings with the 22 project managers to facilitate project progress, tools, and deliver the final products by the due date."
  }
]